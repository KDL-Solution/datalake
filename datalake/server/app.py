import concurrent.futures
import asyncio
import logging
import uvicorn
import os
from typing import Dict, List, Optional
from datetime import datetime
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

from datalake.server.app import DatalakeProcessor
from datalake.utils import setup_logging


class ValidateAssetsRequest(BaseModel):
    """DataFrame ê¸°ë°˜ Assets ìœ íš¨ì„± ê²€ì‚¬ ìš”ì²­"""
    user_id: str
    search_data: List[Dict] 
    sample_percent: Optional[float] = None


class StatusResponse(BaseModel):
    """ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""
    pending: int
    processing: int
    failed: int
    server_status: str
    last_updated: str


class ProcessingJob(BaseModel):
    """ì²˜ë¦¬ ì‘ì—… ìƒíƒœ"""
    job_id: str
    status: str  # "running", "completed", "failed"
    started_at: str
    completed_at: str = None
    result: Dict = None
    error: str = None


processor = None
logger = None
current_jobs: Dict[str, ProcessingJob] = {}
job_lock = asyncio.Lock()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    global processor, logger
    
    BASE_PATH = os.environ["BASE_PATH"]
    LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
    BATCH_SIZE = int(os.environ.get("BATCH_SIZE", 1000))
    NUM_PROC = int(os.environ.get("NUM_PROC", 4))
    CREATE_DIRS = os.environ.get("CREATE_DIRS", "false").lower() == "true"
    
    try:
        processor = DatalakeProcessor(
            base_path=BASE_PATH,
            log_level=LOG_LEVEL,
            num_proc=NUM_PROC,
            batch_size=BATCH_SIZE,
            create_dirs=CREATE_DIRS
        )
        setup_logging(
            user_id="server",
            log_level=LOG_LEVEL, 
            base_path=BASE_PATH
        )
        logger = logging.getLogger(__name__)
        logger.info("âœ… DatalakeProcessor ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ Processor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logger.info("ğŸ”„ ì„œë²„ ì¢…ë£Œ ì¤‘...")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Datalake Processing API",
    description="Datalake ë°ì´í„° ì²˜ë¦¬ ë° ì¹´íƒˆë¡œê·¸ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” API ì„œë²„",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/info")
async def get_server_info():
    """ì„œë²„ ì„¤ì • ì •ë³´ ì¡°íšŒ - ëª¨ë“  ê²½ë¡œ ì •ë³´ í¬í•¨"""
    try:
        if not processor:
            raise HTTPException(status_code=503, detail="Processor not initialized")
        
        return {
            "server_status": "running",
            "version": "1.0.0",
            "num_proc": processor.num_proc,
            "batch_size": processor.batch_size,
            "timestamp": datetime.now().isoformat(),
            
            # ëª¨ë“  ê²½ë¡œ ì •ë³´
            "paths": {
                "base_path": str(processor.base_path),
                "staging_path": str(processor.staging_path),
                "staging_pending_path": str(processor.staging_pending_path),
                "staging_processing_path": str(processor.staging_processing_path),
                "staging_failed_path": str(processor.staging_failed_path),
                "catalog_path": str(processor.catalog_path),
                "assets_path": str(processor.assets_path),
                "collections_path": str(processor.collections_path),
            }
        }
    except Exception as e:
        logger.error(f"ì„œë²„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@app.get("/status", response_model=StatusResponse)
async def get_status():
    """í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        if not processor:
            raise HTTPException(status_code=503, detail="Processor not initialized")
        
        status = processor.get_status()
        
        return StatusResponse(
            pending=status["pending"],
            processing=status["processing"],
            failed=status["failed"],
            server_status="running",
            last_updated=datetime.now().isoformat()
        )
    except Exception as e:
        logger.error(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/process", response_model=Dict)
async def process_pending_data(
    background_tasks: BackgroundTasks,
):
    """Pending ë°ì´í„° ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
    try:
        job_id = f"process_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:19]}"

        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…ì´ ìˆëŠ”ì§€ í™•ì¸
        async with job_lock:
            running_jobs = [job for job in current_jobs.values() if job.status == "running"]
            if running_jobs:
                return {
                    "job_id": running_jobs[0].job_id,
                    "status": "already_running",
                    "message": "ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤"
                }

            job = ProcessingJob(
                job_id=job_id,
                status="running",
                started_at=datetime.now().isoformat()
            )
            current_jobs[job_id] = job

        asyncio.create_task(run_processing_job(job_id))

        return {
            "job_id": job_id,
            "status": "started",
            "message": "ì²˜ë¦¬ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤"
        }

    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/validate-assets")
async def validate_assets(request: ValidateAssetsRequest, background_tasks: BackgroundTasks):
    """DataFrame ê¸°ë°˜ NAS Assets íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ (ë¹„ë™ê¸°)"""
    try:
        job_id = f"validate_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:19]}"
        
        async with job_lock:
            # ê¸°ì¡´ validation job í™•ì¸
            running_validation_jobs = [
                job for job in current_jobs.values() 
                if job.status == "running" and job.job_id.startswith("validate_")
            ]
            
            if running_validation_jobs:
                return {
                    "job_id": running_validation_jobs[0].job_id,
                    "status": "already_running",
                    "message": "ì´ë¯¸ ìœ íš¨ì„± ê²€ì‚¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤"
                }
        
            job = ProcessingJob(
                job_id=job_id,
                status="running",
                started_at=datetime.now().isoformat()
            )
            current_jobs[job_id] = job
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
        asyncio.create_task(run_validation_job(job_id, request))
        
        return {
            "job_id": job_id,
            "status": "started",
            "message": f"íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°: {len(request.search_data)}ê°œ"
        }
        
    except Exception as e:
        logger.error(f"ìœ íš¨ì„± ê²€ì‚¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/jobs/{job_id}")
async def get_job_status(job_id: str):
    async with job_lock:
        job = current_jobs.get(job_id)
    if job is None:
        raise HTTPException(status_code=404, detail="Job not found")
    
    return {
        "job_id": job.job_id,
        "status": job.status,
        "started_at": job.started_at,
        "completed_at": job.completed_at,
        "result": job.result,
        "error": job.error
    }


@app.get("/jobs")
async def list_jobs():
    """ëª¨ë“  ì‘ì—… ëª©ë¡"""
    async with job_lock:
        return {
            "jobs": [
                {
                    "job_id": job.job_id,
                    "status": job.status,
                    "started_at": job.started_at,
                    "completed_at": job.completed_at
                }
                for job in current_jobs.values()
            ]
        }


@app.delete("/jobs/{job_id}")
async def delete_job(job_id: str):
    """ì‘ì—… ì‚­ì œ (ì™„ë£Œëœ ì‘ì—…ë§Œ)"""
    async with job_lock:
        if job_id not in current_jobs:
            raise HTTPException(status_code=404, detail="Job not found")
        
        job = current_jobs[job_id]
        if job.status == "running":
            raise HTTPException(status_code=400, detail="Cannot delete running job")
        
        del current_jobs[job_id]
        logger.info(f"âœ… ì‘ì—… {job_id} ì‚­ì œë¨")
        return {"message": f"Job {job_id} deleted"}


async def run_processing_job(job_id: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•  ì²˜ë¦¬ ì‘ì—…"""
    await _run_background_job(
        job_id=job_id,
        job_name="ì²˜ë¦¬ ì‘ì—…",
        job_func=processor.process_all_pending,
        job_args=(),
    )


async def run_validation_job(job_id: str, request: ValidateAssetsRequest):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•  ìœ íš¨ì„± ê²€ì‚¬ ì‘ì—…"""
    await _run_background_job(
        job_id=job_id,
        job_name="ìœ íš¨ì„± ê²€ì‚¬",
        job_func=processor.validate_assets,
        job_args=(request.user_id, request.search_data, request.sample_percent),
        extra_log_info=f"ìœ ì €: {request.user_id}, ë°ì´í„°: {len(request.search_data)}ê°œ"
    )


async def _run_background_job(
    job_id: str, 
    job_name: str, 
    job_func, 
    job_args: tuple = (),
    extra_log_info: str = ""
):
    try:
        log_msg = f"ğŸ”„ {job_name} ì‹œì‘: {job_id}"
        if extra_log_info:
            log_msg += f" ({extra_log_info})"
        logger.info(log_msg)
        
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, job_func, *job_args)
        
        # ì„±ê³µ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        await _update_job_status(job_id, "completed", result=result)
        logger.info(f"âœ… {job_name} ì™„ë£Œ: {job_id}")
        
    except Exception as e:
        await _handle_job_error(job_id, e, job_name)


async def _update_job_status(
    job_id: str, 
    status: str, 
    result: dict = None, 
    error: str = None
):
    """ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸"""
    async with job_lock:
        if job_id in current_jobs:
            current_jobs[job_id].status = status
            current_jobs[job_id].completed_at = datetime.now().isoformat()
            if result:
                current_jobs[job_id].result = result
            if error:
                current_jobs[job_id].error = error
                
                
async def _handle_job_error(job_id: str, error: Exception, job_type: str):
    error_msg = str(error)
    logger.error(f"âŒ {job_type} ì‹¤íŒ¨: {job_id} - {error_msg}")
    
    async with job_lock:
        if job_id in current_jobs:
            current_jobs[job_id].status = "failed"
            current_jobs[job_id].completed_at = datetime.now().isoformat()
            current_jobs[job_id].error = error_msg


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Datalake Processing API Server")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind to")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    parser.add_argument("--log-level", default="INFO", help="Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)")
    parser.add_argument("--base-path", default="/mnt/AI_NAS/datalake/", help="Base path for datalake")
    parser.add_argument("--num-proc", type=int, default=16, help="Number of processing threads")
    parser.add_argument("--batch-size", type=int, default=1000, help="Batch size for processing")
    parser.add_argument("--create-dirs", action="store_true", help="Create necessary directories if they do not exist")

    args = parser.parse_args()

    os.environ["BASE_PATH"] = args.base_path
    os.environ["LOG_LEVEL"] = args.log_level
    os.environ["NUM_PROC"] = str(args.num_proc)
    os.environ["BATCH_SIZE"] = str(args.batch_size)
    os.environ["CREATE_DIRS"] = str(args.create_dirs).lower()
    print(f"ğŸš€ Starting Datalake Processing API Server on {args.host}:{args.port}")

    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=args.workers,
    )
if __name__ == "__main__":
    main()
