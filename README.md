# DataLake Management System

ëŒ€ìš©ëŸ‰ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë°ì´í„° ë ˆì´í¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

### ğŸ“Š ë°ì´í„° ê´€ë¦¬
- **Raw ë°ì´í„° ì—…ë¡œë“œ**: ì›ë³¸ ë°ì´í„°ì…‹ì„ Provider/Dataset êµ¬ì¡°ë¡œ ì¡°ì§í™”
- **Task ë°ì´í„° ê´€ë¦¬**: OCR, VQA, KIE, Layout ë“± íŠ¹ì • íƒœìŠ¤í¬ìš© ë°ì´í„° ìƒì„±
- **ìŠ¤í‚¤ë§ˆ ê²€ì¦**: ë°ì´í„° íƒ€ì…ê³¼ ë©”íƒ€ë°ì´í„° ìë™ ê²€ì¦
- **ì¤‘ë³µ ì œê±°**: í•´ì‹œ ê¸°ë°˜ ì´ë¯¸ì§€ ë° íŒŒì¼ ì¤‘ë³µ ì œê±°

### ğŸ”„ ë°ì´í„° ì²˜ë¦¬
- **ë³‘ë ¬ ì²˜ë¦¬**: ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í†µí•œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ê³ ì† ì²˜ë¦¬
- **ì´ë¯¸ì§€ ìµœì í™”**: PIL ê¸°ë°˜ ì´ë¯¸ì§€ ì••ì¶• ë° í•´ì‹œ ìƒì„±
- **íŒŒì¼ ê´€ë¦¬**: ìƒ¤ë”© ê¸°ë°˜ íš¨ìœ¨ì  íŒŒì¼ ì €ì¥ ì‹œìŠ¤í…œ
- **ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…**: FastAPI ê¸°ë°˜ ë¹„ë™ê¸° ì²˜ë¦¬ ì„œë²„

### ğŸ” ë°ì´í„° ì¡°íšŒ
- **í†µí•© ê²€ìƒ‰**: DuckDB ë° AWS Athena ì§€ì›
- **íŒŒí‹°ì…˜ ê¸°ë°˜ ì¡°íšŒ**: Provider/Dataset/Task/Variant ê³„ì¸µ êµ¬ì¡°
- **JSON ê²€ìƒ‰**: OCR ê²°ê³¼ ë“± JSON ë°ì´í„° ë‚´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
- **ë‹¤ì–‘í•œ ì¶œë ¥**: Parquet, Arrow Dataset, HuggingFace Dataset í˜•íƒœ

## ğŸ“ ì‹œìŠ¤í…œ êµ¬ì¡°

```
datalake/
â”œâ”€â”€ core/                   
â”‚   â”œâ”€â”€ datalake.py  
â”‚   â””â”€â”€ schema.py        
â”œâ”€â”€ server/          
â”‚   â”œâ”€â”€ app.py          # FastAPI ì²˜ë¦¬ ì„œë²„
â”‚   â””â”€â”€ processor.py    # ë°ì´í„° ì²˜ë¦¬ ì—”ì§„
â”œâ”€â”€ clients/                      # ì¿¼ë¦¬ í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ duckdb_client.py    # DuckDB í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ athena_client.py    # AWS Athena í´ë¼ì´ì–¸íŠ¸
â”‚   â””â”€â”€ queries/
â”‚       â””â”€â”€ json_queries.py
â”œâ”€â”€ main.py             # CLI ì¸í„°í˜ì´ìŠ¤
â””â”€â”€ config.yaml         # CLI config ì„¤ì •
```

## ğŸ› ï¸ ì„¤ì¹˜

### 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
git clone <repository>
cd datalake
pip install -e .
```

### 2. í•„ìˆ˜ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
```bash
mkdir -p /mnt/AI_NAS/datalake/{staging/{pending,processing,failed},catalog,assets,config,logs}
```

### 3. NAS ì²˜ë¦¬ ì„œë²„ ì‹¤í–‰ ( ì„œë²„ ì „ìš© )
```bash
python server/app.py \
    --host 0.0.0.0 \
    --port 8091 \
    --base-path /mnt/AI_NAS/datalake \
    --num-proc 16 \
    --batch-size 1000
```

## ğŸ“– ì‚¬ìš©ë²•

### CLI ì‚¬ìš©ë²•

#### 1. ì´ˆê¸° ì„¤ì •
```bash
# Provider ìƒì„±
python main.py config provider create

# Task ìƒì„± (OCR ì˜ˆì‹œ)
python main.py config task create
# Task ì´ë¦„: ocr
# í•„ìˆ˜ í•„ë“œ: lang, src
# í—ˆìš© ê°’: lang=ko,en,ja,multi / src=real,synthetic
```

#### 2. ë°ì´í„° ì—…ë¡œë“œ
```bash
# Raw ë°ì´í„° ì—…ë¡œë“œ
python main.py upload
# ë°ì´í„° íƒ€ì…: raw
# íŒŒì¼ ê²½ë¡œ: /path/to/dataset
# Provider: huggingface
# Dataset: coco_2017

# Task ë°ì´í„° ì—…ë¡œë“œ
python main.py upload  
# ë°ì´í„° íƒ€ì…: task
# Provider: huggingface
# Dataset: coco_2017 (ê¸°ì¡´)
# Task: ocr
# Variant: base_ocr
# ë©”íƒ€ë°ì´í„°: lang=ko, src=real
```

#### 3. ë°ì´í„° ì²˜ë¦¬
```bash
# ì²˜ë¦¬ ì‹œì‘
python main.py process start

# ì²˜ë¦¬ ìƒíƒœ í™•ì¸
python main.py process status <JOB_ID>

# ë‚´ ë°ì´í„° í˜„í™©
python main.py process list
```

#### 4. ë°ì´í„° ë‹¤ìš´ë¡œë“œ
```bash
# Catalog DB êµ¬ì¶• (ìµœì´ˆ 1íšŒ)
python main.py catalog rebuild

# ë°ì´í„° ë‹¤ìš´ë¡œë“œ
python main.py download
# ê²€ìƒ‰ ë°©ë²•: 1 (íŒŒí‹°ì…˜ ê¸°ë°˜) ë˜ëŠ” 2 (í…ìŠ¤íŠ¸ ê²€ìƒ‰)
# ë‹¤ìš´ë¡œë“œ í˜•íƒœ: 1 (Parquet), 2 (Arrow), 3 (Dataset+ì´ë¯¸ì§€)
```

### Python API ì‚¬ìš©ë²•

#### 1. ê¸°ë³¸ ì‚¬ìš©ë²•
```python
from core.datalake import DatalakeClient

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = DatalakeClient(
    base_path="/mnt/AI_NAS/datalake",
    nas_api_url="http://localhost:8091"
)

# Raw ë°ì´í„° ì—…ë¡œë“œ
staging_dir, job_id = client.upload_raw_data(
    data_file="dataset.parquet",  # ë˜ëŠ” pandas DataFrame
    provider="huggingface",
    dataset="coco_2017",
    dataset_description="COCO 2017 dataset for object detection"
)

# Task ë°ì´í„° ì—…ë¡œë“œ
staging_dir, job_id = client.upload_task_data(
    data_file=processed_df,
    provider="huggingface", 
    dataset="coco_2017",
    task="ocr",
    variant="base_ocr",
    meta={"lang": "ko", "src": "real"}
)
```

#### 2. ë°ì´í„° ì¡°íšŒ
```python
from clients.duckdb_client import DuckDBClient

# DuckDB í´ë¼ì´ì–¸íŠ¸
with DuckDBClient("/mnt/AI_NAS/datalake/catalog.duckdb") as duck:
    # íŒŒí‹°ì…˜ ì¡°íšŒ
    partitions = duck.retrieve_partitions("catalog")
    
    # ì¡°ê±´ë¶€ ë°ì´í„° ì¡°íšŒ
    data = duck.retrieve_with_existing_cols(
        providers=["huggingface"],
        datasets=["coco_2017"], 
        tasks=["ocr"],
        variants=["base_ocr"]
    )
```

## ğŸ“Š ë°ì´í„° êµ¬ì¡°

### Catalog êµ¬ì¡°
```
catalog/
â”œâ”€â”€ provider=huggingface/
â”‚   â””â”€â”€ dataset=coco_2017/
â”‚       â”œâ”€â”€ task=raw/
â”‚       â”‚   â”œâ”€â”€ variant=image/
â”‚       â”‚   â”‚   â”œâ”€â”€ data.parquet     # ë©”íƒ€ë°ì´í„°
â”‚       â”‚   â”‚   â””â”€â”€ _metadata.json   # ì—…ë¡œë“œ ì •ë³´
â”‚       â”‚   â””â”€â”€ variant=mixed/
â”‚       â””â”€â”€ task=ocr/
â”‚           â””â”€â”€ variant=base_ocr/
â””â”€â”€ provider=aihub/
    â””â”€â”€ dataset=document_ocr/
```

### Assets êµ¬ì¡° (ìƒ¤ë”©)
```
assets/
â”œâ”€â”€ provider=huggingface/
â”‚   â””â”€â”€ dataset=coco_2017/
â”‚       â”œâ”€â”€ ab/
â”‚       â”‚   â”œâ”€â”€ cd/
â”‚       â”‚   â”‚   â”œâ”€â”€ abcd1234...hash.jpg
â”‚       â”‚   â”‚   â””â”€â”€ abcd5678...hash.jpg
â”‚       â”‚   â””â”€â”€ ef/
â”‚       â””â”€â”€ gh/
```


### HuggingFace Datasets ì—°ë™
```python
from datasets import load_from_disk

# Dataset í˜•íƒœë¡œ ë‹¤ìš´ë¡œë“œëœ ë°ì´í„° ë¡œë“œ
dataset = load_from_disk("./downloads/my_dataset")

# ì´ë¯¸ì§€ í™•ì¸
dataset[0]['image'].show()

# pandasë¡œ ë³€í™˜
df = dataset.to_pandas()
```

## ğŸ› TroubleShoot

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**1. NAS API ì—°ê²° ì‹¤íŒ¨**
```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8091/health

# ì„œë²„ ì¬ì‹œì‘
python server/app.py --port 8091
```

**2. ë©”ëª¨ë¦¬ ë¶€ì¡±**
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
client = DatalakeClient(num_proc=4, batch_size=500)
```

**3. ê¶Œí•œ ë¬¸ì œ**
```bash
# ë””ë ‰í† ë¦¬ ê¶Œí•œ ì„¤ì •
chmod -R 775 /mnt/AI_NAS/datalake
chown -R $USER:$GROUP /mnt/AI_NAS/datalake
```