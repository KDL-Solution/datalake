import argparse
import json
import shutil
import pandas as pd
import psutil

from datasets import Dataset
from PIL import Image
from pathlib import Path
from datetime import datetime

from managers.datalake_client import DatalakeClient  
from client.src.core.duckdb_client import DuckDBClient

class CatalogError(Exception):
    """Catalog ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class CatalogNotFoundError(CatalogError):
    """Catalog DB íŒŒì¼ì´ ì—†ìŒ"""
    pass

class CatalogEmptyError(CatalogError):
    """Catalogì— ë°ì´í„°ê°€ ì—†ìŒ"""
    pass

class CatalogLockError(CatalogError):
    """Catalog DBê°€ ì ê¸ˆ ìƒíƒœ"""
    pass


class DataManagerCLI:
    """Data Manager CLI ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(
        self, 
        base_path: str = "/mnt/AI_NAS/datalake",
        nas_api_url: str = "http://192.168.20.62:8091",
        log_level: str = "INFO",
        num_proc: int = 16,
    ):
        self.data_manager = DatalakeClient(
            base_path=base_path,
            nas_api_url=nas_api_url,
            log_level=log_level,
            num_proc=num_proc
        )
        self.schema_manager = self.data_manager.schema_manager
        self.duckdb_path = self.data_manager.base_path / "db" / "catalog.duckdb"
    
    def show_catalog_db_info(self):
        """Catalog DB ì •ë³´ í‘œì‹œ"""
        print("\nğŸ“Š Catalog DB ì •ë³´")
        print("="*50)
        
        try:
            db_path = self.duckdb_path
            
            if not db_path.exists():
                print("âŒ Catalog DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                print(f"ğŸ’¡ 'python cli.py catalog rebuild' ëª…ë ¹ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                return False
            
            # DB ê¸°ë³¸ ì •ë³´
            db_size = db_path.stat().st_size / 1024 / 1024
            from datetime import datetime
            db_mtime = datetime.fromtimestamp(db_path.stat().st_mtime)
            
            print(f"ğŸ“ DB íŒŒì¼: {db_path}")
            print(f"ğŸ’¾ íŒŒì¼ í¬ê¸°: {db_size:.1f}MB")
            print(f"ğŸ•’ ìˆ˜ì • ì‹œê°„: {db_mtime.strftime('%Y-%m-%d %H:%M:%S')}")
            
            with DuckDBClient(str(db_path)) as duck_client:
                # í…Œì´ë¸” ì •ë³´
                tables = duck_client.list_tables()
                print(f"\nğŸ“‹ í…Œì´ë¸”: {len(tables)}ê°œ")
                for _, table in tables.iterrows():
                    print(f"  â€¢ {table['name']}")
                
                if 'catalog' in tables['name'].values:
                    # Catalog í…Œì´ë¸” ìƒì„¸ ì •ë³´
                    count_result = duck_client.execute_query("SELECT COUNT(*) as total FROM catalog")
                    total_rows = count_result['total'].iloc[0]
                    
                    partitions_df = duck_client.retrieve_partitions("catalog")
                    
                    print(f"\nğŸ“Š Catalog í…Œì´ë¸”:")
                    print(f"  ğŸ“ˆ ì´ í–‰ ìˆ˜: {total_rows:,}ê°œ")
                    print(f"  ğŸ·ï¸ íŒŒí‹°ì…˜: {len(partitions_df)}ê°œ")
                    
                    # ìƒìœ„ Providerë³„ í†µê³„
                    if not partitions_df.empty:
                        provider_stats = partitions_df.groupby('provider').size().sort_values(ascending=False)
                        print(f"\nğŸ¢ Providerë³„ íŒŒí‹°ì…˜ ìˆ˜:")
                        for provider, count in provider_stats.head(5).items():
                            print(f"  â€¢ {provider}: {count}ê°œ")
                        
                        if len(provider_stats) > 5:
                            print(f"  ... ì™¸ {len(provider_stats) - 5}ê°œ")
                
                return True
                
        except Exception as e:
            print(f"âŒ DB ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return False
        
    def create_provider_interactive(self):
        """ëŒ€í™”í˜• Provider ìƒì„±"""
        print("\n" + "="*50)
        print("ğŸ¢ ìƒˆ Provider ìƒì„±")
        print("="*50)
        
        try:
            # Provider ì´ë¦„ ì…ë ¥
            provider_name = input("ğŸ¢ Provider ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if not provider_name:
                print("âŒ Provider ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return False
            
            # ê¸°ì¡´ Provider í™•ì¸
            if provider_name in self.schema_manager.get_all_providers():
                print(f"âš ï¸ Provider '{provider_name}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                return False
            
            # í™•ì¸ ë° ìƒì„±
            confirm = input(f"\nProvider '{provider_name}'ë¥¼ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm in ['y', 'yes']:
                result = self.schema_manager.add_provider(provider_name)
                if result:
                    print(f"âœ… Provider '{provider_name}' ìƒì„± ì™„ë£Œ!")
                    return True
                else:
                    print(f"âŒ Provider ìƒì„± ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Provider ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def create_task_interactive(self):
        """ëŒ€í™”í˜• Task ìƒì„±"""
        print("\n" + "="*50)
        print("ğŸ”§ ìƒˆ Task ìƒì„±")
        print("="*50)
        
        try:
            # Task ì´ë¦„ ì…ë ¥
            task_name = input("ğŸ“ Task ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if not task_name:
                print("âŒ Task ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return False
            
            # ê¸°ì¡´ Task í™•ì¸
            existing_tasks = self.schema_manager.get_all_tasks()
            if task_name in existing_tasks:
                print(f"âš ï¸ Task '{task_name}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                update = input("ì—…ë°ì´íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if update not in ['y', 'yes']:
                    return False
            
            # í•„ìˆ˜ í•„ë“œ ì…ë ¥
            print("\nğŸ“ í•„ìˆ˜ í•„ë“œ ì„¤ì • (Enterë¡œ ì™„ë£Œ)")
            required_fields = []
            while True:
                field = input(f"í•„ìˆ˜ í•„ë“œ #{len(required_fields)+1}: ").strip()
                if not field:
                    break
                required_fields.append(field)
                print(f"  âœ… ì¶”ê°€ë¨: {field}")
            
            # í—ˆìš© ê°’ ì„¤ì •
            print("\nğŸ”§ í—ˆìš© ê°’ ì„¤ì •")
            allowed_values = {}
            for field in required_fields:
                values_input = input(f"{field}ì˜ í—ˆìš© ê°’ (ì‰¼í‘œë¡œ êµ¬ë¶„, ìƒëµ ê°€ëŠ¥): ").strip()
                if values_input:
                    values = [v.strip() for v in values_input.split(',') if v.strip()]
                    if values:
                        allowed_values[field] = values
                        print(f"  âœ… {field}: {values}")
            
            # í™•ì¸ ë° ìƒì„±
            print(f"\nğŸ“‹ Task ì„¤ì • í™•ì¸:")
            print(f"  ì´ë¦„: {task_name}")
            print(f"  í•„ìˆ˜ í•„ë“œ: {required_fields}")
            print(f"  í—ˆìš© ê°’: {allowed_values}")
            
            confirm = input("\nìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm in ['y', 'yes']:
                if task_name in existing_tasks:
                    result = self.schema_manager.update_task(task_name, required_fields, allowed_values)
                else:
                    result = self.schema_manager.add_task(task_name, required_fields, allowed_values)
                
                if result:
                    print(f"âœ… Task '{task_name}' ìƒì„±/ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                    return True
                else:
                    print(f"âŒ Task ìƒì„±/ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Task ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def remove_provider_interactive(self):
        """ëŒ€í™”í˜• Provider ì œê±°"""
        providers = self.schema_manager.get_all_providers()
        if not providers:
            print("âŒ ì œê±°í•  Providerê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print("\nğŸ¢ ë“±ë¡ëœ Provider:")
        for i, provider in enumerate(providers, 1):
            print(f"  {i}. {provider}")
        
        try:
            choice = input("\nì œê±°í•  Provider ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„: ").strip()
            if choice.isdigit():
                idx = int(choice) - 1
                if 0 <= idx < len(providers):
                    provider = providers[idx]
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return False
            else:
                provider = choice
                if provider not in providers:
                    print(f"âŒ Provider '{provider}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return False
            
            confirm = input(f"\nProvider '{provider}'ë¥¼ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm in ['y', 'yes']:
                result = self.schema_manager.remove_provider(provider)
                if result:
                    print(f"âœ… Provider '{provider}' ì œê±° ì™„ë£Œ!")
                    return True
                else:
                    print(f"âŒ Provider ì œê±° ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ì œê±°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ì œê±°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Provider ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def remove_task_interactive(self):
        """ëŒ€í™”í˜• Task ì œê±°"""
        tasks = self.schema_manager.get_all_tasks()
        if not tasks:
            print("âŒ ì œê±°í•  Taskê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print("\nğŸ“ ë“±ë¡ëœ Task:")
        task_names = list(tasks.keys())
        for i, task_name in enumerate(task_names, 1):
            print(f"  {i}. {task_name}")
        
        try:
            choice = input("\nì œê±°í•  Task ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„: ").strip()
            if choice.isdigit():
                idx = int(choice) - 1
                if 0 <= idx < len(task_names):
                    task = task_names[idx]
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return False
            else:
                task = choice
                if task not in tasks:
                    print(f"âŒ Task '{task}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return False
            
            confirm = input(f"\nTask '{task}'ë¥¼ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm in ['y', 'yes']:
                result = self.schema_manager.remove_task(task)
                if result:
                    print(f"âœ… Task '{task}' ì œê±° ì™„ë£Œ!")
                    return True
                else:
                    print(f"âŒ Task ì œê±° ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ì œê±°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ì œê±°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Task ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def upload_data_interactive(self):
        """ëŒ€í™”í˜• ë°ì´í„° ì—…ë¡œë“œ"""
        print("\n" + "="*50)
        print("ğŸ“¥ ë°ì´í„° ì—…ë¡œë“œ")
        print("="*50)
        
        try:
            # 1. ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì…ë ¥
            data_file = input("ğŸ“ ë°ì´í„° íŒŒì¼ ê²½ë¡œ: ").strip()
            if not data_file or not Path(data_file).exists():
                print("âŒ ìœ íš¨í•œ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                return False
            
            # 2. ë°ì´í„° íƒ€ì… ì„ íƒ (ê°€ì¥ ì¤‘ìš”í•œ ë¶„ê¸°ì )
            data_type = input("\nğŸ“ ë°ì´í„° íƒ€ì… (raw/task) [raw]: ").strip().lower() or "raw"
            if data_type not in ["raw", "task"]:
                print("âŒ ì˜ëª»ëœ ë°ì´í„° íƒ€ì…ì…ë‹ˆë‹¤. (raw ë˜ëŠ” task)")
                return False
            
            # 3. Provider ì„ íƒ
            providers = self.schema_manager.get_all_providers()
            if not providers:
                print("âŒ ë“±ë¡ëœ Providerê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Providerë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                return False
                
            print(f"\nğŸ¢ ì‚¬ìš© ê°€ëŠ¥í•œ Provider:")
            for i, provider in enumerate(providers, 1):
                print(f"  {i}. {provider}")
            
            provider_choice = input("Provider ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„ ì…ë ¥: ").strip()
            if provider_choice.isdigit():
                idx = int(provider_choice) - 1
                if 0 <= idx < len(providers):
                    provider = providers[idx]
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return False
            else:
                provider = provider_choice
                if provider not in providers:
                    print(f"âŒ Provider '{provider}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return False
            
            # 4. ë°ì´í„° íƒ€ì…ë³„ í”Œë¡œìš°
            if data_type == "raw":
                # Raw ë°ì´í„°: ìƒˆ Dataset ìƒì„±
                dataset = input("\nğŸ“¦ ìƒˆ Dataset ì´ë¦„: ").strip()
                if not dataset:
                    print("âŒ Dataset ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    return False
                
                description = input("ğŸ“„ ë°ì´í„°ì…‹ ì„¤ëª… (ì„ íƒì‚¬í•­): ").strip()
                source = input("ğŸ”— ì›ë³¸ ì†ŒìŠ¤ URL (ì„ íƒì‚¬í•­): ").strip()
                
                print(f"\nğŸ“‹ ì—…ë¡œë“œ ì •ë³´:")
                print(f"  ğŸ“ íŒŒì¼: {data_file}")
                print(f"  ğŸ“ íƒ€ì…: Raw ë°ì´í„°")
                print(f"  ğŸ¢ Provider: {provider}")
                print(f"  ğŸ“¦ Dataset: {dataset} (ìƒˆë¡œ ìƒì„±)")
                if description:
                    print(f"  ğŸ“„ ì„¤ëª…: {description}")
                if source:
                    print(f"  ğŸ”— ì†ŒìŠ¤: {source}")
                
                confirm = input("\nì—…ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if confirm in ['y', 'yes']:
                    staging_dir, job_id = self.data_manager.upload_raw_data(
                        data_file=data_file,
                        provider=provider,
                        dataset=dataset,
                        dataset_description=description,
                        original_source=source
                    )
                    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
                    print("ğŸ’¡ 'python cli.py process start' ëª…ë ¹ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    return True
                    
            elif data_type == "task":
                # Task ë°ì´í„°: ê¸°ì¡´ Datasetì—ì„œ ì„ íƒ
                print(f"\nğŸ“¦ ê¸°ì¡´ Dataset ì„ íƒ:")
                print("ğŸ’¡ Task ë°ì´í„°ëŠ” ê¸°ì¡´ì— ì—…ë¡œë“œëœ raw ë°ì´í„°ì—ì„œ ì¶”ì¶œë©ë‹ˆë‹¤.")
                
                # í•´ë‹¹ Providerì˜ ê¸°ì¡´ dataset ëª©ë¡ ì¡°íšŒ
                catalog_path = self.data_manager.catalog_path / f"provider={provider}"
                existing_datasets = []
                
                if catalog_path.exists():
                    for dataset_dir in catalog_path.iterdir():
                        if dataset_dir.is_dir() and dataset_dir.name.startswith("dataset="):
                            dataset_name = dataset_dir.name.replace("dataset=", "")
                            existing_datasets.append(dataset_name)
                
                if not existing_datasets:
                    print(f"âŒ Provider '{provider}'ì— ì—…ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    print("ğŸ’¡ ë¨¼ì € raw ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                    return False
                
                print(f"\nğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ Dataset ({len(existing_datasets)}ê°œ):")
                for i, dataset_name in enumerate(existing_datasets, 1):
                    print(f"  {i}. {dataset_name}")
                
                dataset_choice = input("Dataset ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„ ì…ë ¥: ").strip()
                if dataset_choice.isdigit():
                    idx = int(dataset_choice) - 1
                    if 0 <= idx < len(existing_datasets):
                        dataset = existing_datasets[idx]
                    else:
                        print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                        return False
                else:
                    dataset = dataset_choice
                    if dataset not in existing_datasets:
                        print(f"âŒ Dataset '{dataset}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        return False
                
                # Task ì„ íƒ
                tasks = self.schema_manager.get_all_tasks()
                if not tasks:
                    print("âŒ ë“±ë¡ëœ Taskê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Taskë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                    return False
                    
                print(f"\nğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ Task:")
                task_names = list(tasks.keys())
                for i, task_name in enumerate(task_names, 1):
                    print(f"  {i}. {task_name}")
                
                task_choice = input("Task ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„ ì…ë ¥: ").strip()
                if task_choice.isdigit():
                    idx = int(task_choice) - 1
                    if 0 <= idx < len(task_names):
                        task = task_names[idx]
                    else:
                        print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                        return False
                else:
                    task = task_choice
                    if task not in tasks:
                        print(f"âŒ Task '{task}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        return False
                
                # Variant ì…ë ¥
                variant = input("\nğŸ·ï¸ Variant ì´ë¦„: ").strip()
                if not variant:
                    print("âŒ Variant ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    return False
                
                # í•„ìˆ˜ í•„ë“œ ì…ë ¥
                all_tasks = self.data_manager.schema_manager.get_all_tasks()
                task_info = all_tasks.get(task, {})
                required_fields = task_info.get('required_fields', [])
                allowed_values = task_info.get('allowed_values', {})
                
                metadata = {}
                if required_fields:
                    print(f"\nğŸ“ í•„ìˆ˜ í•„ë“œ ì…ë ¥:")
                    for field in required_fields:
                        if field in allowed_values:
                            print(f"  {field} í—ˆìš©ê°’: {allowed_values[field]}")
                        value = input(f"  {field}: ").strip()
                        if not value:
                            print(f"âŒ í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            return False
                        metadata[field] = value
                
                # ê²€ì¦
                is_valid, error_msg = self.data_manager.schema_manager.validate_task_metadata(task, metadata)
                if not is_valid:
                    print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
                    return False
                
                print(f"\nğŸ“‹ ì—…ë¡œë“œ ì •ë³´:")
                print(f"  ğŸ“ íŒŒì¼: {data_file}")
                print(f"  ğŸ“ íƒ€ì…: Task ë°ì´í„°")
                print(f"  ğŸ¢ Provider: {provider}")
                print(f"  ğŸ“¦ Dataset: {dataset} (ê¸°ì¡´)")
                print(f"  ğŸ“ Task: {task}")
                print(f"  ğŸ·ï¸ Variant: {variant}")
                print(f"  ğŸ“‹ ë©”íƒ€ë°ì´í„°: {metadata}")
                
                confirm = input("\nì—…ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if confirm in ['y', 'yes']:
                    staging_dir, job_id = self.data_manager.upload_task_data(
                        data_file=data_file,
                        provider=provider,
                        dataset=dataset,
                        task=task,
                        variant=variant,
                        **metadata
                    )
                    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
                    print("ğŸ’¡ 'python cli.py process start' ëª…ë ¹ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    return True
                
        except KeyboardInterrupt:
            print("\nâŒ ì—…ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def download_data_interactive(self):
        """ëŒ€í™”í˜• ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        print("\n" + "="*50)
        print("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        print("="*50)
        
        try:
            db_path = self.duckdb_path
            
            # DB íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not db_path.exists():
                raise CatalogNotFoundError(
                    "Catalog DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. "
                    "'python cli.py catalog update' ëª…ë ¹ìœ¼ë¡œ ë¨¼ì € DBë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
                )
            
            # Read-only ëª¨ë“œë¡œ DuckDB ì—°ê²°
            try:
                with DuckDBClient(str(db_path), read_only=True) as duck_client:
                    print("ğŸ”„ Catalog ë°ì´í„° ë¡œë”© ì¤‘...")
                    catalog_path = self.data_manager.catalog_path

                    # ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ë§Œ í™•ì¸í•˜ê³  ê°•ì œ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
                    db_is_current = self._check_and_update_catalog_db(duck_client, catalog_path)
                    if not db_is_current:
                        raise CatalogError(
                            "Catalog DBê°€ ìµœì‹  ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤. "
                            "'python cli.py catalog update' ëª…ë ¹ìœ¼ë¡œ DBë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”."
                        )
                    
                    # ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒí‹°ì…˜ í™•ì¸
                    partitions_df = duck_client.retrieve_partitions("catalog")
                    if partitions_df.empty:
                        raise CatalogEmptyError(
                            "Catalogì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. "
                            "'python cli.py catalog update' ëª…ë ¹ìœ¼ë¡œ DBë¥¼ ë‹¤ì‹œ êµ¬ì¶•í•´ë³´ì„¸ìš”."
                        )
                        
                    print(f"ğŸ“Š {len(partitions_df)} ê°œ íŒŒí‹°ì…˜ ì‚¬ìš© ê°€ëŠ¥")
            
                    search_results = self._perform_search(duck_client, partitions_df)
                    
                    if search_results is None or search_results.empty:
                        raise CatalogError("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(search_results):,}ê°œ í•­ëª©")
                    
                    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                    print("\nğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
                    print(search_results.head(10))
                    if len(search_results) > 3:
                        print(f"... (ì´ {len(search_results):,}ê°œ í•­ëª©)")
                    
                    # ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ì„ íƒ
                    return self._download_options(search_results)

                    
            except Exception as db_error:
                if "lock" in str(db_error).lower() or "locked" in str(db_error).lower():
                    raise CatalogLockError(
                        "DBê°€ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. "
                        "ë‹¤ë¥¸ CLI ì„¸ì…˜ì´ë‚˜ Jupyter ë…¸íŠ¸ë¶ì—ì„œ DBë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
                    ) from db_error
                else:
                    raise CatalogError(f"DB ì—°ê²° ì˜¤ë¥˜: {db_error}") from db_error
                    
        except KeyboardInterrupt:
            raise CatalogError("ë‹¤ìš´ë¡œë“œê°€ ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.") from None
        except CatalogError:
            # ì´ë¯¸ ìš°ë¦¬ê°€ ì •ì˜í•œ ì˜ˆì™¸ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
            raise
        except Exception as e:
            raise CatalogError(f"ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}") from e

    def trigger_processing(self):
        """NAS ì²˜ë¦¬ ìˆ˜ë™ ì‹œì‘"""
        print("\n" + "="*50)
        print("ğŸ”„ NAS ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        print("="*50)
        
        try:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            status = self.data_manager.get_nas_status()
            if status:
                pending_count = status.get('pending', 0)
                processing_count = status.get('processing', 0)
                
                print(f"ğŸ“¦ Pending: {pending_count}ê°œ")
                print(f"ğŸ”„ Processing: {processing_count}ê°œ")
                
                if pending_count == 0:
                    print("ğŸ’¡ ì²˜ë¦¬í•  pending ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return True
                
                if processing_count > 0:
                    print("âš ï¸ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤.")
                    continue_anyway = input("ê·¸ë˜ë„ ìƒˆ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                    if continue_anyway not in ['y', 'yes']:
                        print("âŒ ì²˜ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        return False
            else:
                print("âš ï¸ NAS ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue_anyway = input("ê·¸ë˜ë„ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if continue_anyway not in ['y', 'yes']:
                    print("âŒ ì²˜ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return False
            
            # ì²˜ë¦¬ ì‹œì‘
            job_id = self.data_manager.trigger_nas_processing()
            if job_id:
                print(f"âœ… ì²˜ë¦¬ ì‹œì‘ë¨: {job_id}")
                
                # ëŒ€ê¸° ì—¬ë¶€ í™•ì¸
                wait_completion = input("ì²˜ë¦¬ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if wait_completion in ['y', 'yes']:
                    try:
                        print("â³ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸° ì¤‘... (Ctrl+Cë¡œ ì¤‘ë‹¨)")
                        result = self.data_manager.wait_for_job_completion(job_id, timeout=3600)
                        print(f"ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ: {result}")
                        return True
                    except KeyboardInterrupt:
                        print("\nâ¸ï¸ ëŒ€ê¸° ì¤‘ë‹¨ë¨. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ëŠ” ê³„ì†ë©ë‹ˆë‹¤.")
                        print(f"ğŸ’¡ 'python cli.py process status {job_id}' ëª…ë ¹ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        return True
                    except Exception as e:
                        print(f"âŒ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
                        return False
                else:
                    print(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. Job ID: {job_id}")
                    print(f"ğŸ’¡ 'python cli.py process status {job_id}' ëª…ë ¹ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    return True
            else:
                print("âŒ ì²˜ë¦¬ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ì²˜ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def check_job_status(self, job_id: str):
        """íŠ¹ì • ì‘ì—… ìƒíƒœ í™•ì¸"""
        print(f"\nğŸ” ì‘ì—… ìƒíƒœ í™•ì¸: {job_id}")
        print("="*50)
        
        try:
            job_status = self.data_manager.get_job_status(job_id)
            if job_status:
                status = job_status.get('status', 'unknown')
                started_at = job_status.get('started_at', 'N/A')
                finished_at = job_status.get('finished_at', 'N/A')
                
                status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(status, "â“")
                print(f"{status_emoji} ìƒíƒœ: {status}")
                print(f"â° ì‹œì‘: {started_at}")
                
                if status == 'completed':
                    print(f"ğŸ ì™„ë£Œ: {finished_at}")
                    result = job_status.get('result', {})
                    print(f"ğŸ“Š ì„±ê³µ: {result.get('success', 0)}ê°œ")
                    print(f"âŒ ì‹¤íŒ¨: {result.get('failed', 0)}ê°œ")
                elif status == 'failed':
                    print(f"ğŸ’¥ ì‹¤íŒ¨: {finished_at}")
                    error = job_status.get('error', 'Unknown error')
                    print(f"ğŸ” ì˜¤ë¥˜: {error}")
                elif status == 'running':
                    print("ğŸ”„ ì§„í–‰ ì¤‘...")
                    
                return True
            else:
                print(f"âŒ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
                return False
                
        except Exception as e:
            print(f"âŒ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def list_all_data(self):
        """ë‚´ ë°ì´í„° ì „ì²´ í˜„í™© ì¡°íšŒ"""
        print("\nğŸ“‹ ë‚´ ë°ì´í„° í˜„í™©")
        print("="*50)
        
        total_items = 0
        
        try:
            # 1. ğŸ“¥ ì—…ë¡œë“œë¨ (Pending)
            pending_path = self.data_manager.staging_pending_path
            pending_items = []
            
            if pending_path.exists():
                pending_dirs = [d for d in pending_path.iterdir() if d.is_dir()]
                for pending_dir in sorted(pending_dirs, key=lambda x: x.stat().st_mtime, reverse=True):
                    try:
                        metadata_file = pending_dir / "upload_metadata.json"
                        if metadata_file.exists():
                            with open(metadata_file, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                            
                            provider = metadata.get('provider', 'Unknown')
                            dataset = metadata.get('dataset', 'Unknown')
                            task = metadata.get('task', 'Unknown')
                            uploaded_at = metadata.get('uploaded_at', 'Unknown')
                            total_rows = metadata.get('total_rows', 0)
                            
                            try:
                                from datetime import datetime
                                upload_time = datetime.fromisoformat(uploaded_at.replace('Z', '+00:00'))
                                time_str = upload_time.strftime('%m-%d %H:%M')
                            except:
                                time_str = uploaded_at[:10]
                            
                            pending_items.append({
                                'name': f"{provider}/{dataset}/{task}",
                                'time': time_str,
                                'rows': total_rows
                            })
                    except:
                        continue
            
            # 2. ğŸ”„ ì²˜ë¦¬ ì¤‘/ì™„ë£Œ (Jobs)
            jobs = self.data_manager.list_nas_jobs() or []
            recent_jobs = jobs[-5:] if jobs else []  # ìµœê·¼ 5ê°œ
            
            # ì¶œë ¥
            if pending_items:
                print(f"\nğŸ“¥ ì—…ë¡œë“œë¨ ({len(pending_items)}ê°œ)")
                for item in pending_items:
                    rows_str = f"{item['rows']:,}" if item['rows'] > 0 else "?"
                    print(f"  ğŸ“¦ {item['name']} ({rows_str} rows) - {item['time']}")
                total_items += len(pending_items)
            
            if recent_jobs:
                print(f"\nğŸ”„ ì²˜ë¦¬ ì‘ì—… ({len(recent_jobs)}ê°œ)")
                for job in reversed(recent_jobs):
                    status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(job['status'], "â“")
                    job_id_short = job['job_id'][:8] + "..." if len(job['job_id']) > 8 else job['job_id']
                    started_at = job.get('started_at', 'Unknown')
                    try:
                        time_str = started_at.split('T')[1][:5] if 'T' in started_at else started_at[:5]
                    except:
                        time_str = started_at
                    print(f"  {status_emoji} {job_id_short} ({job['status']}) - {time_str}")
                total_items += len(recent_jobs)
            
            # ìš”ì•½ ë° ì•ˆë‚´
            if total_items == 0:
                print("\nğŸ“­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ 'python cli.py upload' ëª…ë ¹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ë³´ì„¸ìš”.")
            else:
                if pending_items:
                    print(f"\nğŸ’¡ 'python cli.py process' ëª…ë ¹ìœ¼ë¡œ ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° í˜„í™© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
   
    def check_db_processes(self):
        """DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
        print("\nğŸ” DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸")
        print("="*50)
        
        db_path = Path(self.duckdb_path).resolve()  # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
        
        try:
            using_processes = []
            
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    # 1. cmdline ê²€ì‚¬ (ê¸°ì¡´ ë°©ì‹)
                    cmdline_match = False
                    if proc.info['cmdline']:
                        cmdline = ' '.join(proc.info['cmdline'])
                        if str(db_path) in cmdline or 'catalog.duckdb' in cmdline:
                            cmdline_match = True
                    
                    # 2. ì—´ë¦° íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ê²€ì‚¬ (ìƒˆë¡œìš´ ë°©ì‹)
                    file_match = False
                    try:
                        process = psutil.Process(proc.info['pid'])
                        open_files = process.open_files()
                        for f in open_files:
                            file_path = Path(f.path).resolve()
                            # DB íŒŒì¼ì´ë‚˜ ê´€ë ¨ íŒŒì¼ë“¤ í™•ì¸
                            if (file_path == db_path or 
                                file_path.name == db_path.name or
                                str(file_path).endswith('.duckdb') or
                                str(file_path).endswith('.duckdb.wal') or
                                str(file_path).endswith('.duckdb.tmp')):
                                file_match = True
                                break
                    except (psutil.AccessDenied, psutil.NoSuchProcess):
                        # ê¶Œí•œì´ ì—†ê±°ë‚˜ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ë¼ì§„ ê²½ìš°
                        pass
                    
                    # 3. ë©”ëª¨ë¦¬ ë§¤í•‘ ê²€ì‚¬ (ì¶”ê°€)
                    memory_match = False
                    try:
                        process = psutil.Process(proc.info['pid'])
                        memory_maps = process.memory_maps()
                        for m in memory_maps:
                            if str(db_path) in m.path:
                                memory_match = True
                                break
                    except (psutil.AccessDenied, psutil.NoSuchProcess, AttributeError):
                        # ì¼ë¶€ ì‹œìŠ¤í…œì—ì„œëŠ” memory_maps()ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                        pass
                    
                    # í•˜ë‚˜ë¼ë„ ë§¤ì¹˜ë˜ë©´ DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤
                    if cmdline_match or file_match or memory_match:
                        match_type = []
                        if cmdline_match: match_type.append("cmdline")
                        if file_match: match_type.append("open_files")
                        if memory_match: match_type.append("memory_map")
                        
                        using_processes.append({
                            'pid': proc.info['pid'],
                            'name': proc.info['name'],
                            'cmdline': cmdline[:100] + '...' if len(cmdline) > 100 else cmdline,
                            'match_type': ', '.join(match_type)
                        })
                        
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            if using_processes:
                print(f"âš ï¸ {len(using_processes)}ê°œ í”„ë¡œì„¸ìŠ¤ê°€ DBë¥¼ ì‚¬ìš© ì¤‘:")
                for proc in using_processes:
                    print(f"  PID {proc['pid']}: {proc['name']} (ê°ì§€: {proc['match_type']})")
                    print(f"    ëª…ë ¹ì–´: {proc['cmdline']}")
                
                print(f"\nğŸ’¡ ì¢…ë£Œ ë°©ë²•:")
                print(f"  - Jupyter ë…¸íŠ¸ë¶: ì»¤ë„ ì¬ì‹œì‘")
                print(f"  - Python ìŠ¤í¬ë¦½íŠ¸: Ctrl+Cë¡œ ì¢…ë£Œ")
                print(f"  - ê°•ì œ ì¢…ë£Œ: kill -9 <PID>")
                
                # 4. lsofë¡œë„ í•œë²ˆ ë” í™•ì¸ (Linux/Mac)
                print(f"\nğŸ” lsofë¡œ ì¶”ê°€ í™•ì¸:")
                try:
                    import subprocess
                    result = subprocess.run(['lsof', str(db_path)], 
                                        capture_output=True, text=True, timeout=5)
                    if result.stdout:
                        print(result.stdout)
                    else:
                        print("  lsofì—ì„œ ì¶”ê°€ í”„ë¡œì„¸ìŠ¤ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
                except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
                    print("  lsof ëª…ë ¹ì–´ ì‚¬ìš© ë¶ˆê°€")
                    
            else:
                print("âœ… DBë¥¼ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # ê·¸ë˜ë„ ì ê¸ˆ ìƒíƒœë¼ë©´ íŒŒì¼ ì‹œìŠ¤í…œ ì´ìŠˆì¼ ìˆ˜ ìˆìŒ
                print(f"\nğŸ” DB íŒŒì¼ ìƒíƒœ í™•ì¸:")
                print(f"  ê²½ë¡œ: {db_path}")
                print(f"  ì¡´ì¬: {db_path.exists()}")
                if db_path.exists():
                    stat = db_path.stat()
                    print(f"  í¬ê¸°: {stat.st_size:,} bytes")
                    print(f"  ìˆ˜ì •ì‹œê°„: {datetime.fromtimestamp(stat.st_mtime)}")
                    
                    # WAL íŒŒì¼ë„ í™•ì¸
                    wal_file = db_path.with_suffix('.duckdb.wal')
                    if wal_file.exists():
                        print(f"  âš ï¸ WAL íŒŒì¼ ì¡´ì¬: {wal_file} (ë¹„ì •ìƒ ì¢…ë£Œ ê°€ëŠ¥ì„±)")
                        
        except ImportError:
            print("âŒ psutil ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install psutil")
        except Exception as e:
            print(f"âŒ í”„ë¡œì„¸ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")

    def safe_update_catalog_db(self):
        """ì ê¸ˆ ìƒíƒœ í™•ì¸ í›„ ì•ˆì „í•œ DB ì—…ë°ì´íŠ¸"""
        print("\n" + "="*50)
        print("ğŸ”„ Catalog DB ì•ˆì „ ì—…ë°ì´íŠ¸")
        print("="*50)
        
        db_path = self.duckdb_path
        
        # 1. ì ê¸ˆ í…ŒìŠ¤íŠ¸
        print("ğŸ” DB ì ê¸ˆ ìƒíƒœ í™•ì¸ ì¤‘...")
        try:
            # ì„ì‹œ ì—°ê²°ë¡œ ì ê¸ˆ í…ŒìŠ¤íŠ¸
            with DuckDBClient(str(db_path), read_only=False) as test_client:
                test_client.execute_query("SELECT 1")
            print("âœ… DB ì ê¸ˆ ì—†ìŒ - ì—…ë°ì´íŠ¸ ê°€ëŠ¥")
            
        except Exception as e:
            if "lock" in str(e).lower():
                print("âŒ DBê°€ ì ê¸ˆ ìƒíƒœì…ë‹ˆë‹¤.")
                print("\nğŸ” ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸:")
                self.check_db_processes()
                
                force = input("\nê·¸ë˜ë„ ê°•ì œ ì—…ë°ì´íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if force not in ['y', 'yes']:
                    print("âŒ ì—…ë°ì´íŠ¸ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return False
            else:
                print(f"âŒ DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                return False
        
        # 2. ì‹¤ì œ ì—…ë°ì´íŠ¸
        print("\nğŸ”„ ì—…ë°ì´íŠ¸ ì‹œì‘...")
        return self._build_catalog_db()

    def quick_catalog_check(self):
        """ë¹ ë¥¸ ì¹´íƒˆë¡œê·¸ ìƒíƒœ í™•ì¸"""
        print("\nğŸ“Š Catalog ë¹ ë¥¸ ìƒíƒœ í™•ì¸")
        print("="*40)
        
        try:
            db_path = self.duckdb_path
            catalog_path = self.data_manager.catalog_path
            
            if not db_path.exists():
                print("âŒ DB íŒŒì¼ ì—†ìŒ")
                return False
            
            if not catalog_path.exists():
                print("âŒ Catalog ë””ë ‰í† ë¦¬ ì—†ìŒ")
                return False
            
            # íŒŒì¼ ì •ë³´
            from datetime import datetime
            db_mtime = datetime.fromtimestamp(db_path.stat().st_mtime)
            db_size = db_path.stat().st_size / 1024 / 1024
            
            # ìµœì‹  Parquet íŒŒì¼ í™•ì¸
            latest_parquet = None
            latest_parquet_mtime = 0
            
            for parquet_file in catalog_path.rglob("*.parquet"):
                file_mtime = parquet_file.stat().st_mtime
                if file_mtime > latest_parquet_mtime:
                    latest_parquet_mtime = file_mtime
                    latest_parquet = parquet_file
            
            print(f"ğŸ“ DB: {db_size:.1f}MB ({db_mtime.strftime('%m-%d %H:%M')})")
            
            if latest_parquet:
                latest_parquet_dt = datetime.fromtimestamp(latest_parquet_mtime)
                print(f"ğŸ“„ ìµœì‹  Parquet: {latest_parquet_dt.strftime('%m-%d %H:%M')}")
                
                if latest_parquet_mtime > db_path.stat().st_mtime:
                    print("âš ï¸ DB ì—…ë°ì´íŠ¸ í•„ìš”")
                else:
                    print("âœ… DB ìµœì‹  ìƒíƒœ")
            
            # ì ê¸ˆ ìƒíƒœ í™•ì¸
            try:
                with DuckDBClient(str(db_path), read_only=True) as duck_client:
                    tables = duck_client.list_tables()
                    if 'catalog' in tables['name'].values:
                        count = duck_client.execute_query("SELECT COUNT(*) as total FROM catalog")
                        print(f"ğŸ“Š ì´ {count['total'].iloc[0]:,}ê°œ í–‰")
                    else:
                        print("âŒ catalog í…Œì´ë¸” ì—†ìŒ")
            except Exception as e:
                if "lock" in str(e).lower():
                    print("ğŸ”’ DB ì ê¸ˆ ìƒíƒœ (ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš© ì¤‘)")
                else:
                    print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    def validate_data_integrity(self, provider=None, generate_report=False):
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ (Dataset library ìµœì í™” ë²„ì „)"""
        print("\n" + "="*50)
        print("ğŸ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ (ë³‘ë ¬ ì²˜ë¦¬)")
        print("="*50)
        
        issues = {
            'missing_files': [],
        }
        
        try:
            from datasets import Dataset
            
            db_path = self.duckdb_path
            if not db_path.exists():
                raise CatalogNotFoundError("Catalog DBê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            with DuckDBClient(str(db_path), read_only=True) as duck_client:
                # ê²€ì‚¬í•  ë°ì´í„° ì¡°íšŒ
                if provider:
                    print(f"ğŸ¢ Provider '{provider}' ê²€ì‚¬ ì¤‘...")
                    query = f"SELECT * FROM catalog WHERE provider = '{provider}'"
                    catalog_data = duck_client.execute_query(query)
                else:
                    print("ğŸŒ ì „ì²´ ë°ì´í„° ê²€ì‚¬ ì¤‘...")
                    catalog_data = duck_client.execute_query("SELECT * FROM catalog")
                
                total_items = len(catalog_data)
                print(f"ğŸ“Š ê²€ì‚¬ ëŒ€ìƒ: {total_items:,}ê°œ í•­ëª©")
                
                if total_items == 0:
                    print("ğŸ“­ ê²€ì‚¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return True
                
                # DataFrameì„ Datasetìœ¼ë¡œ ë³€í™˜
                dataset = Dataset.from_pandas(catalog_data)
                print(f"ğŸ”„ Dataset ìƒì„± ì™„ë£Œ: {len(dataset):,}ê°œ í–‰")
                
                # 1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ (ë³‘ë ¬ ì²˜ë¦¬)
                print("\n1ï¸âƒ£ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ (ë³‘ë ¬ ì²˜ë¦¬ ì¤‘)...")
                
                def check_file_exists(example):
                    """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
                    path_val = example.get('path')
                    if not path_val:
                        example['file_exists'] = None
                        return example
                    
                    file_path = self.data_manager.assets_path / path_val
                    exists = file_path.exists()
                    example['file_exists'] = exists
                    
                    return example
                
                # ë³‘ë ¬ë¡œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                dataset_with_file_check = dataset.map(
                    check_file_exists,
                    desc="íŒŒì¼ ì¡´ì¬ í™•ì¸",
                    num_proc=min(self.data_manager.num_proc, 16),
                    load_from_cache_file=False
                )
                
                # ëˆ„ë½ëœ íŒŒì¼ ì°¾ê¸°
                missing_files_data = dataset_with_file_check.filter(
                    lambda x: not x['file_exists'],
                    desc="ëˆ„ë½ íŒŒì¼ í•„í„°ë§"
                )
                
                issues['missing_files'] = [
                    {
                        'hash': item['hash'],
                        'path': item.get('path'),
                        'provider': item.get('provider'),
                        'dataset': item.get('dataset'),
                        'task': item.get('task'),
                        'variant': item.get('variant')
                    }
                    for item in missing_files_data
                ]
                
                print(f"    âŒ ëˆ„ë½ëœ íŒŒì¼: {len(issues['missing_files'])}ê°œ")
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "="*50)
            print("ğŸ“‹ ê²€ì‚¬ ê²°ê³¼ ìš”ì•½")
            print("="*50)
            
            total_issues = sum(len(issue_list) for issue_list in issues.values())
            
            if total_issues == 0:
                print("âœ… ëª¨ë“  ê²€ì‚¬ í†µê³¼! ë°ì´í„°ê°€ ì •ìƒì…ë‹ˆë‹¤.")
                return True
            
            print(f"âš ï¸ ì´ {total_issues}ê°œ ë¬¸ì œ ë°œê²¬:")
            
            for issue_type, issue_list in issues.items():
                if issue_list:
                    issue_names = {
                        'missing_files': 'ëˆ„ë½ëœ íŒŒì¼',
                    }
                    print(f"  â€¢ {issue_names[issue_type]}: {len(issue_list)}ê°œ")
            
            # ìƒìœ„ ë¬¸ì œë“¤ ìƒ˜í”Œ ì¶œë ¥
            self._print_issue_samples(issues)
            
            # ì²˜ë¦¬ ì˜µì…˜
            print(f"\nğŸ’¡ ì²˜ë¦¬ ì˜µì…˜:")
            if issues['missing_files']:
                print("  - ëˆ„ë½ëœ íŒŒì¼ì€ ìë™ìœ¼ë¡œ ë³µêµ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì—…ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            if generate_report:
                self._generate_validation_report(issues)
            
            return len(issues['missing_files']) == 0  # ì¤‘ìš”í•œ ë¬¸ì œë§Œ False ë°˜í™˜
            
        except ImportError:
            print("âŒ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install datasets")
            return False
            
        except Exception as e:
            print(f"âŒ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    def _print_issue_samples(self, issues):
        """ë¬¸ì œ ìƒ˜í”Œ ì¶œë ¥"""
        print(f"\nğŸ” ì£¼ìš” ë¬¸ì œ ìƒ˜í”Œ:")
        
        # ëˆ„ë½ëœ íŒŒì¼ ìƒ˜í”Œ
        if issues['missing_files']:
            print(f"\n  ğŸ“ ëˆ„ë½ëœ íŒŒì¼ (ìƒìœ„ 3ê°œ):")
            for item in issues['missing_files'][:3]:
                print(f"    â€¢ {item['hash'][:16]}... ({item['provider']}/{item['dataset']})")


    def _generate_validation_report(self, issues):
        """ê²€ì‚¬ ë³´ê³ ì„œ ìƒì„±"""
        from datetime import datetime
        
        report_path = Path(f"./validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        try:
            report_data = {
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_issues': sum(len(issue_list) for issue_list in issues.values()),
                    'by_type': {k: len(v) for k, v in issues.items()}
                },
                'details': issues
            }
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ ìƒì„±: {report_path}")
            
        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
        
    def _check_and_update_catalog_db(self, duck_client, catalog_path):
        """Catalog DB ìƒíƒœ í™•ì¸ ë° ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ íŒë‹¨ (ì ê¸ˆ ì˜¤ë¥˜ ë°©ì§€)"""
        try:
            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            tables = duck_client.list_tables()
            if tables.empty or 'catalog' not in tables['name'].values:
                print("ğŸ“ ìƒˆë¡œìš´ Catalog DB ìƒì„± í•„ìš”")
                raise Exception("Catalog í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # DB íŒŒì¼ê³¼ Parquet íŒŒì¼ë“¤ì˜ ìˆ˜ì • ì‹œê°„ ë¹„êµ
            db_path = self.duckdb_path
            db_mtime = db_path.stat().st_mtime if db_path.exists() else 0
            
            # ê°€ì¥ ìµœê·¼ Parquet íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ í™•ì¸
            latest_parquet_mtime = 0
            for parquet_file in catalog_path.rglob("*.parquet"):
                file_mtime = parquet_file.stat().st_mtime
                if file_mtime > latest_parquet_mtime:
                    latest_parquet_mtime = file_mtime
            
            if latest_parquet_mtime > db_mtime:
                print("ğŸ”„ Parquet íŒŒì¼ì´ DBë³´ë‹¤ ìµœì‹ ì…ë‹ˆë‹¤.")
                print("âš ï¸ ìµœì‹  ë°ì´í„°ë¥¼ ë³´ë ¤ë©´ DB ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                
                # ì‚¬ìš©ìì—ê²Œ ì„ íƒê¶Œ ì œê³µ
                choice = input("\ní˜„ì¬ DBë¡œ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if choice not in ['y', 'yes']:
                    print("âŒ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    raise Exception("DB ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                
                print("âœ… ê¸°ì¡´ DBë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                return True
            else:
                print("âœ… DBê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
                return True
                
        except Exception as e:
            print(f"âš ï¸ DB ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ 'python cli.py catalog update' ëª…ë ¹ìœ¼ë¡œ DBë¥¼ ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return False

    def _build_catalog_db(self):
        """Catalog DB ê°•ì œ ì¬êµ¬ì¶•"""
        try:
            db_path = self.duckdb_path
            catalog_path = self.data_manager.catalog_path
            if not catalog_path.exists():
                print("âŒ Catalog ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
            
            # # ê¸°ì¡´ DB íŒŒì¼ ë°±ì—…
            # if db_path.exists():
            #     backup_path = db_path.with_suffix('.duckdb.backup')
            #     shutil.copy(db_path, backup_path)
            #     print(f"ğŸ’¾ ê¸°ì¡´ DB ë°±ì—…: {backup_path}")
            
            with DuckDBClient(str(db_path), read_only=False) as duck_client:
                # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ (ìˆë‹¤ë©´)
                try:
                    duck_client.execute_query("DROP TABLE IF EXISTS catalog")
                except:
                    pass
                
                # ìƒˆë¡œ ìƒì„±
                duck_client.create_table_from_parquet(
                    "catalog",
                    str(catalog_path / "**" / "*.parquet"),
                    hive_partitioning=True,
                    union_by_name=True
                )
                
                # ê²°ê³¼ í™•ì¸
                count_result = duck_client.execute_query("SELECT COUNT(*) as total FROM catalog")
                total_rows = count_result['total'].iloc[0]
                
                partitions_df = duck_client.retrieve_partitions("catalog")
                total_partitions = len(partitions_df)
                
                print(f"âœ… Catalog DB ì¬êµ¬ì¶• ì™„ë£Œ!")
                print(f"ğŸ“Š ì´ {total_rows:,}ê°œ í–‰, {total_partitions}ê°œ íŒŒí‹°ì…˜")
                print(f"ğŸ’¾ DB íŒŒì¼: {db_path}")
                print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {db_path.stat().st_size / 1024 / 1024:.1f}MB")
                
                return True
                
        except Exception as e:
            print(f"âŒ DB ì¬êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False
        
    def _perform_search(self, duck_client, partitions_df):
        """ê²€ìƒ‰ ìˆ˜í–‰"""
        print("\nğŸ” ê²€ìƒ‰ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("  1. íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰ (Provider/Dataset/Task/Variant)")
        print("  2. í…ìŠ¤íŠ¸ ê²€ìƒ‰ (JSON ë¼ë²¨ ë‚´ í…ìŠ¤íŠ¸)")

        search_choice = input("ê²€ìƒ‰ ë°©ë²• (1-2) [1]: ").strip() or "1"
        
        if search_choice == "1":
            search_results = self._partition_search(duck_client, partitions_df)
            print("\nğŸ“Š íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼:")
            print(search_results.head(3).to_string(index=False, max_cols=5))
            return search_results
        elif search_choice == "2":
            return self._text_search(duck_client)
        else:
            raise CatalogError("ì˜ëª»ëœ ê²€ìƒ‰ ë°©ë²•ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
    
    def _partition_search(self, duck_client: DuckDBClient, partitions_df: pd.DataFrame):
        """íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰"""
        
        # 1. Provider ì„ íƒ
        providers = self._select_items(
            items=sorted(partitions_df['provider'].unique().tolist()),
            name="Provider",
            partitions_df=partitions_df,
            column='provider',
            level="task",
        )
        if not providers:
            return None
        
        # 2. Dataset ì„ íƒ (í•„í„°ë§ëœ ê²°ê³¼ì—ì„œ)
        filtered_df = partitions_df[partitions_df['provider'].isin(providers)]
        datasets = self._select_items(
            items=sorted(filtered_df['dataset'].unique().tolist()),
            name="Dataset",
            partitions_df=filtered_df,
            column='dataset',
            level="task",
        )
        if not datasets:
            return None
        
        # 3. Task ì„ íƒ
        filtered_df = filtered_df[filtered_df['dataset'].isin(datasets)]
        tasks = self._select_items(
            items=sorted(filtered_df['task'].unique().tolist()),
            name="Task",
            partitions_df=filtered_df,
            column='task',
            level='variant',
        )
        if not tasks:
            return None
        
        # 4. Variant ì„ íƒ
        filtered_df = filtered_df[filtered_df['task'].isin(tasks)]
        variants = self._select_items(
            items=sorted(filtered_df['variant'].unique().tolist()),
            name="Variant",
            partitions_df=filtered_df,
            column='variant',
            level="dataset"
        )
        if not variants:
            return None
        
        # 5. ê²€ìƒ‰ ì‹¤í–‰
        print(f"\nğŸ” ê²€ìƒ‰ ì‹¤í–‰:")
        print(f"  Provider: {providers}")
        print(f"  Dataset: {datasets}")
        print(f"  Task: {tasks}")
        print(f"  Variant: {variants}")
        
        return duck_client.retrieve_with_existing_cols(
            providers=providers,
            datasets=datasets,
            tasks=tasks,
            variants=variants,
            table="catalog"
        )

    def _select_items(self, items, name, partitions_df, column, level):
        """ì•„ì´í…œ ë‹¤ì¤‘ ì„ íƒ"""
        if not items:
            print(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ {name}ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        self._show_matrix(partitions_df, column, level)
                
        print(f"\n{self._get_icon(name)} {name} ì„ íƒ ({len(items)}ê°œ):")
        for i, item in enumerate(items, 1):
            count = len(partitions_df[partitions_df[column] == item])
            print(f"  {i:2d}. {item} ({count}ê°œ)")
            
        print(f"\nì„ íƒ: ë²ˆí˜¸(1,2,3), ë²”ìœ„(1-5), ì´ë¦„, ì „ì²´(Enter)")
        user_input = input(f"{name}: ").strip()
        
        # ì „ì²´ ì„ íƒ
        if not user_input:
            print(f"âœ… ì „ì²´ ì„ íƒ ({len(items)}ê°œ)")
            return items
        
        # ì„ íƒ íŒŒì‹±
        selected = self._parse_input(user_input, items)
        
        if selected:
            print(f"âœ… {len(selected)}ê°œ ì„ íƒ: {selected}")
            return selected
        else:
            print(f"âŒ ì˜ëª»ëœ ì…ë ¥")
            return None

    def _parse_input(self, user_input, items):
        """ì…ë ¥ íŒŒì‹±"""
        selected = set()
        parts = user_input.split(',')
        
        for part in parts:
            part = part.strip()
            
            if '-' in part and not part.startswith('-'):
                # ë²”ìœ„: 1-5
                try:
                    start, end = part.split('-', 1)
                    start_idx = int(start) - 1
                    end_idx = int(end) - 1
                    
                    if 0 <= start_idx < len(items) and 0 <= end_idx < len(items):
                        for i in range(min(start_idx, end_idx), max(start_idx, end_idx) + 1):
                            selected.add(items[i])
                except ValueError:
                    print(f"âš ï¸ ì˜ëª»ëœ ë²”ìœ„: {part}")
                    
            elif part.isdigit():
                # ë²ˆí˜¸: 1, 2, 3
                idx = int(part) - 1
                if 0 <= idx < len(items):
                    selected.add(items[idx])
                else:
                    print(f"âš ï¸ ì˜ëª»ëœ ë²ˆí˜¸: {part}")
                    
            else:
                # ì´ë¦„: imagenet, coco
                if part in items:
                    selected.add(part)
                else:
                    print(f"âš ï¸ ì°¾ì„ ìˆ˜ ì—†ìŒ: {part}")
        
        return list(selected) if selected else None

    def _get_icon(self, name):
        """ì•„ì´ì½˜ ë°˜í™˜"""
        icons = {
            "Provider": "ğŸ¢",
            "Dataset": "ğŸ“¦", 
            "Task": "ğŸ“",
            "Variant": "ğŸ·ï¸"
        }
        return icons.get(name, "ğŸ“‹")

    def _show_matrix(self, partitions_df, level1, level2):
        print(f"\nğŸ“Š {level1.title()}-{level2.title()} ì¡°í•© ë§¤íŠ¸ë¦­ìŠ¤:")
        
        items1 = sorted(partitions_df[level1].unique())
        items2 = sorted(partitions_df[level2].unique())
        
        # í—¤ë” (ì²« ë²ˆì§¸ ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •)
        col_width = max(len(level1.title()), 15)
        print(level1.title().ljust(col_width), end=" | ")
        for item2 in items2:
            print(item2[:8].ljust(8), end=" | ")
        print()
        
        # êµ¬ë¶„ì„ 
        print("-" * (col_width + len(items2) * 11))
        
        # ë°ì´í„° í–‰
        for idx, item1 in enumerate(items1):
            data1 = partitions_df[partitions_df[level1] == item1]
            #print(item1[:col_width-1].ljust(col_width), end=" | ")
            item_name = f"{idx+1:>2}. {item1}"
            print(f"{item_name[:col_width-1].ljust(col_width)}", end=" | ")
            
            for item2 in items2:
                data12 = data1[data1[level2] == item2]
                count = len(data12) if not data12.empty else 0
                
                if count > 0:
                    print(f"{count:>3}".ljust(8), end=" | ")
                else:
                    print(" - ".ljust(8), end=" | ")
            print()
        
        print(f"ğŸ’¡ ìˆ«ì: íŒŒí‹°ì…˜ ìˆ˜, '-': ì¡°í•© ì—†ìŒ")


    def _text_search(self, duck_client: DuckDBClient):
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰"""
        print("\nğŸ”¤ í…ìŠ¤íŠ¸ ê²€ìƒ‰:")
        
        search_text = input("ê²€ìƒ‰í•  í…ìŠ¤íŠ¸: ").strip()
        if not search_text:
            print("âŒ ê²€ìƒ‰ í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return None
        
        columns_df = duck_client.get_table_info("catalog")
        columns = columns_df['column_name'].tolist()
        # ì»¬ëŸ¼ ì„ íƒ
        print(f"\nğŸ“ ì»¬ëŸ¼ ì„ íƒ:")
        for i, col in enumerate(columns, 1):
            print(f"  {i}. {col}")
        
        col_choice = input(f"ì»¬ëŸ¼ ì„ íƒ (1-{len(columns)}) [1]: ").strip() or "1"
        if col_choice.isdigit():
            idx = int(col_choice) - 1
            if 0 <= idx < len(columns):
                selected_column = columns[idx]
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                return None
        else:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")
            return None
            
        # ğŸ†• ê²€ìƒ‰ ë°©ë²• ì„ íƒ
        print(f"\nğŸ” '{selected_column}' ì»¬ëŸ¼ì—ì„œ ê²€ìƒ‰ ë°©ë²•:")
        print("  1. ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (LIKE)")
        print("  2. JSON íŒŒì‹± í›„ ê²€ìƒ‰")
        
        method_choice = input("ê²€ìƒ‰ ë°©ë²• (1-2) [1]: ").strip() or "1"
        
        if method_choice == "1":
            # ë‹¨ìˆœ LIKE ê²€ìƒ‰
            print(f"\nğŸ” ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰:")
            print(f"  í…ìŠ¤íŠ¸: '{search_text}'")
            print(f"  ì»¬ëŸ¼: {selected_column}")
            
            sql = duck_client.json_queries.search_text_in_column(
                table="catalog",
                column=selected_column,
                search_text=search_text,
                search_type="simple",
                engine="duckdb"
            )
            return duck_client.execute_query(sql)
            
        elif method_choice == "2":
            # JSON íŒŒì‹± ê²€ìƒ‰
            json_path = input("JSON ê²½ë¡œ (ì˜ˆ: $.image.text.content): ").strip()
            if not json_path:
                print("âŒ JSON ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return None
            
            # Variant ì„ íƒ (JSON ê²€ìƒ‰ì‹œì—ë§Œ)
            partitions_df = duck_client.retrieve_partitions("catalog")
            variants = sorted(partitions_df['variant'].unique().tolist())
            
            print(f"\nğŸ·ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ Variant ({len(variants)}ê°œ):")
            for i, variant in enumerate(variants, 1):
                count = len(partitions_df[partitions_df['variant'] == variant])
                print(f"  {i}. {variant} ({count}ê°œ íŒŒí‹°ì…˜)")
            
            variant_choice = input(f"Variant ì„ íƒ (1-{len(variants)}) [1]: ").strip() or "1"
            if variant_choice.isdigit():
                idx = int(variant_choice) - 1
                if 0 <= idx < len(variants):
                    selected_variant = variants[idx]
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return None
            else:
                if variant_choice in variants:
                    selected_variant = variant_choice
                else:
                    print(f"âŒ Variant '{variant_choice}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return None
            
            print(f"\nğŸ” JSON íŒŒì‹± ê²€ìƒ‰ ì‹¤í–‰:")
            print(f"  í…ìŠ¤íŠ¸: '{search_text}'")
            print(f"  ì»¬ëŸ¼: {selected_column}")
            print(f"  JSON ê²½ë¡œ: {json_path}")
            print(f"  Variant: {selected_variant}")
            
            # Variant ì¡°ê±´ ì¶”ê°€
            partition_conditions = {"variant": selected_variant}
            
            sql = duck_client.json_queries.search_text_in_column(
                table="catalog",
                column=selected_column,
                search_text=search_text,
                search_type="json",
                json_loc=json_path,
                partition_conditions=partition_conditions,
                engine="duckdb"
            )
            return duck_client.execute_query(sql)
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return None

    def _download_options(self, search_results):
        """ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ì„ íƒ ë° ì‹¤í–‰"""
        print("\nğŸ’¾ ë‹¤ìš´ë¡œë“œ ì˜µì…˜:")
        print("  1. ë©”íƒ€ë°ì´í„°ë§Œ (Parquet)")
        print("  2. ë©”íƒ€ë°ì´í„°ë§Œ (Arrow Dataset)")
        print("  3. ë©”íƒ€ë°ì´í„° + ì´ë¯¸ì§€ (Dataset format)")
        
        download_choice = input("ë‹¤ìš´ë¡œë“œ ì˜µì…˜ (1-3) [1]: ").strip() or "1"
        
        # ì €ì¥ ê²½ë¡œ ì…ë ¥
        default_path = f"./downloads/export_{len(search_results)}_items"
        save_path = input(f"ì €ì¥ ê²½ë¡œ [{default_path}]: ").strip() or default_path
        save_path = Path(save_path)
        
        try:
            if download_choice == "1":
                # Parquet ì €ì¥
                parquet_path = save_path.with_suffix('.parquet')
                parquet_path.parent.mkdir(parents=True, exist_ok=True)
                search_results.to_parquet(parquet_path, index=False)
                print(f"âœ… Parquet ì €ì¥ ì™„ë£Œ: {parquet_path}")
                print(f"ğŸ“Š {len(search_results):,}ê°œ í•­ëª©, {parquet_path.stat().st_size / 1024 / 1024:.1f}MB")
                
            elif download_choice == "2":
                # Arrow Dataset ì €ì¥
                return self._save_as_dataset(search_results, save_path, include_images=False)
                
            elif download_choice == "3":
                # Dataset + ì´ë¯¸ì§€ ì €ì¥
                return self._save_as_dataset(search_results, save_path, include_images=True)
                
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
                return False
                
            return True
            
        except Exception as e:
            print(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
        
    def _save_as_dataset(self, search_results, save_path, include_images=False):
        """datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Dataset í˜•íƒœë¡œ ì €ì¥"""
        try:
            save_path = Path(save_path)
            save_path.mkdir(parents=True, exist_ok=True)
            
            if include_images:
                print(f"\nğŸ“¥ ì´ë¯¸ì§€ í¬í•¨ Dataset ìƒì„± ì¤‘...")
                path_column = 'path'

                if path_column not in search_results.columns:
                    print("âŒ ì´ë¯¸ì§€ ê²½ë¡œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                            
                def load_and_validate_image(example):
                    """ì´ë¯¸ì§€ ë¡œë“œ ë° ìœ íš¨ì„± ê²€ì‚¬"""
                    try:
                        if example[path_column] and pd.notna(example[path_column]):
                            image_path = self.data_manager.assets_path / example[path_column]
                            if image_path.exists():
                                pil_image = Image.open(image_path)
                                pil_image.verify()  # ì†ìƒëœ ì´ë¯¸ì§€ ì²´í¬
                                pil_image = Image.open(image_path) 
                                
                                example['image'] = pil_image
                                example['has_valid_image'] = True
                                return example
                            else:
                                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {example[path_column]}")
                        else:
                            print(f"âš ï¸ ê²½ë¡œ ì—†ìŒ: {example.get('hash', 'unknown')}")
                    except Exception as e:
                        print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {example.get(path_column, 'unknown')} - {e}")
                    
                    # ì‹¤íŒ¨í•œ ê²½ìš°
                    example['image'] = None
                    example['has_valid_image'] = False
                    return example

                # DataFrameì„ Datasetìœ¼ë¡œ ë³€í™˜
                dataset = Dataset.from_pandas(search_results)

                # ì´ë¯¸ì§€ ë¡œë“œ (ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬)
                print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ê²€ì¦ ë° ë¡œë”© ì¤‘...")
                dataset_with_validation = dataset.map(
                    load_and_validate_image,
                    desc="ì´ë¯¸ì§€ ê²€ì¦",
                    num_proc=self.data_manager.num_proc,
                )

                # ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
                print("ğŸ” ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§ ì¤‘...")
                valid_dataset = dataset_with_validation.filter(
                    lambda x: x,
                    desc="ìœ íš¨ ì´ë¯¸ì§€ í•„í„°ë§",
                    input_columns=['has_valid_image'],
                    num_proc=self.data_manager.num_proc
                )

                # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
                valid_dataset = valid_dataset.remove_columns(['has_valid_image'])

                # ê²°ê³¼ ì¶œë ¥
                total_items = len(dataset)
                valid_images = len(valid_dataset)
                filtered_out = total_items - valid_images

                print(f"ğŸ“Š ì´ë¯¸ì§€ ë¡œë”© ê²°ê³¼:")
                print(f"  ğŸ”¢ ì´ í•­ëª©: {total_items:,}ê°œ")
                print(f"  âœ… ìœ íš¨ ì´ë¯¸ì§€: {valid_images:,}ê°œ")
                print(f"  âŒ ì œì™¸ëœ í•­ëª©: {filtered_out:,}ê°œ")

                if filtered_out > 0:
                    success_rate = (valid_images / total_items) * 100
                    print(f"  ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
                    
                    # ì‚¬ìš©ìì—ê²Œ í™•ì¸
                    if filtered_out > total_items * 0.1:  # 10% ì´ìƒ ì‹¤íŒ¨
                        print(f"âš ï¸ ì£¼ì˜: {filtered_out}ê°œ í•­ëª©ì´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        continue_choice = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                        if continue_choice not in ['y', 'yes']:
                            return False

                print(f"\nğŸ’¾ Dataset ì €ì¥ ì¤‘...")
                valid_dataset.save_to_disk(str(save_path))

                print(f"âœ… Dataset ì €ì¥ ì™„ë£Œ: {save_path}")
                print(f"ğŸ“Š ìµœì¢… {valid_images:,}ê°œ í•­ëª© (ì´ë¯¸ì§€ í¬í•¨)")
                print(f"ğŸ’¾ ì´ í¬ê¸°: {sum(f.stat().st_size for f in save_path.rglob('*') if f.is_file()) / 1024 / 1024:.1f}MB")

                # ì‚¬ìš©ë²• ì•ˆë‚´
                print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
                print(f"```python")
                print(f"from datasets import load_from_disk")
                print(f"dataset = load_from_disk('{save_path}')")
                print(f"# ëª¨ë“  í•­ëª©ì— ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ìˆìŠµë‹ˆë‹¤")
                print(f"# dataset[0]['image'].show()")
                print(f"```")
                
            else:
                # ë©”íƒ€ë°ì´í„°ë§Œ Datasetìœ¼ë¡œ ì €ì¥
                print(f"\nğŸ“„ ë©”íƒ€ë°ì´í„° Dataset ìƒì„± ì¤‘...")
                
                # DataFrameì„ Datasetìœ¼ë¡œ ë³€í™˜
                dataset = Dataset.from_pandas(search_results)
                
                # Dataset ì €ì¥
                dataset.save_to_disk(str(save_path))
                
                print(f"âœ… Dataset ì €ì¥ ì™„ë£Œ: {save_path}")
                print(f"ğŸ“Š {len(dataset):,}ê°œ í•­ëª©")
                print(f"ğŸ’¾ í¬ê¸°: {sum(f.stat().st_size for f in save_path.rglob('*') if f.is_file()) / 1024 / 1024:.1f}MB")
                
                # ì‚¬ìš©ë²• ì•ˆë‚´
                print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
                print(f"```python")
                print(f"from datasets import load_from_disk")
                print(f"dataset = load_from_disk('{save_path}')")
                print(f"df = dataset.to_pandas()  # pandasë¡œ ë³€í™˜")
                print(f"```")
            
            return True
            
        except ImportError:
            print("âŒ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹: pip install datasets")
            return False
        except Exception as e:
            print(f"âŒ Dataset ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(
        description="ğŸ“Š Data Manager CLI - ë°ì´í„° ì—…ë¡œë“œ/ì²˜ë¦¬/ë‹¤ìš´ë¡œë“œ ê´€ë¦¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:

ğŸ”§ ì„¤ì • ê´€ë¦¬:
  python cli.py config                         # ì„¤ì • ë„ì›€ë§
  python cli.py config list                    # ì „ì²´ ì„¤ì • í™•ì¸
  python cli.py config provider               # Provider ê´€ë¦¬ ë„ì›€ë§
  python cli.py config provider create        # Provider ìƒì„±
  python cli.py config provider list          # Provider ëª©ë¡
  python cli.py config provider remove        # Provider ì œê±°
  python cli.py config task                   # Task ê´€ë¦¬ ë„ì›€ë§
  python cli.py config task create            # Task ìƒì„±
  python cli.py config task list              # Task ëª©ë¡
  python cli.py config task remove            # Task ì œê±°

ğŸ“¥ ë°ì´í„° ê´€ë¦¬:
  python cli.py upload                         # ë°ì´í„° ì—…ë¡œë“œ
  python cli.py download                       # ë°ì´í„° ë‹¤ìš´ë¡œë“œ

  ë‹¤ìš´ë¡œë“œ í¬ë§·:
    1. Parquet (ë©”íƒ€ë°ì´í„°ë§Œ)
    2. Arrow Dataset (ë©”íƒ€ë°ì´í„°ë§Œ) 
    3. Dataset + ì´ë¯¸ì§€ (HuggingFace datasets í˜•íƒœ)
    
ğŸ”„ ì²˜ë¦¬ ê´€ë¦¬:
  python cli.py process                        # ì²˜ë¦¬ ì‹œì‘ 
  python cli.py process start                  # ìƒˆ ì²˜ë¦¬ ì‹œì‘
  python cli.py process status JOB_ID          # ì‘ì—… ìƒíƒœ í™•ì¸
  python cli.py process list                   # ë‚´ ë°ì´í„° í˜„í™©

ğŸ“Š Catalog DB ê´€ë¦¬:
  python cli.py catalog info                   # Catalog DB ì •ë³´ í™•ì¸
  python cli.py catalog rebuild                # Catalog DB ê°•ì œ ì¬êµ¬ì¶•
  python cli.py catalog update                 # Catalog DB ì—…ë°ì´íŠ¸ 
  
ğŸ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬:
  python cli.py validate                       # ì „ì²´ ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
  python cli.py validate --provider=NAME       # íŠ¹ì • Providerë§Œ ê²€ì‚¬
  python cli.py validate --fix                 # ë¬¸ì œ ìë™ ìˆ˜ì •
  python cli.py validate --report              # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±

ğŸ’¡ íŒ: Dataset í˜•íƒœë¡œ ì €ì¥í•˜ë©´ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     from datasets import load_from_disk
     dataset = load_from_disk('./downloads/my_dataset')
        """
    )
    parser.add_argument("--base-path", default="/mnt/AI_NAS/datalake",
                       help="ë°ì´í„° ì €ì¥ ê¸°ë³¸ ê²½ë¡œ")
    parser.add_argument("--nas-url", default="http://192.168.20.62:8091", 
                       help="NAS API ì„œë²„ URL")
    parser.add_argument("--log-level", default="INFO",
                       help="ë¡œê¹… ë ˆë²¨ (DEBUG, INFO, WARNING, ERROR, CRITICAL)")
    parser.add_argument("--num-proc", type=int, default=8,
                       help="ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜")
    
    subparsers = parser.add_subparsers(dest='command', help='ëª…ë ¹ì–´')
    
    
    # Config ê´€ë¦¬ (Provider + Task)
    config_parser = subparsers.add_parser('config', help='ì„¤ì • ê´€ë¦¬ (Provider, Task)')
    config_subparsers = config_parser.add_subparsers(dest='config_type')
    
    # Provider ê´€ë¦¬
    provider_parser = config_subparsers.add_parser('provider', help='Provider ê´€ë¦¬')
    provider_subparsers = provider_parser.add_subparsers(dest='provider_action')
    provider_subparsers.add_parser('create', help='ìƒˆ Provider ìƒì„±')
    provider_subparsers.add_parser('remove', help='Provider ì œê±°')
    provider_subparsers.add_parser('list', help='Provider ëª©ë¡')
    
    # Task ê´€ë¦¬
    task_parser = config_subparsers.add_parser('task', help='Task ê´€ë¦¬')
    task_subparsers = task_parser.add_subparsers(dest='task_action')
    task_subparsers.add_parser('create', help='ìƒˆ Task ìƒì„±')
    task_subparsers.add_parser('remove', help='Task ì œê±°')
    task_subparsers.add_parser('list', help='Task ëª©ë¡')
    
    # Config ì „ì²´ ëª©ë¡
    config_subparsers.add_parser('list', help='ì „ì²´ ì„¤ì • ëª©ë¡')
    
    # ë°ì´í„° ì—…ë¡œë“œ
    subparsers.add_parser('upload', help='ë°ì´í„° ì—…ë¡œë“œ')
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    subparsers.add_parser('download', help='ë°ì´í„° ë‹¤ìš´ë¡œë“œ')
    
    
    # ì²˜ë¦¬ ê´€ë¦¬
    process_parser = subparsers.add_parser('process', help='ë°ì´í„° ì²˜ë¦¬ ê´€ë¦¬')
    process_subparsers = process_parser.add_subparsers(dest='process_action')
    process_subparsers.add_parser('start', help='ìƒˆ ì²˜ë¦¬ ì‹œì‘')
    process_subparsers.add_parser('list', help='ë‚´ ë°ì´í„° ì „ì²´ í˜„í™© í™•ì¸')
    job_status_parser = process_subparsers.add_parser('status', help='íŠ¹ì • ì‘ì—… ìƒíƒœ í™•ì¸')
    job_status_parser.add_argument('job_id', help='ì‘ì—… ID')
    
    # Catalog DB ê´€ë¦¬
    catalog_parser = subparsers.add_parser('catalog', help='Catalog DB ê´€ë¦¬')
    catalog_subparsers = catalog_parser.add_subparsers(dest='catalog_action')
    catalog_subparsers.add_parser('info', help='Catalog DB ì •ë³´ í™•ì¸')
    catalog_subparsers.add_parser('rebuild', help='Catalog DB ê°•ì œ ì¬êµ¬ì¶•')
    catalog_subparsers.add_parser('check', help='Catalog ë¹ ë¥¸ ìƒíƒœ í™•ì¸')
    catalog_subparsers.add_parser('update', help='Catalog DB ì•ˆì „ ì—…ë°ì´íŠ¸')
    catalog_subparsers.add_parser('processes', help='DB ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ í™•ì¸')
    
    # ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
    validate_parser = subparsers.add_parser('validate', help='Catalog DB ìƒíƒœ ê²€ì‚¬ ë° ë¬¸ì œ í•´ê²°')
    validate_parser.add_argument('--provider', type=str, help='íŠ¹ì • Providerë§Œ ê²€ì‚¬')
    validate_parser.add_argument('--fix', action='store_true', help='ë¬¸ì œ ìë™ ìˆ˜ì •')
    validate_parser.add_argument('--report', action='store_true', help='ê²€ì‚¬ ë³´ê³ ì„œ ìƒì„±')
    validate_parser.add_argument('--generate-report', action='store_true', help='ê²€ì‚¬ í›„ ë³´ê³ ì„œ ìƒì„±')
    # ìƒíƒœ í™•ì¸
    
    args = parser.parse_args()
    if not args.command:
        print("\nğŸš€ Data Manager CLIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
        print("="*60)
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì£¼ìš” ëª…ë ¹ì–´:")
        print("  ğŸ”§ python cli.py config     - ì„¤ì • ê´€ë¦¬ (Provider, Task)")
        print("  ğŸ“¥ python cli.py upload     - ë°ì´í„° ì—…ë¡œë“œ")
        print("  ğŸ“¤ python cli.py download   - ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        print("  ğŸ”„ python cli.py process    - ë°ì´í„° ì²˜ë¦¬")
        print("  ğŸ“Š python cli.py catalog    - Catalog DB ê´€ë¦¬")
        print("  ğŸ” python cli.py validate   - ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬")
        
        print("\nğŸŒŸ ì²˜ìŒ ì‚¬ìš©í•˜ì‹œë‚˜ìš”? ë‹¤ìŒ ìˆœì„œë¡œ ì‹œì‘í•´ë³´ì„¸ìš”:")
        print(" 1ï¸âƒ£  python cli.py config provider create  # ë°ì´í„° ì œê³µì ìƒì„±")
        print(" 2ï¸âƒ£  python cli.py config task create      # ì‘ì—… ìœ í˜• ì •ì˜")
        print(" 3ï¸âƒ£  python cli.py upload                  # ë°ì´í„° ì—…ë¡œë“œ")
        print(" 4ï¸âƒ£  python cli.py process                 # ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        print("\n ğŸ’¡ ë°ì´í„° ë‹¤ìš´ë¡œë“œëŠ” 'python cli.py download' ëª…ë ¹ìœ¼ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        print(" 1ï¸âƒ£  python cli.py catalog update         # Catalog DB êµ¬ì¶•")
        print(" 2ï¸âƒ£  python cli.py download                # ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        print("      â†’ ì˜µì…˜ 1: Parquet (ë©”íƒ€ë°ì´í„°ë§Œ)")
        print("      â†’ ì˜µì…˜ 2: Arrow Dataset (ë©”íƒ€ë°ì´í„°ë§Œ)")  
        print("      â†’ ì˜µì…˜ 3: Dataset + ì´ë¯¸ì§€ (HuggingFace í˜•íƒœ)")

        print("\nğŸ” ë°ì´í„° ê´€ë¦¬ ë° ë¬¸ì œ í•´ê²°:")
        print("  ğŸ“Š python cli.py catalog check            # ë¹ ë¥¸ ìƒíƒœ í™•ì¸")
        print("  ğŸ” python cli.py validate                 # ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬")
        

        print("\nğŸ’¡ ê° ëª…ë ¹ì–´ ë’¤ì— -h ë˜ëŠ” --helpë¥¼ ë¶™ì´ë©´ ìƒì„¸ ë„ì›€ë§ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   ì˜ˆ: python cli.py config -h")
        print("\nğŸ”¥ Dataset í˜•íƒœë¡œ ì €ì¥í•˜ë©´ ML ì‘ì—…ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”!")
        print("   from datasets import load_from_disk")
        print("   dataset = load_from_disk('./downloads/my_dataset')")
        print("\n" + "="*60)
        return

    
    # CLI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    cli = DataManagerCLI(
        base_path=args.base_path,
        nas_api_url=args.nas_url,
        log_level=args.log_level,
        num_proc=args.num_proc
    )
    
    try:
        if args.command == 'config':
            if not args.config_type:
                print("\nâ“ config í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                print("  ğŸ“‹ python cli.py config list      - ì „ì²´ ì„¤ì • í™•ì¸")
                print("  ğŸ¢ python cli.py config provider  - Provider ê´€ë¦¬")
                print("  ğŸ“ python cli.py config task      - Task ê´€ë¦¬")
                print("\nğŸ’¡ ì²˜ìŒ ì‚¬ìš©í•˜ì‹œë‚˜ìš”? ë‹¤ìŒ ìˆœì„œë¡œ ì‹œì‘í•´ë³´ì„¸ìš”:")
                print(" 1ï¸âƒ£  python cli.py config provider create  # Provider ìƒì„±")
                print(" 2ï¸âƒ£  python cli.py config task create      # Task ìƒì„±")
                print(" 3ï¸âƒ£  python cli.py upload                  # ë°ì´í„° ì—…ë¡œë“œ")
                print(" 4ï¸âƒ£  python cli.py process                 # ì²˜ë¦¬ ì‹œì‘")
                return
                
            if args.config_type == 'provider':
                if not args.provider_action:
                    print("\nâ“ provider í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                    print("  ğŸ“‹ python cli.py config provider list    - Provider ëª©ë¡")
                    print("  â• python cli.py config provider create  - Provider ìƒì„±")
                    print("  ğŸ—‘ï¸  python cli.py config provider remove  - Provider ì œê±°")
                    return
                    
                if args.provider_action == 'create':
                    cli.create_provider_interactive()
                elif args.provider_action == 'remove':
                    cli.remove_provider_interactive()
                elif args.provider_action == 'list':
                    providers = cli.schema_manager.get_all_providers()
                    print(f"\nğŸ¢ ë“±ë¡ëœ Provider ({len(providers)}ê°œ):")
                    if providers:
                        for provider in providers:
                            print(f"  â€¢ {provider}")
                    else:
                        print("  ğŸ“­ ë“±ë¡ëœ Providerê°€ ì—†ìŠµë‹ˆë‹¤.")
                        print("  ğŸ’¡ 'python cli.py config provider create' ëª…ë ¹ìœ¼ë¡œ Providerë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            
            elif args.config_type == 'task':
                if not args.task_action:
                    print("\nâ“ task í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                    print("  ğŸ“‹ python cli.py config task list    - Task ëª©ë¡")
                    print("  â• python cli.py config task create  - Task ìƒì„±")
                    print("  ğŸ—‘ï¸  python cli.py config task remove  - Task ì œê±°")
                    return
                    
                if args.task_action == 'create':
                    cli.create_task_interactive()
                elif args.task_action == 'remove':
                    cli.remove_task_interactive()
                elif args.task_action == 'list':
                    tasks = cli.schema_manager.get_all_tasks()
                    print(f"\nğŸ“ ë“±ë¡ëœ Task ({len(tasks)}ê°œ):")
                    if tasks:
                        for task_name, task_config in tasks.items():
                            print(f"  â€¢ {task_name}")
                            required_fields = task_config.get('required_fields', [])
                            if required_fields:
                                print(f"    ğŸ“ í•„ìˆ˜ í•„ë“œ: {', '.join(required_fields)}")
                            allowed_values = task_config.get('allowed_values', {})
                            if allowed_values:
                                print(f"    ğŸ”§ í—ˆìš© ê°’:")
                                for field, values in allowed_values.items():
                                    print(f"      - {field}: {', '.join(values)}")
                    else:
                        print("  ğŸ“­ ë“±ë¡ëœ Taskê°€ ì—†ìŠµë‹ˆë‹¤.")
                        print("  ğŸ’¡ 'python cli.py config task create' ëª…ë ¹ìœ¼ë¡œ Taskë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            
            elif args.config_type == 'list':
                cli.schema_manager.show_schema_info()
        
        elif args.command == 'upload':
            cli.upload_data_interactive()
        elif args.command == 'download':
            cli.download_data_interactive()
        elif args.command == 'process':
            if not args.process_action:
                print("\nâ“ process í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                print("  ğŸš€ python cli.py process start           - ìƒˆ ì²˜ë¦¬ ì‹œì‘")
                print("  ğŸ” python cli.py process status JOB_ID   - ì‘ì—… ìƒíƒœ í™•ì¸")
                print("  ğŸ“‹ python cli.py process list            - ë‚´ ë°ì´í„° í˜„í™©")
                return
                
            if args.process_action == 'start':
                cli.trigger_processing()
            elif args.process_action == 'status':
                cli.check_job_status(args.job_id)
            elif args.process_action == 'list':
                cli.list_all_data()
        
        elif args.command == 'catalog':
            if not args.catalog_action:
                print("\nâ“ catalog í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                print("  ğŸ“Š python cli.py catalog info     - Catalog DB ìƒì„¸ ì •ë³´")
                print("  ğŸ” python cli.py catalog check    - Catalog ë¹ ë¥¸ ìƒíƒœ í™•ì¸")
                print("  ğŸ”„ python cli.py catalog update   - Catalog DB ì•ˆì „ ì—…ë°ì´íŠ¸")
                print("  ğŸ” python cli.py catalog processes - DB ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ í™•ì¸")
                return
                
            if args.catalog_action == 'info':
                cli.show_catalog_db_info()
            elif args.catalog_action == 'check':  # ìƒˆë¡œ ì¶”ê°€
                cli.quick_catalog_check()
            elif args.catalog_action == 'update':  # ìƒˆë¡œ ì¶”ê°€
                cli.safe_update_catalog_db()
            elif args.catalog_action == 'processes':  # ìƒˆë¡œ ì¶”ê°€
                cli.check_db_processes() 
        elif args.command == 'validate':
            # ë§¤ê°œë³€ìˆ˜ í™•ì¸ ë° ì •ë¦¬
            provider = getattr(args, 'provider', None)
            fix_issues = getattr(args, 'fix', False)
            generate_report = getattr(args, 'generate_report', False)
            
            if provider:
                print(f"ğŸ¢ ê²€ì‚¬ ëŒ€ìƒ: Provider '{provider}'")
            if fix_issues:
                # êµ¬í˜„ì•ˆëŒ
                print("ğŸ”§ ë¬¸ì œ ìë™ ìˆ˜ì • ëª¨ë“œ í™œì„±í™”")
                raise NotImplementedError("ë¬¸ì œ ìë™ ìˆ˜ì • ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            if generate_report:
                print("ğŸ“„ ë³´ê³ ì„œ ìƒì„± ëª¨ë“œ í™œì„±í™”")
            
            # ê²€ì‚¬ ì‹¤í–‰
            success = cli.validate_data_integrity(
                provider=provider,
                generate_report=generate_report
            )
            
            if not success:
                print("\nâŒ ê²€ì‚¬ ì¤‘ ì¤‘ìš”í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ 'python cli.py validate --fix' ëª…ë ¹ìœ¼ë¡œ ìë™ ìˆ˜ì •ì„ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                return 1
            else:
                print("\nâœ… ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì™„ë£Œ!")
                if fix_issues:
                    print("ğŸ”§ ë°œê²¬ëœ ë¬¸ì œë“¤ì´ ìë™ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                if generate_report:
                    print("ğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return 0

            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì–¸ì œë“ ì§€ ë‹¤ì‹œ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("ğŸ’¡ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except ValueError as e:
        print(f"âŒ ì…ë ¥ ê°’ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ ì…ë ¥ ê°’ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except ConnectionError as e:
        print(f"âŒ ì—°ê²° ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ NAS ì„œë²„ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except CatalogNotFoundError as e:
        print(f"âŒ {e}")
    except CatalogEmptyError as e:
        print(f"âŒ {e}")
    except CatalogLockError as e:
        print(f"âŒ {e}")
        print("ğŸ’¡ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    except CatalogError as e:
        print(f"âŒ {e}")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()