import argparse
import sys
import json
import shutil
import pandas as pd

from datasets import Dataset
from PIL import Image
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))  # ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€
from managers.datalake_client import DatalakeClient
from client.src.core.duckdb_client import DuckDBClient

class DataManagerCLI:
    """Data Manager CLI ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(
        self, 
        base_path: str = "/mnt/AI_NAS/datalake",
        nas_api_url: str = "http://192.168.20.62:8091",
        log_level: str = "INFO"
    ):
        self.data_manager = DatalakeClient(
            base_path=base_path,
            nas_api_url=nas_api_url,
            log_level=log_level,
        )
        self.schema_manager = self.data_manager.schema_manager
    
    def _check_and_update_catalog_db(self, duck_client, catalog_path):
        """Catalog DB ìƒíƒœ í™•ì¸ ë° ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        try:
            # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            tables = duck_client.list_tables()
            if tables.empty or 'catalog' not in tables['name'].values:
                print("ğŸ“ ìƒˆë¡œìš´ Catalog DB ìƒì„± í•„ìš”")
                return True
            
            # DB íŒŒì¼ê³¼ Parquet íŒŒì¼ë“¤ì˜ ìˆ˜ì • ì‹œê°„ ë¹„êµ
            db_path = self.data_manager.base_path / "catalog.duckdb"
            db_mtime = db_path.stat().st_mtime if db_path.exists() else 0
            
            # ê°€ì¥ ìµœê·¼ Parquet íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ í™•ì¸
            latest_parquet_mtime = 0
            for parquet_file in catalog_path.rglob("*.parquet"):
                file_mtime = parquet_file.stat().st_mtime
                if file_mtime > latest_parquet_mtime:
                    latest_parquet_mtime = file_mtime
            
            if latest_parquet_mtime > db_mtime:
                print("ğŸ”„ Parquet íŒŒì¼ì´ DBë³´ë‹¤ ìµœì‹  â†’ ì—…ë°ì´íŠ¸ í•„ìš”")
                return True
            else:
                print("âœ… DBê°€ ìµœì‹  ìƒíƒœ")
                return False
                
        except Exception as e:
            print(f"âš ï¸ DB ìƒíƒœ í™•ì¸ ì‹¤íŒ¨, ì¬ìƒì„± ì§„í–‰: {e}")
            return True

    def rebuild_catalog_db(self):
        """Catalog DB ê°•ì œ ì¬êµ¬ì¶•"""
        print("\n" + "="*50)
        print("ğŸ”¨ Catalog DB ì¬êµ¬ì¶•")
        print("="*50)
        
        try:
            db_path = self.data_manager.base_path / "catalog.duckdb"
            catalog_path = self.data_manager.catalog_path
            
            if not catalog_path.exists():
                print("âŒ Catalog ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
            
            # ê¸°ì¡´ DB íŒŒì¼ ë°±ì—…
            if db_path.exists():
                backup_path = db_path.with_suffix('.duckdb.backup')
                shutil.copy2(db_path, backup_path)
                print(f"ğŸ’¾ ê¸°ì¡´ DB ë°±ì—…: {backup_path}")
            
            print("ğŸ”„ Catalog DB ì¬êµ¬ì¶• ì¤‘...")
            
            with DuckDBClient(str(db_path), read_only=True) as duck_client:
                # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ (ìˆë‹¤ë©´)
                try:
                    duck_client.execute_query("DROP TABLE IF EXISTS catalog")
                except:
                    pass
                
                # ìƒˆë¡œ ìƒì„±
                duck_client.create_table_from_parquet(
                    "catalog",
                    str(catalog_path / "**" / "*.parquet"),
                    hive_partitioning=True,
                    union_by_name=True
                )
                
                # ê²°ê³¼ í™•ì¸
                count_result = duck_client.execute_query("SELECT COUNT(*) as total FROM catalog")
                total_rows = count_result['total'].iloc[0]
                
                partitions_df = duck_client.retrieve_partitions("catalog")
                total_partitions = len(partitions_df)
                
                print(f"âœ… Catalog DB ì¬êµ¬ì¶• ì™„ë£Œ!")
                print(f"ğŸ“Š ì´ {total_rows:,}ê°œ í–‰, {total_partitions}ê°œ íŒŒí‹°ì…˜")
                print(f"ğŸ’¾ DB íŒŒì¼: {db_path}")
                print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {db_path.stat().st_size / 1024 / 1024:.1f}MB")
                
                return True
                
        except Exception as e:
            print(f"âŒ DB ì¬êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False
        
    def show_catalog_db_info(self):
        """Catalog DB ì •ë³´ í‘œì‹œ"""
        print("\nğŸ“Š Catalog DB ì •ë³´")
        print("="*50)
        
        try:
            db_path = self.data_manager.base_path / "catalog.duckdb"
            
            if not db_path.exists():
                print("âŒ Catalog DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                print(f"ğŸ’¡ 'python cli.py catalog rebuild' ëª…ë ¹ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                return False
            
            # DB ê¸°ë³¸ ì •ë³´
            db_size = db_path.stat().st_size / 1024 / 1024
            from datetime import datetime
            db_mtime = datetime.fromtimestamp(db_path.stat().st_mtime)
            
            print(f"ğŸ“ DB íŒŒì¼: {db_path}")
            print(f"ğŸ’¾ íŒŒì¼ í¬ê¸°: {db_size:.1f}MB")
            print(f"ğŸ•’ ìˆ˜ì • ì‹œê°„: {db_mtime.strftime('%Y-%m-%d %H:%M:%S')}")
            
            with DuckDBClient(str(db_path)) as duck_client:
                # í…Œì´ë¸” ì •ë³´
                tables = duck_client.list_tables()
                print(f"\nğŸ“‹ í…Œì´ë¸”: {len(tables)}ê°œ")
                for _, table in tables.iterrows():
                    print(f"  â€¢ {table['name']}")
                
                if 'catalog' in tables['name'].values:
                    # Catalog í…Œì´ë¸” ìƒì„¸ ì •ë³´
                    count_result = duck_client.execute_query("SELECT COUNT(*) as total FROM catalog")
                    total_rows = count_result['total'].iloc[0]
                    
                    partitions_df = duck_client.retrieve_partitions("catalog")
                    
                    print(f"\nğŸ“Š Catalog í…Œì´ë¸”:")
                    print(f"  ğŸ“ˆ ì´ í–‰ ìˆ˜: {total_rows:,}ê°œ")
                    print(f"  ğŸ·ï¸ íŒŒí‹°ì…˜: {len(partitions_df)}ê°œ")
                    
                    # ìƒìœ„ Providerë³„ í†µê³„
                    if not partitions_df.empty:
                        provider_stats = partitions_df.groupby('provider').size().sort_values(ascending=False)
                        print(f"\nğŸ¢ Providerë³„ íŒŒí‹°ì…˜ ìˆ˜:")
                        for provider, count in provider_stats.head(5).items():
                            print(f"  â€¢ {provider}: {count}ê°œ")
                        
                        if len(provider_stats) > 5:
                            print(f"  ... ì™¸ {len(provider_stats) - 5}ê°œ")
                
                return True
                
        except Exception as e:
            print(f"âŒ DB ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return False
        
    def create_provider_interactive(self):
        """ëŒ€í™”í˜• Provider ìƒì„±"""
        print("\n" + "="*50)
        print("ğŸ¢ ìƒˆ Provider ìƒì„±")
        print("="*50)
        
        try:
            # Provider ì´ë¦„ ì…ë ¥
            provider_name = input("ğŸ¢ Provider ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if not provider_name:
                print("âŒ Provider ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return False
            
            # ê¸°ì¡´ Provider í™•ì¸
            if provider_name in self.schema_manager.get_all_providers():
                print(f"âš ï¸ Provider '{provider_name}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                return False
            
            # í™•ì¸ ë° ìƒì„±
            confirm = input(f"\nProvider '{provider_name}'ë¥¼ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm in ['y', 'yes']:
                result = self.schema_manager.add_provider(provider_name)
                if result:
                    print(f"âœ… Provider '{provider_name}' ìƒì„± ì™„ë£Œ!")
                    return True
                else:
                    print(f"âŒ Provider ìƒì„± ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Provider ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def create_task_interactive(self):
        """ëŒ€í™”í˜• Task ìƒì„±"""
        print("\n" + "="*50)
        print("ğŸ”§ ìƒˆ Task ìƒì„±")
        print("="*50)
        
        try:
            # Task ì´ë¦„ ì…ë ¥
            task_name = input("ğŸ“ Task ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if not task_name:
                print("âŒ Task ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return False
            
            # ê¸°ì¡´ Task í™•ì¸
            existing_tasks = self.schema_manager.get_all_tasks()
            if task_name in existing_tasks:
                print(f"âš ï¸ Task '{task_name}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                update = input("ì—…ë°ì´íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if update not in ['y', 'yes']:
                    return False
            
            # í•„ìˆ˜ í•„ë“œ ì…ë ¥
            print("\nğŸ“ í•„ìˆ˜ í•„ë“œ ì„¤ì • (Enterë¡œ ì™„ë£Œ)")
            required_fields = []
            while True:
                field = input(f"í•„ìˆ˜ í•„ë“œ #{len(required_fields)+1}: ").strip()
                if not field:
                    break
                required_fields.append(field)
                print(f"  âœ… ì¶”ê°€ë¨: {field}")
            
            # í—ˆìš© ê°’ ì„¤ì •
            print("\nğŸ”§ í—ˆìš© ê°’ ì„¤ì •")
            allowed_values = {}
            for field in required_fields:
                values_input = input(f"{field}ì˜ í—ˆìš© ê°’ (ì‰¼í‘œë¡œ êµ¬ë¶„, ìƒëµ ê°€ëŠ¥): ").strip()
                if values_input:
                    values = [v.strip() for v in values_input.split(',') if v.strip()]
                    if values:
                        allowed_values[field] = values
                        print(f"  âœ… {field}: {values}")
            
            # í™•ì¸ ë° ìƒì„±
            print(f"\nğŸ“‹ Task ì„¤ì • í™•ì¸:")
            print(f"  ì´ë¦„: {task_name}")
            print(f"  í•„ìˆ˜ í•„ë“œ: {required_fields}")
            print(f"  í—ˆìš© ê°’: {allowed_values}")
            
            confirm = input("\nìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm in ['y', 'yes']:
                if task_name in existing_tasks:
                    result = self.schema_manager.update_task(task_name, required_fields, allowed_values)
                else:
                    result = self.schema_manager.add_task(task_name, required_fields, allowed_values)
                
                if result:
                    print(f"âœ… Task '{task_name}' ìƒì„±/ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                    return True
                else:
                    print(f"âŒ Task ìƒì„±/ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ìƒì„±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Task ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def remove_provider_interactive(self):
        """ëŒ€í™”í˜• Provider ì œê±°"""
        providers = self.schema_manager.get_all_providers()
        if not providers:
            print("âŒ ì œê±°í•  Providerê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print("\nğŸ¢ ë“±ë¡ëœ Provider:")
        for i, provider in enumerate(providers, 1):
            print(f"  {i}. {provider}")
        
        try:
            choice = input("\nì œê±°í•  Provider ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„: ").strip()
            if choice.isdigit():
                idx = int(choice) - 1
                if 0 <= idx < len(providers):
                    provider = providers[idx]
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return False
            else:
                provider = choice
                if provider not in providers:
                    print(f"âŒ Provider '{provider}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return False
            
            confirm = input(f"\nProvider '{provider}'ë¥¼ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm in ['y', 'yes']:
                result = self.schema_manager.remove_provider(provider)
                if result:
                    print(f"âœ… Provider '{provider}' ì œê±° ì™„ë£Œ!")
                    return True
                else:
                    print(f"âŒ Provider ì œê±° ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ì œê±°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ì œê±°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Provider ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def remove_task_interactive(self):
        """ëŒ€í™”í˜• Task ì œê±°"""
        tasks = self.schema_manager.get_all_tasks()
        if not tasks:
            print("âŒ ì œê±°í•  Taskê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print("\nğŸ“ ë“±ë¡ëœ Task:")
        task_names = list(tasks.keys())
        for i, task_name in enumerate(task_names, 1):
            print(f"  {i}. {task_name}")
        
        try:
            choice = input("\nì œê±°í•  Task ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„: ").strip()
            if choice.isdigit():
                idx = int(choice) - 1
                if 0 <= idx < len(task_names):
                    task = task_names[idx]
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return False
            else:
                task = choice
                if task not in tasks:
                    print(f"âŒ Task '{task}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return False
            
            confirm = input(f"\nTask '{task}'ë¥¼ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm in ['y', 'yes']:
                result = self.schema_manager.remove_task(task)
                if result:
                    print(f"âœ… Task '{task}' ì œê±° ì™„ë£Œ!")
                    return True
                else:
                    print(f"âŒ Task ì œê±° ì‹¤íŒ¨")
                    return False
            else:
                print("âŒ ì œê±°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ì œê±°ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Task ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def upload_data_interactive(self):
        """ëŒ€í™”í˜• ë°ì´í„° ì—…ë¡œë“œ"""
        print("\n" + "="*50)
        print("ğŸ“¥ ë°ì´í„° ì—…ë¡œë“œ")
        print("="*50)
        
        try:
            # 1. ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì…ë ¥
            data_file = input("ğŸ“ ë°ì´í„° íŒŒì¼ ê²½ë¡œ: ").strip()
            if not data_file or not Path(data_file).exists():
                print("âŒ ìœ íš¨í•œ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                return False
            
            # 2. ë°ì´í„° íƒ€ì… ì„ íƒ (ê°€ì¥ ì¤‘ìš”í•œ ë¶„ê¸°ì )
            data_type = input("\nğŸ“ ë°ì´í„° íƒ€ì… (raw/task) [raw]: ").strip().lower() or "raw"
            if data_type not in ["raw", "task"]:
                print("âŒ ì˜ëª»ëœ ë°ì´í„° íƒ€ì…ì…ë‹ˆë‹¤. (raw ë˜ëŠ” task)")
                return False
            
            # 3. Provider ì„ íƒ
            providers = self.schema_manager.get_all_providers()
            if not providers:
                print("âŒ ë“±ë¡ëœ Providerê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Providerë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                return False
                
            print(f"\nğŸ¢ ì‚¬ìš© ê°€ëŠ¥í•œ Provider:")
            for i, provider in enumerate(providers, 1):
                print(f"  {i}. {provider}")
            
            provider_choice = input("Provider ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„ ì…ë ¥: ").strip()
            if provider_choice.isdigit():
                idx = int(provider_choice) - 1
                if 0 <= idx < len(providers):
                    provider = providers[idx]
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return False
            else:
                provider = provider_choice
                if provider not in providers:
                    print(f"âŒ Provider '{provider}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return False
            
            # 4. ë°ì´í„° íƒ€ì…ë³„ í”Œë¡œìš°
            if data_type == "raw":
                # Raw ë°ì´í„°: ìƒˆ Dataset ìƒì„±
                dataset = input("\nğŸ“¦ ìƒˆ Dataset ì´ë¦„: ").strip()
                if not dataset:
                    print("âŒ Dataset ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    return False
                
                description = input("ğŸ“„ ë°ì´í„°ì…‹ ì„¤ëª… (ì„ íƒì‚¬í•­): ").strip()
                source = input("ğŸ”— ì›ë³¸ ì†ŒìŠ¤ URL (ì„ íƒì‚¬í•­): ").strip()
                
                print(f"\nğŸ“‹ ì—…ë¡œë“œ ì •ë³´:")
                print(f"  ğŸ“ íŒŒì¼: {data_file}")
                print(f"  ğŸ“ íƒ€ì…: Raw ë°ì´í„°")
                print(f"  ğŸ¢ Provider: {provider}")
                print(f"  ğŸ“¦ Dataset: {dataset} (ìƒˆë¡œ ìƒì„±)")
                if description:
                    print(f"  ğŸ“„ ì„¤ëª…: {description}")
                if source:
                    print(f"  ğŸ”— ì†ŒìŠ¤: {source}")
                
                confirm = input("\nì—…ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if confirm in ['y', 'yes']:
                    staging_dir, job_id = self.data_manager.upload_raw_data(
                        data_file=data_file,
                        provider=provider,
                        dataset=dataset,
                        dataset_description=description,
                        original_source=source
                    )
                    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
                    print("ğŸ’¡ 'python cli.py process start' ëª…ë ¹ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    return True
                    
            elif data_type == "task":
                # Task ë°ì´í„°: ê¸°ì¡´ Datasetì—ì„œ ì„ íƒ
                print(f"\nğŸ“¦ ê¸°ì¡´ Dataset ì„ íƒ:")
                print("ğŸ’¡ Task ë°ì´í„°ëŠ” ê¸°ì¡´ì— ì—…ë¡œë“œëœ raw ë°ì´í„°ì—ì„œ ì¶”ì¶œë©ë‹ˆë‹¤.")
                
                # í•´ë‹¹ Providerì˜ ê¸°ì¡´ dataset ëª©ë¡ ì¡°íšŒ
                catalog_path = self.data_manager.catalog_path / f"provider={provider}"
                existing_datasets = []
                
                if catalog_path.exists():
                    for dataset_dir in catalog_path.iterdir():
                        if dataset_dir.is_dir() and dataset_dir.name.startswith("dataset="):
                            dataset_name = dataset_dir.name.replace("dataset=", "")
                            existing_datasets.append(dataset_name)
                
                if not existing_datasets:
                    print(f"âŒ Provider '{provider}'ì— ì—…ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    print("ğŸ’¡ ë¨¼ì € raw ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                    return False
                
                print(f"\nğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ Dataset ({len(existing_datasets)}ê°œ):")
                for i, dataset_name in enumerate(existing_datasets, 1):
                    print(f"  {i}. {dataset_name}")
                
                dataset_choice = input("Dataset ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„ ì…ë ¥: ").strip()
                if dataset_choice.isdigit():
                    idx = int(dataset_choice) - 1
                    if 0 <= idx < len(existing_datasets):
                        dataset = existing_datasets[idx]
                    else:
                        print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                        return False
                else:
                    dataset = dataset_choice
                    if dataset not in existing_datasets:
                        print(f"âŒ Dataset '{dataset}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        return False
                
                # Task ì„ íƒ
                tasks = self.schema_manager.get_all_tasks()
                if not tasks:
                    print("âŒ ë“±ë¡ëœ Taskê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Taskë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                    return False
                    
                print(f"\nğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ Task:")
                task_names = list(tasks.keys())
                for i, task_name in enumerate(task_names, 1):
                    print(f"  {i}. {task_name}")
                
                task_choice = input("Task ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„ ì…ë ¥: ").strip()
                if task_choice.isdigit():
                    idx = int(task_choice) - 1
                    if 0 <= idx < len(task_names):
                        task = task_names[idx]
                    else:
                        print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                        return False
                else:
                    task = task_choice
                    if task not in tasks:
                        print(f"âŒ Task '{task}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        return False
                
                # Variant ì…ë ¥
                variant = input("\nğŸ·ï¸ Variant ì´ë¦„: ").strip()
                if not variant:
                    print("âŒ Variant ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    return False
                
                # í•„ìˆ˜ í•„ë“œ ì…ë ¥
                all_tasks = self.data_manager.schema_manager.get_all_tasks()
                task_info = all_tasks.get(task, {})
                required_fields = task_info.get('required_fields', [])
                allowed_values = task_info.get('allowed_values', {})
                
                metadata = {}
                if required_fields:
                    print(f"\nğŸ“ í•„ìˆ˜ í•„ë“œ ì…ë ¥:")
                    for field in required_fields:
                        if field in allowed_values:
                            print(f"  {field} í—ˆìš©ê°’: {allowed_values[field]}")
                        value = input(f"  {field}: ").strip()
                        if not value:
                            print(f"âŒ í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            return False
                        metadata[field] = value
                
                # ê²€ì¦
                is_valid, error_msg = self.data_manager.schema_manager.validate_task_metadata(task, metadata)
                if not is_valid:
                    print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
                    return False
                
                print(f"\nğŸ“‹ ì—…ë¡œë“œ ì •ë³´:")
                print(f"  ğŸ“ íŒŒì¼: {data_file}")
                print(f"  ğŸ“ íƒ€ì…: Task ë°ì´í„°")
                print(f"  ğŸ¢ Provider: {provider}")
                print(f"  ğŸ“¦ Dataset: {dataset} (ê¸°ì¡´)")
                print(f"  ğŸ“ Task: {task}")
                print(f"  ğŸ·ï¸ Variant: {variant}")
                print(f"  ğŸ“‹ ë©”íƒ€ë°ì´í„°: {metadata}")
                
                confirm = input("\nì—…ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if confirm in ['y', 'yes']:
                    staging_dir, job_id = self.data_manager.upload_task_data(
                        data_file=data_file,
                        provider=provider,
                        dataset=dataset,
                        task=task,
                        variant=variant,
                        **metadata
                    )
                    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
                    print("ğŸ’¡ 'python cli.py process start' ëª…ë ¹ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    return True
                
        except KeyboardInterrupt:
            print("\nâŒ ì—…ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def download_data_interactive(self):
        """ëŒ€í™”í˜• ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        print("\n" + "="*50)
        print("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        print("="*50)
        
        try:
            # DuckDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì„ì‹œ DB ì‚¬ìš©)
            db_path = self.data_manager.base_path / "catalog.duckdb"
            with DuckDBClient(str(db_path), read_only=True) as duck_client:
                
                print("ğŸ”„ Catalog ë°ì´í„° ë¡œë”© ì¤‘...")
                catalog_path = self.data_manager.catalog_path

                needs_update = self._check_and_update_catalog_db(duck_client, catalog_path)
                if needs_update:
                    print("ğŸ”„ Catalog DB ì—…ë°ì´íŠ¸ ì¤‘...")
                    try:
                        duck_client.create_table_from_parquet(
                            "catalog",
                            str(catalog_path / "**" / "*.parquet"),
                            hive_partitioning=True,
                            union_by_name=True
                        )
                        print("âœ… Catalog DB ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    except Exception as e:
                        print(f"âŒ Catalog DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                        return False
                else:
                    print("âœ… ê¸°ì¡´ Catalog DB ì‚¬ìš©")
                    
                # ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒí‹°ì…˜ í™•ì¸
                partitions_df = duck_client.retrieve_partitions("catalog")
                if partitions_df.empty:
                    print("âŒ Catalogì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                    
                print(f"ğŸ“Š {len(partitions_df)} ê°œ íŒŒí‹°ì…˜ ì‚¬ìš© ê°€ëŠ¥")
            
                # 1. ê²€ìƒ‰ ë°©ë²• ì„ íƒ
                print("\nğŸ” ê²€ìƒ‰ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
                print("  1. íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰ (Provider/Dataset/Task/Variant)")
                print("  2. í…ìŠ¤íŠ¸ ê²€ìƒ‰ (JSON ë¼ë²¨ ë‚´ í…ìŠ¤íŠ¸)")
            
                search_choice = input("ê²€ìƒ‰ ë°©ë²• (1-2) [1]: ").strip() or "1"
                
                search_results = None
                
                if search_choice == "1":
                    # íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰
                    search_results = self._partition_based_search(duck_client, partitions_df)
                    print("\nğŸ“Š íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼:")
                    print(search_results.head(3).to_string(index=False, max_cols=5))
                elif search_choice == "2":
                    # í…ìŠ¤íŠ¸ ê²€ìƒ‰
                    search_results = self._text_based_search(duck_client)
                else:
                    print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
                    return False
                
                if search_results is None or search_results.empty:
                    print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(search_results):,}ê°œ í•­ëª©")
                
                # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                print("\nğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
                print(search_results.head(10))
                if len(search_results) > 3:
                    print(f"... (ì´ {len(search_results):,}ê°œ í•­ëª©)")
                
                # ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ì„ íƒ
                return self._download_options(search_results)
                
        except KeyboardInterrupt:
            print("\nâŒ ë‹¤ìš´ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _partition_based_search(self, duck_client: DuckDBClient, partitions_df: pd.DataFrame):
        """íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰ (Provider/Dataset/Task/Variant)"""
        print("\nğŸ¢ Provider ì„ íƒ:")
        
        providers = sorted(partitions_df['provider'].unique().tolist())
        print("ì‚¬ìš© ê°€ëŠ¥í•œ Provider:")
        for i, provider in enumerate(providers, 1):
            count = len(partitions_df[partitions_df['provider'] == provider])
            print(f"  {i}. {provider} ({count}ê°œ íŒŒí‹°ì…˜)")
        
        provider_choice = input("Provider ì„ íƒ (ë²ˆí˜¸/ì´ë¦„, ì „ì²´ëŠ” Enter): ").strip()
        selected_providers = []
        
        if not provider_choice:
            selected_providers = providers
            print("âœ… ëª¨ë“  Provider ì„ íƒ")
        elif provider_choice.isdigit():
            idx = int(provider_choice) - 1
            if 0 <= idx < len(providers):
                selected_providers = [providers[idx]]
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                return None
        else:
            if provider_choice in providers:
                selected_providers = [provider_choice]
            else:
                print(f"âŒ Provider '{provider_choice}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return None
        
        # Dataset ì„ íƒ
        filtered_partitions = partitions_df[partitions_df['provider'].isin(selected_providers)]
        datasets = sorted(filtered_partitions['dataset'].unique().tolist())
        
        print(f"\nğŸ“¦ Dataset ì„ íƒ ({len(datasets)}ê°œ ì‚¬ìš© ê°€ëŠ¥):")
        for i, dataset in enumerate(datasets, 1):
            count = len(filtered_partitions[filtered_partitions['dataset'] == dataset])
            print(f"  {i}. {dataset} ({count}ê°œ íŒŒí‹°ì…˜)")
        
        dataset_choice = input("Dataset ì„ íƒ (ë²ˆí˜¸/ì´ë¦„, ì „ì²´ëŠ” Enter): ").strip()
        selected_datasets = []
        
        if not dataset_choice:
            selected_datasets = datasets
            print("âœ… ëª¨ë“  Dataset ì„ íƒ")
        elif dataset_choice.isdigit():
            idx = int(dataset_choice) - 1
            if 0 <= idx < len(datasets):
                selected_datasets = [datasets[idx]]
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                return None
        else:
            if dataset_choice in datasets:
                selected_datasets = [dataset_choice]
            else:
                print(f"âŒ Dataset '{dataset_choice}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return None
        
        # Task ì„ íƒ
        filtered_partitions = filtered_partitions[filtered_partitions['dataset'].isin(selected_datasets)]
        tasks = sorted(filtered_partitions['task'].unique().tolist())
        
        print(f"\nğŸ“ Task ì„ íƒ ({len(tasks)}ê°œ ì‚¬ìš© ê°€ëŠ¥):")
        for i, task in enumerate(tasks, 1):
            count = len(filtered_partitions[filtered_partitions['task'] == task])
            print(f"  {i}. {task} ({count}ê°œ íŒŒí‹°ì…˜)")
        
        task_choice = input("Task ì„ íƒ (ë²ˆí˜¸/ì´ë¦„, ì „ì²´ëŠ” Enter): ").strip()
        selected_tasks = []
        
        if not task_choice:
            selected_tasks = tasks
            print("âœ… ëª¨ë“  Task ì„ íƒ")
        elif task_choice.isdigit():
            idx = int(task_choice) - 1
            if 0 <= idx < len(tasks):
                selected_tasks = [tasks[idx]]
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                return None
        else:
            if task_choice in tasks:
                selected_tasks = [task_choice]
            else:
                print(f"âŒ Task '{task_choice}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return None
        
        # Variant ì„ íƒ
        filtered_partitions = filtered_partitions[filtered_partitions['task'].isin(selected_tasks)]
        variants = sorted(filtered_partitions['variant'].unique().tolist())
        
        print(f"\nğŸ·ï¸ Variant ì„ íƒ ({len(variants)}ê°œ ì‚¬ìš© ê°€ëŠ¥):")
        for i, variant in enumerate(variants, 1):
            count = len(filtered_partitions[filtered_partitions['variant'] == variant])
            print(f"  {i}. {variant} ({count}ê°œ íŒŒí‹°ì…˜)")
        
        variant_choice = input("Variant ì„ íƒ (ë²ˆí˜¸/ì´ë¦„, ì „ì²´ëŠ” Enter): ").strip()
        selected_variants = []
        
        if not variant_choice:
            selected_variants = variants
            print("âœ… ëª¨ë“  Variant ì„ íƒ")
        elif variant_choice.isdigit():
            idx = int(variant_choice) - 1
            if 0 <= idx < len(variants):
                selected_variants = [variants[idx]]
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                return None
        else:
            if variant_choice in variants:
                selected_variants = [variant_choice]
            else:
                print(f"âŒ Variant '{variant_choice}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return None
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        print(f"\nğŸ” ê²€ìƒ‰ ì¤‘...")
        print(f"  Provider: {selected_providers}")
        print(f"  Dataset: {selected_datasets}")
        print(f"  Task: {selected_tasks}")
        print(f"  Variant: {selected_variants}")
        
        return duck_client.retrieve_with_existing_cols(
            providers=selected_providers,
            datasets=selected_datasets,
            tasks=selected_tasks,
            variants=selected_variants,
            table="catalog"
        )

    def _text_based_search(self, duck_client: DuckDBClient):
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰"""
        print("\nğŸ”¤ í…ìŠ¤íŠ¸ ê²€ìƒ‰:")
        
        search_text = input("ê²€ìƒ‰í•  í…ìŠ¤íŠ¸: ").strip()
        if not search_text:
            print("âŒ ê²€ìƒ‰ í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return None
        
        columns_df = duck_client.get_table_info("catalog")
        columns = columns_df['column_name'].tolist()
        # ì»¬ëŸ¼ ì„ íƒ
        print(f"\nğŸ“ ì»¬ëŸ¼ ì„ íƒ:")
        for i, col in enumerate(columns, 1):
            print(f"  {i}. {col}")
        
        col_choice = input(f"ì»¬ëŸ¼ ì„ íƒ (1-{len(columns)}) [1]: ").strip() or "1"
        if col_choice.isdigit():
            idx = int(col_choice) - 1
            if 0 <= idx < len(columns):
                selected_column = columns[idx]
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                return None
        else:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")
            return None
            
        # ğŸ†• ê²€ìƒ‰ ë°©ë²• ì„ íƒ
        print(f"\nğŸ” '{selected_column}' ì»¬ëŸ¼ì—ì„œ ê²€ìƒ‰ ë°©ë²•:")
        print("  1. ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (LIKE)")
        print("  2. JSON íŒŒì‹± í›„ ê²€ìƒ‰")
        
        method_choice = input("ê²€ìƒ‰ ë°©ë²• (1-2) [1]: ").strip() or "1"
        
        if method_choice == "1":
            # ë‹¨ìˆœ LIKE ê²€ìƒ‰
            print(f"\nğŸ” ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰:")
            print(f"  í…ìŠ¤íŠ¸: '{search_text}'")
            print(f"  ì»¬ëŸ¼: {selected_column}")
            
            sql = duck_client.json_queries.search_text_in_column(
                table="catalog",
                column=selected_column,
                search_text=search_text,
                search_type="simple",
                engine="duckdb"
            )
            return duck_client.execute_query(sql)
            
        elif method_choice == "2":
            # JSON íŒŒì‹± ê²€ìƒ‰
            json_path = input("JSON ê²½ë¡œ (ì˜ˆ: $.image.text.content): ").strip()
            if not json_path:
                print("âŒ JSON ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return None
            
            # Variant ì„ íƒ (JSON ê²€ìƒ‰ì‹œì—ë§Œ)
            partitions_df = duck_client.retrieve_partitions("catalog")
            variants = sorted(partitions_df['variant'].unique().tolist())
            
            print(f"\nğŸ·ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ Variant ({len(variants)}ê°œ):")
            for i, variant in enumerate(variants, 1):
                count = len(partitions_df[partitions_df['variant'] == variant])
                print(f"  {i}. {variant} ({count}ê°œ íŒŒí‹°ì…˜)")
            
            variant_choice = input(f"Variant ì„ íƒ (1-{len(variants)}) [1]: ").strip() or "1"
            if variant_choice.isdigit():
                idx = int(variant_choice) - 1
                if 0 <= idx < len(variants):
                    selected_variant = variants[idx]
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return None
            else:
                if variant_choice in variants:
                    selected_variant = variant_choice
                else:
                    print(f"âŒ Variant '{variant_choice}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return None
            
            print(f"\nğŸ” JSON íŒŒì‹± ê²€ìƒ‰ ì‹¤í–‰:")
            print(f"  í…ìŠ¤íŠ¸: '{search_text}'")
            print(f"  ì»¬ëŸ¼: {selected_column}")
            print(f"  JSON ê²½ë¡œ: {json_path}")
            print(f"  Variant: {selected_variant}")
            
            # Variant ì¡°ê±´ ì¶”ê°€
            partition_conditions = {"variant": selected_variant}
            
            sql = duck_client.json_queries.search_text_in_column(
                table="catalog",
                column=selected_column,
                search_text=search_text,
                search_type="json",
                json_loc=json_path,
                partition_conditions=partition_conditions,
                engine="duckdb"
            )
            return duck_client.execute_query(sql)
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return None

    def _download_options(self, search_results):
        """ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ì„ íƒ ë° ì‹¤í–‰"""
        print("\nğŸ’¾ ë‹¤ìš´ë¡œë“œ ì˜µì…˜:")
        print("  1. ë©”íƒ€ë°ì´í„°ë§Œ (Parquet)")
        print("  2. ë©”íƒ€ë°ì´í„°ë§Œ (Arrow Dataset)")
        print("  3. ë©”íƒ€ë°ì´í„° + ì´ë¯¸ì§€ (Dataset format)")
        
        download_choice = input("ë‹¤ìš´ë¡œë“œ ì˜µì…˜ (1-3) [1]: ").strip() or "1"
        
        # ì €ì¥ ê²½ë¡œ ì…ë ¥
        default_path = f"./downloads/export_{len(search_results)}_items"
        save_path = input(f"ì €ì¥ ê²½ë¡œ [{default_path}]: ").strip() or default_path
        save_path = Path(save_path)
        
        try:
            if download_choice == "1":
                # Parquet ì €ì¥
                parquet_path = save_path.with_suffix('.parquet')
                parquet_path.parent.mkdir(parents=True, exist_ok=True)
                search_results.to_parquet(parquet_path, index=False)
                print(f"âœ… Parquet ì €ì¥ ì™„ë£Œ: {parquet_path}")
                print(f"ğŸ“Š {len(search_results):,}ê°œ í•­ëª©, {parquet_path.stat().st_size / 1024 / 1024:.1f}MB")
                
            elif download_choice == "2":
                # Arrow Dataset ì €ì¥
                return self._save_as_dataset(search_results, save_path, include_images=False)
                
            elif download_choice == "3":
                # Dataset + ì´ë¯¸ì§€ ì €ì¥
                return self._save_as_dataset(search_results, save_path, include_images=True)
                
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
                return False
                
            return True
            
        except Exception as e:
            print(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
        
    def _save_as_dataset(self, search_results, save_path, include_images=False):
        """datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Dataset í˜•íƒœë¡œ ì €ì¥"""
        try:
            save_path = Path(save_path)
            save_path.mkdir(parents=True, exist_ok=True)
            
            if include_images:
                print(f"\nğŸ“¥ ì´ë¯¸ì§€ í¬í•¨ Dataset ìƒì„± ì¤‘...")
                
                path_column = None
                for col in ['hash', 'path']:
                    if col in search_results.columns:
                        path_column = col
                        break
                
                if path_column is None:
                    print("âŒ ì´ë¯¸ì§€ ê²½ë¡œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                # ì´ë¯¸ì§€ ë¡œë“œ í•¨ìˆ˜
                def load_image(example):
                    try:
                        if example[path_column] and pd.notna(example[path_column]):
                            image_path = self.data_manager.assets_path / example[path_column]
                            if image_path.exists():
                                # PIL Imageë¡œ ë¡œë“œ
                                pil_image = Image.open(image_path)
                                example['image'] = pil_image
                            else:
                                example['image'] = None
                        else:
                            example['image'] = None
                    except Exception as e:
                        print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {example.get(path_column, 'unknown')} - {e}")
                        example['image'] = None
                    return example
                
                # DataFrameì„ Datasetìœ¼ë¡œ ë³€í™˜
                dataset = Dataset.from_pandas(search_results)
                
                # ì´ë¯¸ì§€ ë¡œë“œ (ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬)
                print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...")
                dataset_with_images = dataset.map(
                    load_image,
                    desc="ì´ë¯¸ì§€ ë¡œë”©",
                    num_proc=self.data_manager.num_proc,
                )
                
                # ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸
                valid_images = sum(1 for example in dataset_with_images if example['image'] is not None)
                total_items = len(dataset_with_images)
                
                print(f"ğŸ“Š ì´ë¯¸ì§€ ë¡œë”© ì™„ë£Œ: {valid_images}/{total_items}ê°œ ì„±ê³µ")
                
                # Dataset ì €ì¥
                dataset_with_images.save_to_disk(str(save_path))
                
                print(f"âœ… Dataset ì €ì¥ ì™„ë£Œ: {save_path}")
                print(f"ğŸ“Š {total_items:,}ê°œ í•­ëª© (ì´ë¯¸ì§€ {valid_images:,}ê°œ)")
                print(f"ğŸ’¾ ì´ í¬ê¸°: {sum(f.stat().st_size for f in save_path.rglob('*') if f.is_file()) / 1024 / 1024:.1f}MB")
                
                # ì‚¬ìš©ë²• ì•ˆë‚´
                print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
                print(f"```python")
                print(f"from datasets import load_from_disk")
                print(f"dataset = load_from_disk('{save_path}')")
                print(f"# ì´ë¯¸ì§€ í™•ì¸: dataset[0]['image'].show()")
                print(f"```")
                
            else:
                # ë©”íƒ€ë°ì´í„°ë§Œ Datasetìœ¼ë¡œ ì €ì¥
                print(f"\nğŸ“„ ë©”íƒ€ë°ì´í„° Dataset ìƒì„± ì¤‘...")
                
                # DataFrameì„ Datasetìœ¼ë¡œ ë³€í™˜
                dataset = Dataset.from_pandas(search_results)
                
                # Dataset ì €ì¥
                dataset.save_to_disk(str(save_path))
                
                print(f"âœ… Dataset ì €ì¥ ì™„ë£Œ: {save_path}")
                print(f"ğŸ“Š {len(dataset):,}ê°œ í•­ëª©")
                print(f"ğŸ’¾ í¬ê¸°: {sum(f.stat().st_size for f in save_path.rglob('*') if f.is_file()) / 1024 / 1024:.1f}MB")
                
                # ì‚¬ìš©ë²• ì•ˆë‚´
                print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
                print(f"```python")
                print(f"from datasets import load_from_disk")
                print(f"dataset = load_from_disk('{save_path}')")
                print(f"df = dataset.to_pandas()  # pandasë¡œ ë³€í™˜")
                print(f"```")
            
            return True
            
        except ImportError:
            print("âŒ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹: pip install datasets")
            return False
        except Exception as e:
            print(f"âŒ Dataset ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
        
    def trigger_processing(self):
        """NAS ì²˜ë¦¬ ìˆ˜ë™ ì‹œì‘"""
        print("\n" + "="*50)
        print("ğŸ”„ NAS ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        print("="*50)
        
        try:
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            status = self.data_manager.get_nas_status()
            if status:
                pending_count = status.get('pending', 0)
                processing_count = status.get('processing', 0)
                
                print(f"ğŸ“¦ Pending: {pending_count}ê°œ")
                print(f"ğŸ”„ Processing: {processing_count}ê°œ")
                
                if pending_count == 0:
                    print("ğŸ’¡ ì²˜ë¦¬í•  pending ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return True
                
                if processing_count > 0:
                    print("âš ï¸ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤.")
                    continue_anyway = input("ê·¸ë˜ë„ ìƒˆ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                    if continue_anyway not in ['y', 'yes']:
                        print("âŒ ì²˜ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        return False
            else:
                print("âš ï¸ NAS ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue_anyway = input("ê·¸ë˜ë„ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if continue_anyway not in ['y', 'yes']:
                    print("âŒ ì²˜ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return False
            
            # ì²˜ë¦¬ ì‹œì‘
            job_id = self.data_manager.trigger_nas_processing()
            if job_id:
                print(f"âœ… ì²˜ë¦¬ ì‹œì‘ë¨: {job_id}")
                
                # ëŒ€ê¸° ì—¬ë¶€ í™•ì¸
                wait_completion = input("ì²˜ë¦¬ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if wait_completion in ['y', 'yes']:
                    try:
                        print("â³ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸° ì¤‘... (Ctrl+Cë¡œ ì¤‘ë‹¨)")
                        result = self.data_manager.wait_for_job_completion(job_id, timeout=3600)
                        print(f"ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ: {result}")
                        return True
                    except KeyboardInterrupt:
                        print("\nâ¸ï¸ ëŒ€ê¸° ì¤‘ë‹¨ë¨. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ëŠ” ê³„ì†ë©ë‹ˆë‹¤.")
                        print(f"ğŸ’¡ 'python cli.py process status {job_id}' ëª…ë ¹ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        return True
                    except Exception as e:
                        print(f"âŒ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
                        return False
                else:
                    print(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. Job ID: {job_id}")
                    print(f"ğŸ’¡ 'python cli.py process status {job_id}' ëª…ë ¹ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    return True
            else:
                print("âŒ ì²˜ë¦¬ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
                
        except KeyboardInterrupt:
            print("\nâŒ ì²˜ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def check_job_status(self, job_id: str):
        """íŠ¹ì • ì‘ì—… ìƒíƒœ í™•ì¸"""
        print(f"\nğŸ” ì‘ì—… ìƒíƒœ í™•ì¸: {job_id}")
        print("="*50)
        
        try:
            job_status = self.data_manager.get_job_status(job_id)
            if job_status:
                status = job_status.get('status', 'unknown')
                started_at = job_status.get('started_at', 'N/A')
                finished_at = job_status.get('finished_at', 'N/A')
                
                status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(status, "â“")
                print(f"{status_emoji} ìƒíƒœ: {status}")
                print(f"â° ì‹œì‘: {started_at}")
                
                if status == 'completed':
                    print(f"ğŸ ì™„ë£Œ: {finished_at}")
                    result = job_status.get('result', {})
                    print(f"ğŸ“Š ì„±ê³µ: {result.get('success', 0)}ê°œ")
                    print(f"âŒ ì‹¤íŒ¨: {result.get('failed', 0)}ê°œ")
                elif status == 'failed':
                    print(f"ğŸ’¥ ì‹¤íŒ¨: {finished_at}")
                    error = job_status.get('error', 'Unknown error')
                    print(f"ğŸ” ì˜¤ë¥˜: {error}")
                elif status == 'running':
                    print("ğŸ”„ ì§„í–‰ ì¤‘...")
                    
                return True
            else:
                print(f"âŒ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
                return False
                
        except Exception as e:
            print(f"âŒ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def list_all_data(self):
        """ë‚´ ë°ì´í„° ì „ì²´ í˜„í™© ì¡°íšŒ"""
        print("\nğŸ“‹ ë‚´ ë°ì´í„° í˜„í™©")
        print("="*50)
        
        total_items = 0
        
        try:
            # 1. ğŸ“¥ ì—…ë¡œë“œë¨ (Pending)
            pending_path = self.data_manager.staging_pending_path
            pending_items = []
            
            if pending_path.exists():
                pending_dirs = [d for d in pending_path.iterdir() if d.is_dir()]
                for pending_dir in sorted(pending_dirs, key=lambda x: x.stat().st_mtime, reverse=True):
                    try:
                        metadata_file = pending_dir / "upload_metadata.json"
                        if metadata_file.exists():
                            with open(metadata_file, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                            
                            provider = metadata.get('provider', 'Unknown')
                            dataset = metadata.get('dataset', 'Unknown')
                            task = metadata.get('task', 'Unknown')
                            uploaded_at = metadata.get('uploaded_at', 'Unknown')
                            total_rows = metadata.get('total_rows', 0)
                            
                            try:
                                from datetime import datetime
                                upload_time = datetime.fromisoformat(uploaded_at.replace('Z', '+00:00'))
                                time_str = upload_time.strftime('%m-%d %H:%M')
                            except:
                                time_str = uploaded_at[:10]
                            
                            pending_items.append({
                                'name': f"{provider}/{dataset}/{task}",
                                'time': time_str,
                                'rows': total_rows
                            })
                    except:
                        continue
            
            # 2. ğŸ”„ ì²˜ë¦¬ ì¤‘/ì™„ë£Œ (Jobs)
            jobs = self.data_manager.list_nas_jobs() or []
            recent_jobs = jobs[-5:] if jobs else []  # ìµœê·¼ 5ê°œ
            
            # ì¶œë ¥
            if pending_items:
                print(f"\nğŸ“¥ ì—…ë¡œë“œë¨ ({len(pending_items)}ê°œ)")
                for item in pending_items:
                    rows_str = f"{item['rows']:,}" if item['rows'] > 0 else "?"
                    print(f"  ğŸ“¦ {item['name']} ({rows_str} rows) - {item['time']}")
                total_items += len(pending_items)
            
            if recent_jobs:
                print(f"\nğŸ”„ ì²˜ë¦¬ ì‘ì—… ({len(recent_jobs)}ê°œ)")
                for job in reversed(recent_jobs):
                    status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(job['status'], "â“")
                    job_id_short = job['job_id'][:8] + "..." if len(job['job_id']) > 8 else job['job_id']
                    started_at = job.get('started_at', 'Unknown')
                    try:
                        time_str = started_at.split('T')[1][:5] if 'T' in started_at else started_at[:5]
                    except:
                        time_str = started_at
                    print(f"  {status_emoji} {job_id_short} ({job['status']}) - {time_str}")
                total_items += len(recent_jobs)
            
            # ìš”ì•½ ë° ì•ˆë‚´
            if total_items == 0:
                print("\nğŸ“­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ 'python cli.py upload' ëª…ë ¹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ë³´ì„¸ìš”.")
            else:
                if pending_items:
                    print(f"\nğŸ’¡ 'python cli.py process' ëª…ë ¹ìœ¼ë¡œ ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° í˜„í™© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def show_status(self):
        """ìƒíƒœ ì •ë³´ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š Data Manager Status")
        print("="*60)
        
        # Schema ì •ë³´
        self.schema_manager.show_schema_info()
        
        # NAS ìƒíƒœ
        self.data_manager.show_nas_dashboard()


def main():
    parser = argparse.ArgumentParser(
        description="ğŸ“Š Data Manager CLI - ë°ì´í„° ì—…ë¡œë“œ/ì²˜ë¦¬/ë‹¤ìš´ë¡œë“œ ê´€ë¦¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:

ğŸ”§ ì„¤ì • ê´€ë¦¬:
  python cli.py config                         # ì„¤ì • ë„ì›€ë§
  python cli.py config list                    # ì „ì²´ ì„¤ì • í™•ì¸
  python cli.py config provider               # Provider ê´€ë¦¬ ë„ì›€ë§
  python cli.py config provider create        # Provider ìƒì„±
  python cli.py config provider list          # Provider ëª©ë¡
  python cli.py config provider remove        # Provider ì œê±°
  python cli.py config task                   # Task ê´€ë¦¬ ë„ì›€ë§
  python cli.py config task create            # Task ìƒì„±
  python cli.py config task list              # Task ëª©ë¡
  python cli.py config task remove            # Task ì œê±°

ğŸ“¥ ë°ì´í„° ê´€ë¦¬:
  python cli.py upload                         # ë°ì´í„° ì—…ë¡œë“œ
  python cli.py download                       # ë°ì´í„° ë‹¤ìš´ë¡œë“œ

  ë‹¤ìš´ë¡œë“œ í¬ë§·:
    1. Parquet (ë©”íƒ€ë°ì´í„°ë§Œ)
    2. Arrow Dataset (ë©”íƒ€ë°ì´í„°ë§Œ) 
    3. Dataset + ì´ë¯¸ì§€ (HuggingFace datasets í˜•íƒœ)
ğŸ”„ ì²˜ë¦¬ ê´€ë¦¬:
  python cli.py process                        # ì²˜ë¦¬ ì‹œì‘ 
  python cli.py process start                  # ìƒˆ ì²˜ë¦¬ ì‹œì‘
  python cli.py process status JOB_ID          # ì‘ì—… ìƒíƒœ í™•ì¸
  python cli.py process list                   # ë‚´ ë°ì´í„° í˜„í™©

ğŸ“Š Catalog DB ê´€ë¦¬:
  python cli.py catalog info                   # Catalog DB ì •ë³´ í™•ì¸
  python cli.py catalog rebuild                # Catalog DB ê°•ì œ ì¬êµ¬ì¶•
  
ğŸ“Š ìƒíƒœ í™•ì¸:
  python cli.py status                         # ì „ì²´ ìƒíƒœ ëŒ€ì‹œë³´ë“œ

ğŸ’¡ íŒ: Dataset í˜•íƒœë¡œ ì €ì¥í•˜ë©´ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     from datasets import load_from_disk
     dataset = load_from_disk('./downloads/my_dataset')
        """
    )
    parser.add_argument("--base-path", default="/mnt/AI_NAS/datalake/migrate_test",
                       help="ë°ì´í„° ì €ì¥ ê¸°ë³¸ ê²½ë¡œ")
    parser.add_argument("--nas-url", default="http://192.168.20.62:8091", 
                       help="NAS API ì„œë²„ URL")
    parser.add_argument("--log-level", default="INFO",
                       help="ë¡œê¹… ë ˆë²¨ (DEBUG, INFO, WARNING, ERROR, CRITICAL)")
    
    subparsers = parser.add_subparsers(dest='command', help='ëª…ë ¹ì–´')
    
    
    # Config ê´€ë¦¬ (Provider + Task)
    config_parser = subparsers.add_parser('config', help='ì„¤ì • ê´€ë¦¬ (Provider, Task)')
    config_subparsers = config_parser.add_subparsers(dest='config_type')
    
    # Provider ê´€ë¦¬
    provider_parser = config_subparsers.add_parser('provider', help='Provider ê´€ë¦¬')
    provider_subparsers = provider_parser.add_subparsers(dest='provider_action')
    provider_subparsers.add_parser('create', help='ìƒˆ Provider ìƒì„±')
    provider_subparsers.add_parser('remove', help='Provider ì œê±°')
    provider_subparsers.add_parser('list', help='Provider ëª©ë¡')
    
    # Task ê´€ë¦¬
    task_parser = config_subparsers.add_parser('task', help='Task ê´€ë¦¬')
    task_subparsers = task_parser.add_subparsers(dest='task_action')
    task_subparsers.add_parser('create', help='ìƒˆ Task ìƒì„±')
    task_subparsers.add_parser('remove', help='Task ì œê±°')
    task_subparsers.add_parser('list', help='Task ëª©ë¡')
    
    # Config ì „ì²´ ëª©ë¡
    config_subparsers.add_parser('list', help='ì „ì²´ ì„¤ì • ëª©ë¡')
    
    # ë°ì´í„° ì—…ë¡œë“œ
    subparsers.add_parser('upload', help='ë°ì´í„° ì—…ë¡œë“œ')
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    subparsers.add_parser('download', help='ë°ì´í„° ë‹¤ìš´ë¡œë“œ')
    
    
    # ì²˜ë¦¬ ê´€ë¦¬
    process_parser = subparsers.add_parser('process', help='ë°ì´í„° ì²˜ë¦¬ ê´€ë¦¬')
    process_subparsers = process_parser.add_subparsers(dest='process_action')
    process_subparsers.add_parser('start', help='ìƒˆ ì²˜ë¦¬ ì‹œì‘')
    process_subparsers.add_parser('list', help='ë‚´ ë°ì´í„° ì „ì²´ í˜„í™© í™•ì¸')
    job_status_parser = process_subparsers.add_parser('status', help='íŠ¹ì • ì‘ì—… ìƒíƒœ í™•ì¸')
    job_status_parser.add_argument('job_id', help='ì‘ì—… ID')
    
    # Catalog DB ê´€ë¦¬
    catalog_parser = subparsers.add_parser('catalog', help='Catalog DB ê´€ë¦¬')
    catalog_subparsers = catalog_parser.add_subparsers(dest='catalog_action')
    catalog_subparsers.add_parser('info', help='Catalog DB ì •ë³´ í™•ì¸')
    catalog_subparsers.add_parser('rebuild', help='Catalog DB ê°•ì œ ì¬êµ¬ì¶•')
    
    # ìƒíƒœ í™•ì¸
    subparsers.add_parser('status', help='ì „ì²´ ìƒíƒœ í™•ì¸')
    
    args = parser.parse_args()
    if not args.command:
        print("\nğŸš€ Data Manager CLIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
        print("="*60)
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì£¼ìš” ëª…ë ¹ì–´:")
        print("  ğŸ”§ python cli.py config     - ì„¤ì • ê´€ë¦¬ (Provider, Task)")
        print("  ğŸ“¥ python cli.py upload     - ë°ì´í„° ì—…ë¡œë“œ")
        print("  ğŸ“¤ python cli.py download   - ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        print("  ğŸ”„ python cli.py process    - ë°ì´í„° ì²˜ë¦¬")
        print("  ğŸ“Š python cli.py catalog    - Catalog DB ê´€ë¦¬")
        print("  ğŸ“Š python cli.py status     - ìƒíƒœ í™•ì¸")
        
        print("\nğŸŒŸ ì²˜ìŒ ì‚¬ìš©í•˜ì‹œë‚˜ìš”? ë‹¤ìŒ ìˆœì„œë¡œ ì‹œì‘í•´ë³´ì„¸ìš”:")
        print("  1ï¸âƒ£ python cli.py config provider create  # ë°ì´í„° ì œê³µì ìƒì„±")
        print("  2ï¸âƒ£ python cli.py config task create      # ì‘ì—… ìœ í˜• ì •ì˜")
        print("  3ï¸âƒ£ python cli.py upload                  # ë°ì´í„° ì—…ë¡œë“œ")
        print("  4ï¸âƒ£ python cli.py process                 # ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        print("\n ğŸ’¡ ë°ì´í„° ë‹¤ìš´ë¡œë“œëŠ” 'python cli.py download' ëª…ë ¹ìœ¼ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        print("  1ï¸âƒ£ python cli.py catalog rebuild         # Catalog DB êµ¬ì¶•")
        print("  2ï¸âƒ£ python cli.py download                # ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        print("      â†’ ì˜µì…˜ 1: Parquet (ë©”íƒ€ë°ì´í„°ë§Œ)")
        print("      â†’ ì˜µì…˜ 2: Arrow Dataset (ë©”íƒ€ë°ì´í„°ë§Œ)")  
        print("      â†’ ì˜µì…˜ 3: Dataset + ì´ë¯¸ì§€ (HuggingFace í˜•íƒœ)")


        print("\nğŸ’¡ ê° ëª…ë ¹ì–´ ë’¤ì— -h ë˜ëŠ” --helpë¥¼ ë¶™ì´ë©´ ìƒì„¸ ë„ì›€ë§ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   ì˜ˆ: python cli.py config -h")
        print("\nğŸ”¥ Dataset í˜•íƒœë¡œ ì €ì¥í•˜ë©´ ML ì‘ì—…ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”!")
        print("   from datasets import load_from_disk")
        print("   dataset = load_from_disk('./downloads/my_dataset')")
        print("\n" + "="*60)
        return
    
    # CLI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    cli = DataManagerCLI(
        base_path=args.base_path,
        nas_api_url=args.nas_url,
        log_level=args.log_level,
    )
    
    try:
        if args.command == 'config':
            if not args.config_type:
                print("\nâ“ config í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                print("  ğŸ“‹ python cli.py config list      - ì „ì²´ ì„¤ì • í™•ì¸")
                print("  ğŸ¢ python cli.py config provider  - Provider ê´€ë¦¬")
                print("  ğŸ“ python cli.py config task      - Task ê´€ë¦¬")
                print("\nğŸ’¡ ì²˜ìŒ ì‚¬ìš©í•˜ì‹œë‚˜ìš”? ë‹¤ìŒ ìˆœì„œë¡œ ì‹œì‘í•´ë³´ì„¸ìš”:")
                print("  1ï¸âƒ£ python cli.py config provider create  # Provider ìƒì„±")
                print("  2ï¸âƒ£ python cli.py config task create      # Task ìƒì„±")
                print("  3ï¸âƒ£ python cli.py upload                  # ë°ì´í„° ì—…ë¡œë“œ")
                print("  4ï¸âƒ£ python cli.py process                 # ì²˜ë¦¬ ì‹œì‘")
                return
                
            if args.config_type == 'provider':
                if not args.provider_action:
                    print("\nâ“ provider í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                    print("  ğŸ“‹ python cli.py config provider list    - Provider ëª©ë¡")
                    print("  â• python cli.py config provider create  - Provider ìƒì„±")
                    print("  ğŸ—‘ï¸  python cli.py config provider remove  - Provider ì œê±°")
                    return
                    
                if args.provider_action == 'create':
                    cli.create_provider_interactive()
                elif args.provider_action == 'remove':
                    cli.remove_provider_interactive()
                elif args.provider_action == 'list':
                    providers = cli.schema_manager.get_all_providers()
                    print(f"\nğŸ¢ ë“±ë¡ëœ Provider ({len(providers)}ê°œ):")
                    if providers:
                        for provider in providers:
                            print(f"  â€¢ {provider}")
                    else:
                        print("  ğŸ“­ ë“±ë¡ëœ Providerê°€ ì—†ìŠµë‹ˆë‹¤.")
                        print("  ğŸ’¡ 'python cli.py config provider create' ëª…ë ¹ìœ¼ë¡œ Providerë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            
            elif args.config_type == 'task':
                if not args.task_action:
                    print("\nâ“ task í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                    print("  ğŸ“‹ python cli.py config task list    - Task ëª©ë¡")
                    print("  â• python cli.py config task create  - Task ìƒì„±")
                    print("  ğŸ—‘ï¸  python cli.py config task remove  - Task ì œê±°")
                    return
                    
                if args.task_action == 'create':
                    cli.create_task_interactive()
                elif args.task_action == 'remove':
                    cli.remove_task_interactive()
                elif args.task_action == 'list':
                    tasks = cli.schema_manager.get_all_tasks()
                    print(f"\nğŸ“ ë“±ë¡ëœ Task ({len(tasks)}ê°œ):")
                    if tasks:
                        for task_name, task_config in tasks.items():
                            print(f"  â€¢ {task_name}")
                            required_fields = task_config.get('required_fields', [])
                            if required_fields:
                                print(f"    ğŸ“ í•„ìˆ˜ í•„ë“œ: {', '.join(required_fields)}")
                            allowed_values = task_config.get('allowed_values', {})
                            if allowed_values:
                                print(f"    ğŸ”§ í—ˆìš© ê°’:")
                                for field, values in allowed_values.items():
                                    print(f"      - {field}: {', '.join(values)}")
                    else:
                        print("  ğŸ“­ ë“±ë¡ëœ Taskê°€ ì—†ìŠµë‹ˆë‹¤.")
                        print("  ğŸ’¡ 'python cli.py config task create' ëª…ë ¹ìœ¼ë¡œ Taskë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            
            elif args.config_type == 'list':
                cli.schema_manager.show_schema_info()
        
        elif args.command == 'upload':
            cli.upload_data_interactive()
        elif args.command == 'download':
            cli.download_data_interactive()
        elif args.command == 'process':
            if not args.process_action:
                print("\nâ“ process í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                print("  ğŸš€ python cli.py process start           - ìƒˆ ì²˜ë¦¬ ì‹œì‘")
                print("  ğŸ” python cli.py process status JOB_ID   - ì‘ì—… ìƒíƒœ í™•ì¸")
                print("  ğŸ“‹ python cli.py process list            - ë‚´ ë°ì´í„° í˜„í™©")
                return
                
            if args.process_action == 'start':
                cli.trigger_processing()
            elif args.process_action == 'status':
                cli.check_job_status(args.job_id)
            elif args.process_action == 'list':
                cli.list_all_data()
        
        elif args.command == 'status':
            cli.show_status()
        
        elif args.command == 'catalog':
            if not args.catalog_action:
                print("\nâ“ catalog í•˜ìœ„ ëª…ë ¹ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
                print("  ğŸ“Š python cli.py catalog info     - Catalog DB ì •ë³´ í™•ì¸")
                print("  ğŸ”¨ python cli.py catalog rebuild  - Catalog DB ê°•ì œ ì¬êµ¬ì¶•")
                return
                
            if args.catalog_action == 'info':
                cli.show_catalog_db_info()
            elif args.catalog_action == 'rebuild':
                cli.rebuild_catalog_db()
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì–¸ì œë“ ì§€ ë‹¤ì‹œ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("ğŸ’¡ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except ValueError as e:
        print(f"âŒ ì…ë ¥ ê°’ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ ì…ë ¥ ê°’ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except ConnectionError as e:
        print(f"âŒ ì—°ê²° ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ NAS ì„œë²„ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒí•˜ì§€ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("ğŸ’¡ ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()