import logging
import json
import shutil
import hashlib
import io
import threading
import time
import gc
import pandas as pd
import random

from collections import Counter
from datetime import datetime
from tqdm import tqdm
from pathlib import Path
from typing import Dict, List, Union, Optional
from PIL import Image
from datasets import Dataset, load_from_disk
from datasets.features import Image as ImageFeature
from functools import partial

from utils.logging import setup_logging

class DatalakeProcessor:
    
    def __init__(
        self,
        base_path: str = "/mnt/AI_NAS/datalake/",
        log_level: str = "INFO",
        num_proc: int = 4,
        batch_size: int = 1000,  # map()ì˜ ë°°ì¹˜ í¬ê¸°
        create_dirs: bool = True,
    ):
        # ê²½ë¡œ ì„¤ì •
        self.base_path = Path(base_path)
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        
        self.catalog_path = self.base_path / "catalog"
        self.assets_path = self.base_path / "assets"
        self.collections_path = self.base_path / "collections"
        
        self.num_proc = num_proc
        self.batch_size = batch_size
        
        # LocalDataManagerì™€ ë™ì¼
        self.image_data_key = 'image'  # ê¸°ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‚¤
        self.file_path_key = 'file_path'  # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ í‚¤
        
        self._initialize(log_level, create_dirs=create_dirs)
        
        self.existing_hashes = set()
        self.cache_built = False
        self.cache_lock = threading.Lock()
        
        # ì²˜ë¦¬ ì‹¤íŒ¨ ì¶”ì ìš©
        self.processing_failed = False
        self.failure_lock = threading.Lock()
        self.error_messages = []
        
        self.logger.info(f"ğŸš€ DatalakeProcessor ì´ˆê¸°í™” (ë³‘ë ¬: {self.num_proc}, ë°°ì¹˜: {batch_size})")
 
    def get_status(self) -> Dict:
        """ê°„ë‹¨í•œ ìƒíƒœ ì¡°íšŒ"""
        return {
            "pending": len(list(self.staging_pending_path.glob("*"))) if self.staging_pending_path.exists() else 0,
            "processing": len(list(self.staging_processing_path.glob("*"))) if self.staging_processing_path.exists() else 0,
            "failed": len(list(self.staging_failed_path.glob("*"))) if self.staging_failed_path.exists() else 0
        }
    
    def process_all_pending(self) -> Dict:
        """ëª¨ë“  Pending ë°ì´í„° ì²˜ë¦¬ (ì—ëŸ¬ ì •ë³´ í¬í•¨)"""
        self.logger.info("ğŸ”„ Pending ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        if not self.staging_pending_path.exists():
            return self._create_processing_result(message="Pending ë””ë ‰í† ë¦¬ ì—†ìŒ")
        
        pending_dirs = [
            d for d in self.staging_pending_path.iterdir()
            if d.is_dir() and (d / "upload_metadata.json").exists()
        ]
        
        if not pending_dirs:
            return self._create_processing_result(message="ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ")
        
        self.logger.info(f"ğŸ“¦ ì²˜ë¦¬ ëŒ€ìƒ: {len(pending_dirs)}ê°œ")
        
        success_count = 0
        failed_count = 0
        success_details = []
        failed_details = []
        error_summary = []
        
        for pending_dir in pending_dirs:
            processing_dir = None
            dir_name = pending_dir.name
            
            try:
                # processingìœ¼ë¡œ ì´ë™
                processing_dir = self.staging_processing_path / dir_name
                shutil.move(str(pending_dir), str(processing_dir))
                
                # ì²˜ë¦¬ ì‹¤íŒ¨ í”Œë˜ê·¸ ì´ˆê¸°í™”
                self.processing_failed = False
                self.error_messages = []
                
                # ì²˜ë¦¬
                self._process_single_directory(processing_dir)
                
                # ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
                if self.processing_failed or self.error_messages:
                    # ë‚´ë¶€ ì²˜ë¦¬ ì‹¤íŒ¨
                    error_msg = "; ".join(self.error_messages) if self.error_messages else "ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                    raise Exception(f"ë‚´ë¶€ ì²˜ë¦¬ ì‹¤íŒ¨: {error_msg}")
                
                # ì„±ê³µ ì‹œ ì •ë¦¬
                shutil.rmtree(processing_dir)
                success_count += 1
                
                success_details.append({
                    "directory": dir_name,
                    "status": "success",
                    "timestamp": datetime.now().isoformat(),
                })
                
                self.logger.info(f"âœ… ì™„ë£Œ: {dir_name}")
                
            except Exception as e:
                failed_count += 1
                error_msg = str(e)
                
                # ìƒì„¸ ì—ëŸ¬ ì •ë³´ ìˆ˜ì§‘
                error_info = {
                    "directory": dir_name,
                    "error": error_msg,
                    "error_type": type(e).__name__,
                    "timestamp": datetime.now().isoformat(),
                }
                
                failed_details.append(error_info)
                error_summary.append(f"{dir_name}: {error_msg}")
                
                self.logger.error(f"âŒ ì‹¤íŒ¨: {dir_name} - {error_msg}")
                self._move_to_failed(processing_dir, dir_name, error_info)
                
        self._cleanup_processing_dirs()
        
        return self._create_processing_result(
            success_count=success_count,
            failed_count=failed_count,
            success_details=success_details,
            failed_details=failed_details,
            error_summary=error_summary
        )

    def validate_assets(
        self,
        user_id: str,
        search_data: List[Dict],
        sample_percent: Optional[float] = None,
        chunk_size: int = 10000,
    ) -> Dict:
        """NAS íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ (DataFrame ê¸°ë°˜)"""
        self.logger.info(f"ğŸ” íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ ì‹œì‘ - ì‚¬ìš©ì: {user_id}, ë°ì´í„°: {len(search_data)}ê°œ")
        
        try:
            if not search_data:
                return self._create_validation_result(
                    user_id=user_id,
                    message="ê²€ì‚¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
                )
            
            total_items = len(search_data)
            
            # ìƒ˜í”Œë§
            if sample_percent and sample_percent < 1.0:
                sample_size = int(total_items * sample_percent)
                search_data = random.sample(search_data, sample_size)
                self.logger.info(f"ğŸ“Š ìƒ˜í”Œ ê²€ì‚¬: {len(search_data):,}ê°œ ({sample_percent*100:.1f}%)")
            
            # Datasetìœ¼ë¡œ ë³€í™˜
            total_checked = 0
            total_missing = 0
            missing_files = []
            total_batches = (len(search_data) - 1) // chunk_size + 1
            
            self.logger.info(f"ğŸ“Š ì´ ë°°ì¹˜ ìˆ˜: {total_batches}, ì²­í¬ í¬ê¸°: {chunk_size}")
            
            for i in range(0, len(search_data), chunk_size):
                chunk_data = search_data[i:i + chunk_size]
                batch_num = i // chunk_size + 1
                self.logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì‹œì‘ - ì²­í¬ ë°ì´í„°: {len(chunk_data)}ê°œ")
                
                try:
                    chunk_df = pd.DataFrame(chunk_data)
                    self.logger.debug(f"ğŸ“Š DataFrame ìƒì„± ì™„ë£Œ: {len(chunk_df)}í–‰, ì»¬ëŸ¼: {chunk_df.columns.tolist()}")
                    chunk_dataset = Dataset.from_pandas(chunk_df)
                    self.logger.debug(f"ğŸ“Š Dataset ìƒì„± ì™„ë£Œ: {len(chunk_dataset)}ê°œ")
                    
                    self.logger.debug(f"ğŸ” í•„í„°ë§ ì „ ì²« ë²ˆì§¸ ë°ì´í„°: {chunk_dataset[0] if len(chunk_dataset) > 0 else 'None'}")
                    filtered_dataset  = chunk_dataset.filter(
                        lambda x: x.get('hash') and x.get('path'),
                        desc=f"ë°°ì¹˜ {batch_num} í•„ë“œ í•„í„°ë§"
                    )
                    
                    self.logger.info(f"ğŸ” ë°°ì¹˜ {batch_num} í•„í„°ë§ ê²°ê³¼: {len(chunk_dataset)} â†’ {len(filtered_dataset)}")
                    if len(filtered_dataset) == 0:
                        self.logger.warning(f"âš ï¸ ë°°ì¹˜ {batch_num}: í•„í„°ë§ í›„ ë°ì´í„° ì—†ìŒ - continue")
                        del chunk_df, chunk_dataset, filtered_dataset
                        continue
                    
                    self.logger.debug(f"ğŸ“ ë°°ì¹˜ {batch_num} íŒŒì¼ ì¡´ì¬ í™•ì¸ ì‹œì‘...")
                    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    checked_dataset = filtered_dataset.map(
                        self._check_file_exists,
                        desc=f"ë°°ì¹˜ {batch_num} íŒŒì¼ í™•ì¸",
                        num_proc=self.num_proc, 
                        load_from_cache_file=False
                    )
                    self.logger.debug(f"ğŸ“ ë°°ì¹˜ {batch_num} íŒŒì¼ í™•ì¸ ì™„ë£Œ: {len(checked_dataset)}ê°œ")
                    
                    missing_dataset = checked_dataset.filter(
                        lambda x: not x['file_exists'],
                        desc=f"ë°°ì¹˜ {batch_num} ëˆ„ë½ í•„í„°ë§"
                    )
                    
                    total_checked += len(filtered_dataset)
                    batch_missing_count = len(missing_dataset)
                    total_missing += batch_missing_count
                    missing_files.extend(missing_dataset.to_list())
                    
                    self.logger.info(f"âœ… ë°°ì¹˜ ì™„ë£Œ: ê²€ì‚¬={len(filtered_dataset)}, ëˆ„ë½={batch_missing_count}")
                    
                    del chunk_df, chunk_dataset, filtered_dataset, checked_dataset, missing_dataset
                    gc.collect()
                except Exception as batch_error:
                    self.logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {batch_error}")
                    continue
                
            self.logger.info(f"ğŸ ì „ì²´ ê²€ì‚¬ ì™„ë£Œ: ì´ ë°°ì¹˜ {total_batches}ê°œ ì²˜ë¦¬ë¨")

            return self._create_validation_result(
                user_id=user_id,
                total_items=total_items,
                checked_items=total_checked,
                missing_files=missing_files,
                message="íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ ì™„ë£Œ"
            )
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_validation_result(
                user_id=user_id,
                message="íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                error=str(e)
            ) 
    
    def _check_file_exists(self, example):
        """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        path_val = example.get('path')
        if not path_val:
            example['file_exists'] = False
            return example
        
        file_path = self.assets_path / path_val
        exists = file_path.exists()
        example['file_exists'] = exists
        
        if not exists:
            example['checked_path'] = str(file_path)
            
        return example
    
    def _create_validation_result(
        self,
        user_id: str,
        total_items: int = 0,
        checked_items: int = 0, 
        missing_files: List[Dict] = None,
        message: str = "ê²€ì‚¬ ì™„ë£Œ",  # ê¸°ë³¸ ë©”ì‹œì§€
        error: str = None  # ğŸ”¥ ì—ëŸ¬ë„ í•¨ê»˜ ì²˜ë¦¬
    ) -> Dict:
        """ìœ íš¨ì„± ê²€ì‚¬ ê²°ê³¼ ìƒì„± (ì„±ê³µ/ì‹¤íŒ¨ í†µí•©)"""
        missing_files = missing_files or []
        missing_count = len(missing_files)
        integrity_rate = ((checked_items - missing_count) / checked_items * 100) if checked_items > 0 else 0
        
        result = {
            'user_id': user_id,
            'total_items': total_items,
            'checked_items': checked_items,
            'missing_files': missing_files[:100],
            'missing_count': missing_count,
            'integrity_rate': round(integrity_rate, 2),
            'message': message,
        }
        
        if error:
            result['error'] = error
        return result
    
    def _create_processing_result(
        self, 
        success_count: int = 0, 
        failed_count: int = 0, 
        success_details: List[Dict] = None,
        failed_details: List[Dict] = None,
        error_summary: List[str] = None,
        message: str = "ì²˜ë¦¬ ì™„ë£Œ"  # ê¸°ë³¸ê°’ë§Œ ì œê³µ
    ) -> Dict:
        """ì²˜ë¦¬ ê²°ê³¼ ìƒì„± (ìˆœìˆ˜í•˜ê²Œ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ë§Œ)"""
        success_details = success_details or []
        failed_details = failed_details or []
        error_summary = error_summary or []
        
        total_processed = success_count + failed_count
        success_rate = f"{(success_count/total_processed*100):.1f}%" if total_processed > 0 else "0%"
        
        # ì—ëŸ¬ ë¶„ì„ë§Œ
        most_common_errors = []
        if failed_details:
            error_types = [detail.get("error_type", "Unknown") for detail in failed_details]
            most_common = Counter(error_types).most_common(3)
            most_common_errors = [{"error_type": et, "count": c} for et, c in most_common]
        
        return {
            "success": success_count,
            "failed": failed_count,
            "total_processed": total_processed,
            "message": message,  # ê·¸ëƒ¥ ë°›ì€ ê·¸ëŒ€ë¡œ
            "summary": {
                "success_rate": success_rate,
                "most_common_errors": most_common_errors,
                "processing_time": datetime.now().isoformat(),
            },
            "success_details": success_details,
            "failed_details": failed_details,
            "errors": error_summary,
        }

    def _move_to_failed(self, processing_dir: Path, dir_name: str, error_info: Dict):
        """ì‹¤íŒ¨í•œ ë””ë ‰í† ë¦¬ë¥¼ failedë¡œ ì´ë™"""
        if processing_dir and processing_dir.exists():
            failed_dir = self.staging_failed_path / dir_name
            failed_dir.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
            try:
                error_file = failed_dir.parent / f"{dir_name}_error.json"
                shutil.move(str(processing_dir), str(failed_dir))
                
                # ì—ëŸ¬ ì •ë³´ ì €ì¥
                with open(error_file, 'w', encoding='utf-8') as f:
                    json.dump(error_info, f, ensure_ascii=False, indent=2)
                    
            except Exception as move_error:
                error_info["move_error"] = str(move_error)
                self.logger.error(f"Failed ë””ë ‰í† ë¦¬ ì´ë™ ì‹¤íŒ¨: {move_error}")
    
    def _cleanup_processing_dirs(self):
        """ì²˜ë¦¬ ì¤‘ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
        remain_processing_dirs = [
            d for d in self.staging_processing_path.iterdir()
            if d.is_dir() and not (d / "upload_metadata.json").exists()
        ]
        
        for remain_dir in remain_processing_dirs:
            try:
                shutil.rmtree(remain_dir)
                self.logger.info(f"âœ… ì²˜ë¦¬ ì¤‘ ë””ë ‰í† ë¦¬ ì •ë¦¬: {remain_dir.name}")
            except Exception as e:
                self.logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {remain_dir.name} - {str(e)}")
                
    def _initialize(self, log_level: str = "INFO", create_dirs: bool = True):
        
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
            'collections': self.collections_path,
        }
        if create_dirs:
            for path_name, path_obj in required_paths.items():
                if not path_obj.exists():
                    path_obj.mkdir(mode=0o777, parents=True, exist_ok=True)
                    self.logger.debug(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {path_name}")
        else:
            missing_paths = []
            for path_name, path_obj in required_paths.items():
                if not path_obj.exists():
                    missing_paths.append(f"  - {path_name}: {path_obj}")
            
            if missing_paths:
                missing_list = '\n'.join(missing_paths)
                raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
            
        setup_logging(
            user_id="processor",
            log_level=log_level, 
            base_path=str(self.base_path)
        )
        self.logger = logging.getLogger(__name__)
        self.logger.debug("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")
        
    def _process_single_directory(self, processing_dir: Path):
        """ë‹¨ì¼ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ - datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©"""
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        metadata_file = processing_dir / "upload_metadata.json"
        if not metadata_file.exists():
            raise ValueError("ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        
        with open(metadata_file, encoding='utf-8') as f:
            metadata = json.load(f)
        
        # datasetsë¡œ ë¡œë“œ
        dataset_obj = load_from_disk(str(processing_dir))
        self.logger.info(f"{processing_dir.name} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)}ê°œ í–‰")
        self.logger.debug(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {dataset_obj.column_names}")
        
        provider = metadata['provider']
        dataset_name = metadata['dataset']
        assets_base = self.assets_path / f"provider={provider}" / f"dataset={dataset_name}"
        # í•´ì‹œ ìºì‹œ êµ¬ì¶• (ê³µí†µ)
        self._build_hash_cache(assets_base)

        # ì´ë¯¸ì§€ ì²˜ë¦¬
        if metadata.get('has_images', False) and self.image_data_key in dataset_obj.column_names:
            dataset_obj = self._process_images_with_map(dataset_obj, metadata, assets_base)
        
        # íŒŒì¼ ì²˜ë¦¬
        if metadata.get('has_files', False) and self.file_path_key in dataset_obj.column_names:
            dataset_obj = self._process_files_with_map(dataset_obj, metadata, assets_base)
        
        # Catalogì— ì €ì¥
        self._save_to_catalog(dataset_obj, metadata)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del dataset_obj
        gc.collect()
    
    def _process_images_with_map(self, dataset_obj: Dataset, metadata: Dict, assets_base: Path) -> Dataset:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (PIL Image/bytes â†’ hash.jpg)"""
        total_images = len(dataset_obj)
        self.logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘: {self.image_data_key} ({total_images}ê°œ)")

        shard_config = self._get_shard_config(total_images)
        self.logger.info(f"ğŸ”§ ìƒ¤ë”© ì„¤ì •: {shard_config}")
        
        dataset_obj = dataset_obj.cast_column(self.image_data_key, ImageFeature())
        assets_base.mkdir(mode=0o775, parents=True, exist_ok=True)
        process_batch_func = partial(
            self._process_image_batch,
            assets_base=assets_base,
            shard_config=shard_config
        )

        try:
            processed_dataset = dataset_obj.map(
                process_batch_func,
                batched=True,
                batch_size=self.batch_size,
                num_proc=min(self.num_proc, total_images // self.batch_size + 1),  # ìµœì†Œ 1ê°œ í”„ë¡œì„¸ìŠ¤
                remove_columns=[self.image_data_key],  # ì›ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì œê±°
                desc="ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬",
                load_from_cache_file=False,  # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            )
            self.logger.debug(f"ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì»¬ëŸ¼: {processed_dataset.column_names}")
            # ì²˜ë¦¬ ì¤‘ ì‹¤íŒ¨ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
            if self.processing_failed:
                error_summary = f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {'; '.join(self.error_messages[:5])}"
                raise RuntimeError(error_summary)
                
            self.logger.info(f"âœ… ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {len(processed_dataset)}ê°œ")
            return processed_dataset
            
        except Exception as e:
            self.logger.error(f"âŒ datasets.map() ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
        
    def _process_files_with_map(self, dataset_obj: Dataset, metadata: Dict, assets_base: Path) -> Dataset:
        """íŒŒì¼ ì²˜ë¦¬ (staging/assets â†’ final/assets + hash)"""
        total_files = len(dataset_obj)
        self.logger.info(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {self.file_path_key} ({total_files}ê°œ)")
        
        shard_config = self._get_shard_config(total_files)
        self.logger.info(f"ğŸ”§ ìƒ¤ë”© ì„¤ì •: {shard_config}")
        assets_base.mkdir(mode=0o775, parents=True, exist_ok=True)
        process_batch_func = partial(
            self._process_file_batch,
            assets_base=assets_base,
            shard_config=shard_config,
        )
        
        try:
            processed_dataset = dataset_obj.map(
                process_batch_func,
                batched=True,
                batch_size=self.batch_size,
                num_proc=min(self.num_proc, total_files // self.batch_size + 1),  # ìµœì†Œ 1ê°œ í”„ë¡œì„¸ìŠ¤
                remove_columns=[self.file_path_key],  # ì›ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ ì œê±°
                desc="ğŸ“„ íŒŒì¼ ì´ë™",
                load_from_cache_file=False,
            )
            
            self.logger.info(f"âœ… íŒŒì¼ ì´ë™ ì™„ë£Œ: {len(processed_dataset)}ê°œ")
            return processed_dataset
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _process_image_batch(self, batch: Dict, assets_base: Path, shard_config: Dict) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ ì´ë¯¸ì§€ ì²˜ë¦¬ (PIL Image/bytes â†’ hash.jpg)"""
        
        input_images = batch[self.image_data_key]
        self.logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬: {len(input_images)}ê°œ ì´ë¯¸ì§€")
        
        output_hashes = []
        output_paths = []
        saved_count = 0
        duplicate_count = 0
        
        for idx, raw_image_data in enumerate(input_images):
            try:
                if self.processing_failed:
                    break
                
                if raw_image_data is None:
                    output_hashes.append(None)
                    output_paths.append(None)
                    continue
                
                # PIL Imageë¡œ ë³€í™˜
                if hasattr(raw_image_data, 'save'):
                    pil_image = raw_image_data
                else:
                    pil_image = Image.open(io.BytesIO(raw_image_data))
                
                # í•´ì‹œ ê³„ì‚° ë° ëª©ì ì§€ ê²½ë¡œ ìƒì„±
                file_hash = self._get_image_hash(pil_image)
                target_file_path = self._get_level_path(assets_base, shard_config, file_hash)
                
                # ì¤‘ë³µ ì´ë¯¸ì§€ ì²˜ë¦¬
                if file_hash in self.existing_hashes:
                    duplicate_count += 1    
                else:
                    target_file_path.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
                    
                    if pil_image.mode != 'RGB':
                        pil_image = pil_image.convert('RGB')
                    pil_image.save(str(target_file_path), 'JPEG', quality=95)
                    
                    with self.cache_lock:
                        self.existing_hashes.add(file_hash)
                    
                    saved_count += 1
                
                # ê²°ê³¼ ì €ì¥ (assets ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ)
                relative_target_path = str(target_file_path.relative_to(self.assets_path))
                output_hashes.append(file_hash)
                output_paths.append(relative_target_path)
                
            except Exception as e:
                with self.failure_lock:
                    if not self.processing_failed:
                        self.processing_failed = True
                        error_msg = f"ì´ë¯¸ì§€ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                        self.error_messages.append(error_msg)
                        self.logger.error(f"âŒ {error_msg}")
                
                raise RuntimeError(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        if saved_count > 0 or duplicate_count > 0:
            self.logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬: ì €ì¥={saved_count}, ì¤‘ë³µ={duplicate_count}")
        
        return {
            "path": output_paths,
            "hash": output_hashes,
        }
        
    def _process_file_batch(self, batch: Dict, assets_base: Path, shard_config: Dict) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ íŒŒì¼ ì²˜ë¦¬ (staging/assets â†’ final/assets + hash)"""
        
        input_file_paths = batch[self.file_path_key]
        self.logger.debug(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬: {len(input_file_paths)}ê°œ")
        
        output_hashes = []
        output_paths = []
        saved_count = 0
        duplicate_count = 0
        
        for idx, relative_path in enumerate(input_file_paths):
            try:
                if self.processing_failed:
                    break
                
                if relative_path is None:
                    output_hashes.append(None)
                    output_paths.append(None)
                    continue
                
                # stagingì—ì„œ íŒŒì¼ ì½ê¸°
                source_file_path = self.staging_processing_path / relative_path
                if not source_file_path.exists():
                    raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_file_path}")
                
                # í•´ì‹œ ê³„ì‚° ë° ëª©ì ì§€ ê²½ë¡œ ìƒì„±
                file_hash = self._get_file_hash(source_file_path)
                target_file_path = self._get_level_path(assets_base, shard_config, file_hash)
                
                # ì¤‘ë³µ íŒŒì¼ ì²˜ë¦¬
                if file_hash in self.existing_hashes:
                    duplicate_count += 1
                else:
                    target_file_path.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
                    shutil.move(str(source_file_path), str(target_file_path))
                    with self.cache_lock:
                        self.existing_hashes.add(file_hash)
                    saved_count += 1
                
                # ê²°ê³¼ ì €ì¥ (assets ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ)
                relative_target_path = str(target_file_path.relative_to(self.assets_path))
                output_hashes.append(file_hash)
                output_paths.append(relative_target_path)
                    
            except Exception as e:
                with self.failure_lock:
                    if not self.processing_failed:
                        self.processing_failed = True
                        error_msg = f"íŒŒì¼ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                        self.error_messages.append(error_msg)
                        self.logger.error(f"âŒ {error_msg}")
                
                raise RuntimeError(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        if saved_count > 0 or duplicate_count > 0:
            self.logger.debug(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬: ì €ì¥={saved_count}, ì¤‘ë³µ={duplicate_count}")

        return {
            "path": output_paths,
            "hash": output_hashes
        }
        
    def _build_hash_cache(self, assets_base: Path):
        """ê¸°ì¡´ ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ êµ¬ì¶•"""
        if self.cache_built:
            return
            
        with self.cache_lock:
            if self.cache_built:
                return
            start_time = time.time()
            
            # ëª¨ë“  .jpg íŒŒì¼ì—ì„œ í•´ì‹œ ì¶”ì¶œ
            for image_file in assets_base.rglob("*.jpg"):
                hash_from_filename = image_file.stem
                if len(hash_from_filename) == 64:  # SHA256 ê¸¸ì´ ê²€ì¦
                    self.existing_hashes.add(hash_from_filename)
            
            build_time = time.time() - start_time
            self.logger.info(f"ğŸ” ê¸°ì¡´ ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ êµ¬ì¶• ì™„ë£Œ: {len(self.existing_hashes)}ê°œ, ì‹œê°„: {build_time:.2f}ì´ˆ")
            self.cache_built = True
    
    @staticmethod
    def _get_image_hash(pil_image: Image.Image) -> str:
        """ì´ë¯¸ì§€ í•´ì‹œ ê³„ì‚°"""
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        img_buffer = io.BytesIO()
        pil_image.save(img_buffer, format='JPEG', quality=95)
        jpeg_bytes = img_buffer.getvalue()
        return hashlib.sha256(jpeg_bytes).hexdigest()
    @staticmethod
    def _get_file_hash(file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (SHA256)"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    @staticmethod
    def _get_shard_config(total_images: int) -> Dict:
        
        if total_images < 10000:
            # ìƒ¤ë”© ì—†ìŒ
            return {"levels": 0, "dirs": 1}
        elif total_images < 2500000:  # 256 * 10000
            # 1ë‹¨ê³„: xx/ (256ê°œ í´ë”)
            return {"levels": 1, "dirs": 256}
        else:
            # 2ë‹¨ê³„: xx/xx/ (65536ê°œ í´ë”)  
            return {"levels": 2, "dirs": 65536}
    @staticmethod
    def _get_level_path(base_path: Path, shard_config: Dict, image_hash: str) -> Path:
        
        levels = shard_config["levels"]
        
        if levels == 0:
            return base_path / f"{image_hash}.jpg"
        elif levels == 1:
            return base_path / image_hash[:2] / f"{image_hash}.jpg"
        elif levels == 2:  
            return base_path / image_hash[:2] / image_hash[2:4] / f"{image_hash}.jpg"
    
    def _save_to_catalog(self, dataset_obj: Dataset, metadata: Dict):
        provider = metadata['provider']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        
        output_dir = (
            self.catalog_path /
            f"provider={provider}" /
            f"dataset={dataset_name}" /
            f"task={task}" /
            f"variant={variant}"
        )
        output_dir.mkdir(mode=0o775, parents=True, exist_ok=True)
        
        # Parquet ì €ì¥ (datasets ë‚´ì¥ ìµœì í™”)
        parquet_file = output_dir / "data.parquet"
        dataset_obj.to_parquet(str(parquet_file))
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_file = output_dir / "_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # íŒŒì¼ í¬ê¸° ë¡œê·¸
        file_size_mb = parquet_file.stat().st_size / (1024 * 1024)
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {parquet_file.name} ({file_size_mb:.1f}MB, {len(dataset_obj)}í–‰)")
        
if __name__ == "__main__":
    # datasets.map() í™œìš© ë²„ì „
    processor = DatalakeProcessor(
        batch_size=1000,    # map()ì˜ ë°°ì¹˜ í¬ê¸°
        num_proc=4         # ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜
    )
    
    # ìƒíƒœ í™•ì¸
    status = processor.get_status()
    print(f"í˜„ì¬ ìƒíƒœ: {status}")
    
    # ì²˜ë¦¬ ì‹¤í–‰
    result = processor.process_all_pending()
    print(f"ì²˜ë¦¬ ê²°ê³¼: {result}")