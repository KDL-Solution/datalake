import logging
import json
import shutil
import hashlib
import io
import threading
import time
import gc
from collections import Counter
from datetime import datetime
from tqdm import tqdm
from pathlib import Path
from typing import Dict, Optional, List
from PIL import Image
from datasets import Dataset, load_from_disk
from datasets.features import Image as ImageFeature
from functools import partial

from utils.logging import setup_logging

class NASDataProcessor:
    
    def __init__(
        self,
        base_path: str = "/mnt/AI_NAS/datalake/",
        log_level: str = "INFO",
        num_proc: int = 4,
        batch_size: int = 1000,  # map()ì˜ ë°°ì¹˜ í¬ê¸°
    ):
        # ê²½ë¡œ ì„¤ì •
        self.base_path = Path(base_path)
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        
        self.catalog_path = self.base_path / "catalog"
        self.assets_path = self.base_path / "assets"
        
        self.num_proc = num_proc
        self.batch_size = batch_size
        
        # LocalDataManagerì™€ ë™ì¼
        self.image_data_key = 'image'  # ê¸°ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‚¤
        self.file_path_key = 'file_path'  # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ í‚¤
        
        self._check_path_and_setup_logging(log_level)
        
        self.existing_hashes = set()
        self.cache_built = False
        self.cache_lock = threading.Lock()
        
        # ì²˜ë¦¬ ì‹¤íŒ¨ ì¶”ì ìš©
        self.processing_failed = False
        self.failure_lock = threading.Lock()
        self.error_messages = []
        
        self.logger.info(f"ğŸš€ NASDataProcessor ì´ˆê¸°í™” (ë³‘ë ¬: {self.num_proc}, ë°°ì¹˜: {batch_size})")
 
    def get_status(self) -> Dict:
        """ê°„ë‹¨í•œ ìƒíƒœ ì¡°íšŒ"""
        return {
            "pending": len(list(self.staging_pending_path.glob("*"))) if self.staging_pending_path.exists() else 0,
            "processing": len(list(self.staging_processing_path.glob("*"))) if self.staging_processing_path.exists() else 0,
            "failed": len(list(self.staging_failed_path.glob("*"))) if self.staging_failed_path.exists() else 0
        }
    
    def process_all_pending(self) -> Dict:
        """ëª¨ë“  Pending ë°ì´í„° ì²˜ë¦¬ (ì—ëŸ¬ ì •ë³´ í¬í•¨)"""
        self.logger.info("ğŸ”„ Pending ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        if not self.staging_pending_path.exists():
            return {
                "success": 0, 
                "failed": 0, 
                "message": "Pending ë””ë ‰í† ë¦¬ ì—†ìŒ",
                "errors": [],
                "success_details": [],
                "failed_details": []
            }
        
        pending_dirs = [
            d for d in self.staging_pending_path.iterdir()
            if d.is_dir() and (d / "upload_metadata.json").exists()
        ]
        
        if not pending_dirs:
            return {
                "success": 0, 
                "failed": 0, 
                "message": "ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ",
                "errors": [],
                "success_details": [],
                "failed_details": []
            }
        
        self.logger.info(f"ğŸ“¦ ì²˜ë¦¬ ëŒ€ìƒ: {len(pending_dirs)}ê°œ")
        
        success_count = 0
        failed_count = 0
        success_details = []
        failed_details = []
        error_summary = []
        
        for pending_dir in pending_dirs:
            processing_dir = None
            dir_name = pending_dir.name
            
            try:
                # processingìœ¼ë¡œ ì´ë™
                processing_dir = self.staging_processing_path / dir_name
                shutil.move(str(pending_dir), str(processing_dir))
                
                # ì²˜ë¦¬ ì‹¤íŒ¨ í”Œë˜ê·¸ ì´ˆê¸°í™”
                self.processing_failed = False
                self.error_messages = []
                
                # ì²˜ë¦¬
                self._process_single_directory(processing_dir)
                
                # ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
                if self.processing_failed or self.error_messages:
                    # ë‚´ë¶€ ì²˜ë¦¬ ì‹¤íŒ¨
                    error_msg = "; ".join(self.error_messages) if self.error_messages else "ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                    raise Exception(f"ë‚´ë¶€ ì²˜ë¦¬ ì‹¤íŒ¨: {error_msg}")
                
                # ì„±ê³µ ì‹œ ì •ë¦¬
                shutil.rmtree(processing_dir)
                success_count += 1
                
                success_info = {
                    "directory": dir_name,
                    "status": "success",
                    "timestamp": datetime.now().isoformat(),
                }
                success_details.append(success_info)
                
                self.logger.info(f"âœ… ì™„ë£Œ: {dir_name}")
                
            except Exception as e:
                failed_count += 1
                error_msg = str(e)
                
                # ìƒì„¸ ì—ëŸ¬ ì •ë³´ ìˆ˜ì§‘
                error_info = {
                    "directory": dir_name,
                    "error": error_msg,
                    "error_type": type(e).__name__,
                    "timestamp": datetime.now().isoformat(),
                }
                
                # ì¶”ê°€ ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                try:
                    if processing_dir and processing_dir.exists():
                        error_info["processing_dir_exists"] = True
                        # ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
                        metadata_file = processing_dir / "upload_metadata.json"
                        if metadata_file.exists():
                            error_info["metadata_exists"] = True
                            try:
                                with open(metadata_file, 'r', encoding='utf-8') as f:
                                    metadata = json.load(f)
                                    error_info["metadata_info"] = {
                                        "file_count": len(metadata.get("files", [])),
                                        "upload_time": metadata.get("upload_time", "unknown")
                                    }
                            except:
                                error_info["metadata_read_error"] = True
                        else:
                            error_info["metadata_exists"] = False
                    else:
                        error_info["processing_dir_exists"] = False
                        
                except Exception as context_error:
                    error_info["context_collection_error"] = str(context_error)
                
                failed_details.append(error_info)
                error_summary.append(f"{dir_name}: {error_msg}")
                
                self.logger.error(f"âŒ ì‹¤íŒ¨: {dir_name} - {error_msg}")
                
                # Failed ë””ë ‰í† ë¦¬ë¡œ ì´ë™
                if processing_dir and processing_dir.exists():
                    failed_dir = self.staging_failed_path / dir_name
                    failed_dir.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
                    try:
                        # ì—ëŸ¬ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥
                        error_file = failed_dir.parent / f"{dir_name}_error.json"
                        shutil.move(str(processing_dir), str(failed_dir))
                        
                        # ì—ëŸ¬ ì •ë³´ ì €ì¥
                        with open(error_file, 'w', encoding='utf-8') as f:
                            json.dump(error_info, f, ensure_ascii=False, indent=2)
                            
                    except Exception as move_error:
                        move_error_msg = str(move_error)
                        error_info["move_error"] = move_error_msg
                        self.logger.error(f"Failed ë””ë ‰í† ë¦¬ ì´ë™ ì‹¤íŒ¨: {move_error_msg}")
        remain_processing_dirs = [
            d for d in self.staging_processing_path.iterdir()
            if d.is_dir() and not (d / "upload_metadata.json").exists()
        ]
        for remain_dir in remain_processing_dirs:
            try:
                shutil.rmtree(remain_dir)
                self.logger.info(f"âœ… ì²˜ë¦¬ ì¤‘ ë””ë ‰í† ë¦¬ ì •ë¦¬: {remain_dir.name}")
            except Exception as e:
                self.logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {remain_dir.name} - {str(e)}")
        most_common_errors = []
        if failed_details:
            error_types = [detail.get("error_type", "Unknown") for detail in failed_details]
            most_common = Counter(error_types).most_common(3)
            most_common_errors = [{"error_type": error_type, "count": count} for error_type, count in most_common]
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "success": success_count,
            "failed": failed_count,
            "total_processed": success_count + failed_count,
            "success_details": success_details,
            "failed_details": failed_details,
            "errors": error_summary,
            "summary": {
                "success_rate": f"{(success_count/(success_count + failed_count)*100):.1f}%" if (success_count + failed_count) > 0 else "0%",
                "most_common_errors": most_common_errors,
                "processing_time": datetime.now().isoformat(),
            }
        }
        
        if failed_count > 0:
            result["message"] = f"ì²˜ë¦¬ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨"
        else:
            result["message"] = f"ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ì„±ê³µ: {success_count}ê°œ"
        
        return result
    
    def _check_path_and_setup_logging(self, log_level: str = "INFO"):
        
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
        }
        
        missing_paths = []
        for path_name, path_obj in required_paths.items():
            if not path_obj.exists():
                missing_paths.append(f"  - {path_name}: {path_obj}")
        
        if missing_paths:
            missing_list = '\n'.join(missing_paths)
            raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
        setup_logging(
            user_id="processor",
            log_level=log_level, 
            base_path=str(self.base_path)
        )
        self.logger = logging.getLogger(__name__)
        self.logger.debug("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")
   
    def _process_single_directory(self, processing_dir: Path):
        """ë‹¨ì¼ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ - datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©"""
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        metadata_file = processing_dir / "upload_metadata.json"
        if not metadata_file.exists():
            raise ValueError("ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        
        with open(metadata_file, encoding='utf-8') as f:
            metadata = json.load(f)
        
        # datasetsë¡œ ë¡œë“œ
        dataset_obj = load_from_disk(str(processing_dir))
        self.logger.info(f"{processing_dir.name} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)}ê°œ í–‰")
        self.logger.debug(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {dataset_obj.column_names}")
        
        provider = metadata['provider']
        dataset_name = metadata['dataset']
        assets_base = self.assets_path / f"provider={provider}" / f"dataset={dataset_name}"
        # í•´ì‹œ ìºì‹œ êµ¬ì¶• (ê³µí†µ)
        self._build_hash_cache(assets_base)

        # ì´ë¯¸ì§€ ì²˜ë¦¬
        if metadata.get('has_images', False) and self.image_data_key in dataset_obj.column_names:
            dataset_obj = self._process_images_with_map(dataset_obj, metadata, assets_base)
        
        # íŒŒì¼ ì²˜ë¦¬
        if metadata.get('has_files', False) and self.file_path_key in dataset_obj.column_names:
            dataset_obj = self._process_files_with_map(dataset_obj, metadata, assets_base)
        
        # Catalogì— ì €ì¥
        self._save_to_catalog(dataset_obj, metadata)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del dataset_obj
        gc.collect()
    
    def _process_images_with_map(self, dataset_obj: Dataset, metadata: Dict, assets_base: Path) -> Dataset:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (PIL Image/bytes â†’ hash.jpg)"""
        total_images = len(dataset_obj)
        self.logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘: {self.image_data_key} ({total_images}ê°œ)")

        shard_config = self._get_shard_config(total_images)
        self.logger.info(f"ğŸ”§ ìƒ¤ë”© ì„¤ì •: {shard_config}")
        
        dataset_obj = dataset_obj.cast_column(self.image_data_key, ImageFeature())
        assets_base.mkdir(mode=0o775, parents=True, exist_ok=True)
        process_batch_func = partial(
            self._process_image_batch,
            assets_base=assets_base,
            shard_config=shard_config
        )

        try:
            processed_dataset = dataset_obj.map(
                process_batch_func,
                batched=True,
                batch_size=self.batch_size,
                num_proc=min(self.num_proc, total_images // self.batch_size + 1),  # ìµœì†Œ 1ê°œ í”„ë¡œì„¸ìŠ¤
                remove_columns=[self.image_data_key],  # ì›ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì œê±°
                desc="ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬",
                load_from_cache_file=False,  # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            )
            self.logger.debug(f"ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì»¬ëŸ¼: {processed_dataset.column_names}")
            # ì²˜ë¦¬ ì¤‘ ì‹¤íŒ¨ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
            if self.processing_failed:
                error_summary = f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {'; '.join(self.error_messages[:5])}"
                raise RuntimeError(error_summary)
                
            self.logger.info(f"âœ… ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {len(processed_dataset)}ê°œ")
            return processed_dataset
            
        except Exception as e:
            self.logger.error(f"âŒ datasets.map() ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
        
        
    def _process_files_with_map(self, dataset_obj: Dataset, metadata: Dict, assets_base: Path) -> Dataset:
        """íŒŒì¼ ì²˜ë¦¬ (staging/assets â†’ final/assets + hash)"""
        print(metadata)
        total_files = len(dataset_obj)
        self.logger.info(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {self.file_path_key} ({total_files}ê°œ)")
        
        shard_config = self._get_shard_config(total_files)
        self.logger.info(f"ğŸ”§ ìƒ¤ë”© ì„¤ì •: {shard_config}")
        assets_base.mkdir(mode=0o775, parents=True, exist_ok=True)
        process_batch_func = partial(
            self._process_file_batch,
            assets_base=assets_base,
            shard_config=shard_config,
        )
        
        try:
            processed_dataset = dataset_obj.map(
                process_batch_func,
                batched=True,
                batch_size=self.batch_size,
                num_proc=min(self.num_proc, total_files // self.batch_size + 1),  # ìµœì†Œ 1ê°œ í”„ë¡œì„¸ìŠ¤
                remove_columns=[self.file_path_key],  # ì›ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ ì œê±°
                desc="ğŸ“„ íŒŒì¼ ì´ë™",
                load_from_cache_file=False,
            )
            
            self.logger.info(f"âœ… íŒŒì¼ ì´ë™ ì™„ë£Œ: {len(processed_dataset)}ê°œ")
            return processed_dataset
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _process_image_batch(self, batch: Dict, assets_base: Path, shard_config: Dict) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ ì´ë¯¸ì§€ ì²˜ë¦¬ (PIL Image/bytes â†’ hash.jpg)"""
        images = batch[self.image_data_key]
        self.logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬: {len(images)}ê°œ ì´ë¯¸ì§€")        
        image_hashes = []
        image_paths = []
        
        saved_count = 0
        duplicate_count = 0
        
        for idx, image_data in enumerate(images):
            try:
                # ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                if self.processing_failed:
                    break
                
                if image_data is None:
                    image_hashes.append(None)
                    image_paths.append(None)
                    continue
                
                # PIL Image ì²˜ë¦¬
                pil_image = image_data if hasattr(image_data, 'save') else Image.open(io.BytesIO(image_data))
                
                image_hash  = self._get_image_hash(pil_image)
                image_path = self._get_level_path(assets_base, shard_config, image_hash)
                if image_hash in self.existing_hashes:
                    duplicate_count += 1    
                else:
                    image_path.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
                    
                    if pil_image.mode != 'RGB':
                        pil_image = pil_image.convert('RGB')
                    pil_image.save(str(image_path), 'JPEG', quality=95)
                    
                    # ìºì‹œì— ì¶”ê°€ (thread-safe)
                    with self.cache_lock:
                        self.existing_hashes.add(image_hash)
                    
                    saved_count += 1
                
                relative_path = str(image_path.relative_to(self.assets_path))
                image_hashes.append(image_hash)
                image_paths.append(relative_path)
                
            except Exception as e:
                # ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì‹¤íŒ¨ë¡œ ë§ˆí‚¹
                with self.failure_lock:
                    if not self.processing_failed:
                        self.processing_failed = True
                        error_msg = f"ì´ë¯¸ì§€ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                        self.error_messages.append(error_msg)
                        self.logger.error(f"âŒ {error_msg}")
                
                # ì‹¤íŒ¨ ì¦‰ì‹œ ì¤‘ë‹¨
                raise RuntimeError(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        # ë¡œê·¸ ì¶œë ¥ (ë°°ì¹˜ë³„)
        if saved_count > 0 or duplicate_count > 0:
            self.logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬: ì €ì¥={saved_count}, ì¤‘ë³µ={duplicate_count}")
        
        return {
            "path": image_paths,
            "hash": image_hashes,
        }
        
    def _process_file_batch(self, batch: Dict, assets_base: Path, shard_config: Dict) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ íŒŒì¼ ì²˜ë¦¬ (staging/assets â†’ final/assets + hash)"""
        
        file_paths = batch[self.file_path_key]
        self.logger.debug(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬: {len(file_paths)}ê°œ")
        file_hashes = []
        new_file_paths = []
        
        saved_count = 0
        duplicate_count = 0
        # print iterdir staging_pending_path
        for idx, file_path in enumerate(file_paths):
            try:
                if self.processing_failed:
                    break
                
                if file_path is None:
                    file_hashes.append(None)
                    file_paths.append(None)
                    continue
                    
                file_path = self.staging_processing_path / file_path
                if not file_path.exists():
                    raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                
                file_hash = self._get_file_hash(file_path)
                new_file_path = self._get_level_path(assets_base, shard_config, file_hash)
                if file_hash in self.existing_hashes:
                    duplicate_count += 1
                else:
                    new_file_path.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
                    shutil.move(str(file_path), str(new_file_path))
                    with self.cache_lock:
                        self.existing_hashes.add(file_hash)
                    saved_count += 1
                relative_path = str(new_file_path.relative_to(self.assets_path))
                file_hashes.append(file_hash)
                new_file_paths.append(relative_path)
                    
            except Exception as e:
                with self.failure_lock:
                    if not self.processing_failed:
                        self.processing_failed = True
                        error_msg = f"íŒŒì¼ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                        self.error_messages.append(error_msg)
                        self.logger.error(f"âŒ {error_msg}")
                
                # ì‹¤íŒ¨ ì¦‰ì‹œ ì¤‘ë‹¨
                raise RuntimeError(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        if saved_count > 0 or duplicate_count > 0:
            self.logger.debug(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬: ì €ì¥={saved_count}, ì¤‘ë³µ={duplicate_count}")

        return {
            "path": new_file_paths,
            "hash": file_hashes
        }
        
    def _build_hash_cache(self, assets_base: Path):
        """ê¸°ì¡´ ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ êµ¬ì¶•"""
        if self.cache_built:
            return
            
        with self.cache_lock:
            if self.cache_built:
                return
            start_time = time.time()
            
            # ëª¨ë“  .jpg íŒŒì¼ì—ì„œ í•´ì‹œ ì¶”ì¶œ
            for image_file in assets_base.rglob("*.jpg"):
                hash_from_filename = image_file.stem
                if len(hash_from_filename) == 64:  # SHA256 ê¸¸ì´ ê²€ì¦
                    self.existing_hashes.add(hash_from_filename)
            
            build_time = time.time() - start_time
            self.logger.info(f"ğŸ” ê¸°ì¡´ ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ êµ¬ì¶• ì™„ë£Œ: {len(self.existing_hashes)}ê°œ, ì‹œê°„: {build_time:.2f}ì´ˆ")
            self.cache_built = True
    
    @staticmethod
    def _get_image_hash(pil_image: Image.Image) -> str:
        """ì´ë¯¸ì§€ í•´ì‹œ ê³„ì‚°"""
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        img_buffer = io.BytesIO()
        pil_image.save(img_buffer, format='JPEG', quality=95)
        jpeg_bytes = img_buffer.getvalue()
        return hashlib.sha256(jpeg_bytes).hexdigest()
    @staticmethod
    def _get_file_hash(file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (SHA256)"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    @staticmethod
    def _get_shard_config(total_images: int) -> Dict:
        
        if total_images < 10000:
            # ìƒ¤ë”© ì—†ìŒ
            return {"levels": 0, "dirs": 1}
        elif total_images < 2500000:  # 256 * 10000
            # 1ë‹¨ê³„: xx/ (256ê°œ í´ë”)
            return {"levels": 1, "dirs": 256}
        else:
            # 2ë‹¨ê³„: xx/xx/ (65536ê°œ í´ë”)  
            return {"levels": 2, "dirs": 65536}
    @staticmethod
    def _get_level_path(base_path: Path, shard_config: Dict, image_hash: str) -> Path:
        
        levels = shard_config["levels"]
        
        if levels == 0:
            return base_path / f"{image_hash}.jpg"
        elif levels == 1:
            return base_path / image_hash[:2] / f"{image_hash}.jpg"
        elif levels == 2:  
            return base_path / image_hash[:2] / image_hash[2:4] / f"{image_hash}.jpg"
    
    def _save_to_catalog(self, dataset_obj: Dataset, metadata: Dict):
        """Catalogì— ì €ì¥"""
        provider = metadata['provider']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        
        # Catalog ê²½ë¡œ
        catalog_dir = (
            self.catalog_path /
            f"provider={provider}" /
            f"dataset={dataset_name}" /
            f"task={task}" /
            f"variant={variant}"
        )
        catalog_dir.mkdir(mode=0o775, parents=True, exist_ok=True)
        
        # Parquet ì €ì¥ (datasets ë‚´ì¥ ìµœì í™”)
        parquet_file = catalog_dir / "data.parquet"
        dataset_obj.to_parquet(str(parquet_file))
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_file = catalog_dir / "_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # íŒŒì¼ í¬ê¸° ë¡œê·¸
        file_size_mb = parquet_file.stat().st_size / (1024 * 1024)
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {parquet_file.name} ({file_size_mb:.1f}MB, {len(dataset_obj)}í–‰)")
        
if __name__ == "__main__":
    # datasets.map() í™œìš© ë²„ì „
    processor = NASDataProcessor(
        batch_size=1000,    # map()ì˜ ë°°ì¹˜ í¬ê¸°
        num_proc=4         # ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜
    )
    
    # ìƒíƒœ í™•ì¸
    status = processor.get_status()
    print(f"í˜„ì¬ ìƒíƒœ: {status}")
    
    # ì²˜ë¦¬ ì‹¤í–‰
    result = processor.process_all_pending()
    print(f"ì²˜ë¦¬ ê²°ê³¼: {result}")