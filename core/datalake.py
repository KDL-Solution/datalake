import logging
import os
import uuid
import json
import shutil
import pandas as pd
import requests 
import time 
import subprocess
import psutil
from pathlib import Path
from datetime import datetime
from datasets import Dataset, load_from_disk
from typing import Dict, Optional, List, Union
from PIL import Image

from core.schema import SchemaManager
from utils.logging import setup_logging
from clients.duckdb_client import DuckDBClient

class DatalakeClient:
    def __init__(
        self, 
        base_path: str = "/mnt/AI_NAS/datalake/",
        nas_api_url: str = "http://192.168.20.62:8091",
        log_level: str = "INFO",
        num_proc: int = 8, # ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
    ):
        self.base_path = Path(base_path)
        self.nas_api_url = nas_api_url.rstrip('/')
        
        # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        self.catalog_path = self.base_path / "catalog"
        self.assets_path  = self.base_path / "assets"
        self.duckdb_path = self.base_path / "db" / "catalog.duckdb"
        
        self.num_proc = num_proc
        self.image_data_candidates = ['image', 'image_bytes']
        self.image_data_key = 'image'  # ê¸°ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‚¤
        self.file_path_candidates = ['image_path', 'file', 'file_path']
        self.file_path_key = 'file_path'  # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ í‚¤
        
        self._check_path_and_setup_logging(log_level)
        self._check_nas_api_connection()

        self.schema_manager = SchemaManager(
            base_path=self.base_path, 
            create_default=True
        )
          
    def upload_raw_data(
        self,
        data_file: str,
        provider: str,
        dataset: str,
        dataset_description: str = "", # ë°ì´í„°ì…‹ ì„¤ëª…
        original_source: str = "", # ì›ë³¸ ì†ŒìŠ¤ URL 
        auto_process: bool = False, # ìë™ ì²˜ë¦¬ ì—¬ë¶€
        overwrite: bool = False, # ê¸°ì¡´ pending ë°ì´í„° ì œê±° ì—¬ë¶€
    ):
        task = "raw"
        
        self.logger.info(f"ğŸ“¥ Raw data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}")
        
        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        existing_dirs =  self._cleanup_existing_pending(provider, dataset, task, is_raw=True)
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        if existing_dirs:
            if not overwrite:
                self.logger.warning(f"âš ï¸ ì´ë¯¸ pending ë°ì´í„°ê°€ ìˆì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {len(existing_dirs)}ê°œ")
                self.logger.info("ğŸ’¡ ë®ì–´ì“°ë ¤ë©´ overwrite=Trueë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
                return None, None  # ë˜ëŠ” ê¸°ì¡´ staging_dir ì •ë³´ ë°˜í™˜
                
            for existing_dir in existing_dirs:
                try:
                    shutil.rmtree(existing_dir)
                    self.logger.info(f"ğŸ—‘ï¸ ì‚­ì œ ì™„ë£Œ: {existing_dir.name}")
                except Exception as e:
                    self.logger.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {existing_dir.name} - {e}")
        
        dataset_obj, file_info = self._load_data(data_file)

        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=file_info['type'],
            total_rows=len(dataset_obj),
            data_type="raw",
            has_images=file_info['has_image_data'],
            has_files= file_info['has_file_paths'],
            dataset_description=dataset_description,
            original_source=original_source,
        )
        
        staging_dir = self._save_to_staging(dataset_obj, metadata, has_file=file_info['has_file_paths'])
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        job_id = None
        if auto_process:
            job_id = self.trigger_nas_processing()
            if job_id:
                self.logger.info(f"ğŸ”„ ìë™ ì²˜ë¦¬ ì‹œì‘ë¨: {job_id}")
        
        return staging_dir, job_id

    def upload_task_data(
        self,
        data_file: Union[str, Path, pd.DataFrame],
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        dataset_description: str = "",
        auto_process: bool = False,
        overwrite: bool = False,
        meta: Optional[Dict] = None,
    ) -> str:
        """Task ë°ì´í„° ì—…ë¡œë“œ (ê¸°ì¡´ catalogì—ì„œ íŠ¹ì • task ì¶”ì¶œ, ì´ë¯¸ì§€ ì°¸ì¡°ë§Œ)"""
        self.logger.info(f"ğŸ“¥ Task data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}/{task}/{variant}")
        
        if not self._check_raw_data_exists(provider, dataset):
            self.logger.warning(f"âš ï¸ Raw ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {provider}/{dataset}")
            self.logger.info("ğŸ’¡ ë¨¼ì € upload_raw_data()ë¡œ ì›ë³¸ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”")
            raise FileNotFoundError(
                f"âŒ Raw ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {provider}/{dataset}"
            )
        
        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        is_valid, error_msg = self.schema_manager.validate_task_metadata(task, meta)
        if not is_valid:
            raise ValueError(f"âŒ Task ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error_msg}")

        # ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬
        existing_dirs =  self._cleanup_existing_pending(provider, dataset, task, variant=variant, is_raw=False)
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        if existing_dirs:
            if not overwrite:
                self.logger.warning(f"âš ï¸ ì´ë¯¸ pending ë°ì´í„°ê°€ ìˆì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {len(existing_dirs)}ê°œ")
                self.logger.info("ğŸ’¡ ë®ì–´ì“°ë ¤ë©´ overwrite=Trueë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
                return None, None  # ë˜ëŠ” ê¸°ì¡´ staging_dir ì •ë³´ ë°˜í™˜
                
            for existing_dir in existing_dirs:
                try:
                    shutil.rmtree(existing_dir)
                    self.logger.info(f"ğŸ—‘ï¸ ì‚­ì œ ì™„ë£Œ: {existing_dir.name}")
                except Exception as e:
                    self.logger.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {existing_dir.name} - {e}")
        
        # ë°ì´í„° ë¡œë“œ ë° ì»¬ëŸ¼ ë³€í™˜ (ì´ë¯¸ì§€ ì œì™¸)
        dataset_obj, file_info = self._load_data(data_file)
        

        columns_to_remove = [key for key in meta.keys()
                            if key in dataset_obj.column_names]
        
        if columns_to_remove:
            dataset_obj = dataset_obj.remove_columns(columns_to_remove)
            self.logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œê±°: {columns_to_remove}")
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=variant,
            dataset_description=dataset_description,
            has_images=file_info['has_image_data'],
            has_files=file_info['has_file_paths'],
            total_rows=len(dataset_obj),
            data_type='task',
            meta=meta,
        )
        
        # Stagingì— ì €ì¥
        staging_dir = self._save_to_staging(dataset_obj, metadata)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        job_id = None
        if auto_process:
            job_id = self.trigger_nas_processing()
            if job_id:
                self.logger.info(f"ğŸ”„ ìë™ ì²˜ë¦¬ ì‹œì‘ë¨: {job_id}")
        
        return staging_dir, job_id
    
    def get_nas_status(self) -> Optional[Dict]:
        """NAS ì„œë²„ ìƒíƒœ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/status", timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def list_nas_jobs(self) -> Optional[List[Dict]]:
        """ëª¨ë“  ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/jobs", timeout=10)
            if response.status_code == 200:
                return response.json().get('jobs', [])
            else:
                self.logger.error(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def show_nas_dashboard(self):
        """NAS ìƒíƒœ ëŒ€ì‹œë³´ë“œ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š NAS Data Processing Dashboard")
        print("="*60)
        
        # ìƒíƒœ ì¡°íšŒ
        status = self.get_nas_status()
        if status:
            print(f"ğŸ“¦ Pending: {status['pending']}ê°œ")
            print(f"ğŸ”„ Processing: {status['processing']}ê°œ")
            print(f"âŒ Failed: {status['failed']}ê°œ")
            print(f"ğŸ–¥ï¸ Server Status: {status['server_status']}")
            print(f"â° Last Updated: {status['last_updated']}")
        else:
            print("âŒ NAS ì„œë²„ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨")
        
        # ì‘ì—… ëª©ë¡
        jobs = self.list_nas_jobs()
        if jobs:
            print(f"\nğŸ“‹ Recent Jobs ({len(jobs)}ê°œ):")
            for job in jobs[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(job['status'], "â“")
                print(f"  {status_emoji} {job['job_id']} - {job['status']} ({job['started_at']})")
        
        print("="*60 + "\n")
        
    def trigger_nas_processing(self) -> Optional[str]:
        """NASì—ì„œ ì²˜ë¦¬ ì‹œì‘"""
        self.logger.info("ğŸ”„ NAS ì²˜ë¦¬ ìš”ì²­ ì¤‘...")
        start_time = time.time()
        try:
            response = requests.post(
                f"{self.nas_api_url}/process", 
                timeout=30,
                headers={'Content-Type': 'application/json'}
            )
            elapsed = time.time() - start_time
            self.logger.debug(f"â±ï¸ NAS ì²˜ë¦¬ ìš”ì²­ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            if response.status_code == 200:
                result = response.json()
                job_id = result.get('job_id')
                status = result.get('status')
                message = result.get('message', '')
                
                if status == 'already_running':
                    self.logger.info("ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤")
                    return job_id
                elif status == 'started':
                    self.logger.info(f"âœ… ì²˜ë¦¬ ì‘ì—… ì‹œì‘ë¨: {job_id}")
                    return job_id
                else:
                    self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ: {status}, ë©”ì‹œì§€: {message}")
                    return job_id
            else:                    
                self.logger.error(f"âŒ ì²˜ë¦¬ ì‹œì‘ ì‹¤íŒ¨: {response.status_code}")
                try:
                    error_detail = response.json().get('detail', response.text)
                    self.logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {error_detail}")
                except:
                    self.logger.error(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
                return None
            
        except requests.exceptions.Timeout:
            elapsed = time.time() - start_time
            self.logger.error(f"âŒ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ({elapsed:.2f}ì´ˆ)")
            return None
        except requests.exceptions.ConnectionError:
            self.logger.error(f"âŒ NAS ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {self.nas_api_url}")
            return None
        except requests.exceptions.RequestException as e:
            elapsed = time.time() - start_time
            self.logger.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
            return None
    
    def get_job_status(self, job_id: str) -> Optional[dict]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/jobs/{job_id}", timeout=10)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 404:
                self.logger.warning(f"âš ï¸ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {job_id}")
                return None
            else:
                self.logger.error(f"âŒ ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def wait_for_job_completion(self, job_id: str, polling_interval: int = 60, timeout: int = 3600) -> dict:
        """ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (í´ë§)"""
        self.logger.info(f"â³ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì¤‘: {job_id}")
        
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            job_status = self.get_job_status(job_id)
            if not job_status:
                raise RuntimeError(f"ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {job_id}")
            
            status = job_status.get('status')
            
            if status == 'completed':
                result = job_status.get('result', {})
                self.logger.info(f"âœ… ì‘ì—… ì™„ë£Œ: {job_id}")
                self.logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì„±ê³µ={result.get('success', 0)}, ì‹¤íŒ¨={result.get('failed', 0)}")
                return job_status
                
            elif status == 'failed':
                error = job_status.get('error', 'Unknown error')
                self.logger.error(f"âŒ ì‘ì—… ì‹¤íŒ¨: {job_id}, ì˜¤ë¥˜: {error}")
                raise RuntimeError(f"ì‘ì—… ì‹¤íŒ¨: {error}")
                
            elif status == 'running':
                self.logger.debug(f"ğŸ”„ ì‘ì—… ì§„í–‰ ì¤‘: {job_id}")
                time.sleep(polling_interval)
            else:
                self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… ìƒíƒœ: {status}")
                time.sleep(polling_interval)
        
        raise TimeoutError(f"ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {job_id}")    

    def get_catalog_info(self) -> Dict:
        """Catalog DB ì •ë³´ ì¡°íšŒ"""
        self.logger.info("ğŸ“Š Catalog DB ì •ë³´ ì¡°íšŒ ì¤‘...")
        
        try:
            if not self.duckdb_path.exists():
                return {
                    'exists': False,
                    'message': 'Catalog DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. build_catalog_db()ë¡œ ìƒì„±í•˜ì„¸ìš”.'
                }
            
            # DB ê¸°ë³¸ ì •ë³´
            db_size = self.duckdb_path.stat().st_size / 1024 / 1024
            db_mtime = datetime.fromtimestamp(self.duckdb_path.stat().st_mtime)
            
            info = {
                'exists': True,
                'path': str(self.duckdb_path),
                'size_mb': round(db_size, 1),
                'modified_time': db_mtime.strftime('%Y-%m-%d %H:%M:%S'),
                'is_outdated': self._is_db_outdated()
            }
            
            with DuckDBClient(str(self.duckdb_path), read_only=True) as duck_client:
                # í…Œì´ë¸” ì •ë³´
                tables = duck_client.list_tables()
                info['tables'] = tables['name'].tolist()
                
                if 'catalog' in info['tables']:
                    # Catalog í…Œì´ë¸” ìƒì„¸ ì •ë³´
                    count_result = duck_client.execute_query("SELECT COUNT(*) as total FROM catalog")
                    total_rows = count_result['total'].iloc[0]
                    info['total_rows'] = total_rows
                    
                    # íŒŒí‹°ì…˜ ì •ë³´
                    try:
                        partitions_df = duck_client.retrieve_partitions("catalog")
                        info['partitions'] = len(partitions_df)
                        
                        # Providerë³„ í†µê³„
                        if not partitions_df.empty:
                            provider_stats = partitions_df.groupby('provider').size().to_dict()
                            info['provider_stats'] = provider_stats
                    except Exception as e:
                        self.logger.warning(f"íŒŒí‹°ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        info['partitions'] = 0
                        info['provider_stats'] = {}
                
            return info
            
        except Exception as e:
            self.logger.error(f"âŒ Catalog DB ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'exists': False,
                'error': str(e)
            }
    
    def build_catalog_db(self, force_rebuild: bool = False) -> bool:
        """Catalog DB êµ¬ì¶• ë˜ëŠ” ì¬êµ¬ì¶•"""
        self.logger.info("ğŸ”¨ Catalog DB êµ¬ì¶• ì‹œì‘...")
        
        try:
            if not self.catalog_path.exists():
                raise FileNotFoundError(f"Catalog ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.catalog_path}")
            
            # ê¸°ì¡´ DB íŒŒì¼ ì²˜ë¦¬
            if self.duckdb_path.exists():
                if force_rebuild:
                    self.logger.info("ğŸ—‘ï¸ ê¸°ì¡´ DB íŒŒì¼ ì‚­ì œ ì¤‘...")
                    self._cleanup_db_files()
                else:
                    self.logger.info("âš ï¸ ê¸°ì¡´ DB íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. force_rebuild=Trueë¡œ ì¬êµ¬ì¶•í•˜ì„¸ìš”.")
                    return False
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            self.duckdb_path.parent.mkdir(mode=0o777, parents=True, exist_ok=True)
            
            # Parquet íŒŒì¼ë“¤ í™•ì¸
            parquet_files = list(self.catalog_path.rglob("*.parquet"))
            if not parquet_files:
                raise FileNotFoundError("Parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            self.logger.info(f"ğŸ“‚ ë°œê²¬ëœ Parquet íŒŒì¼: {len(parquet_files)}ê°œ")
            
            # ìƒˆ DB ìƒì„±
            with DuckDBClient(str(self.duckdb_path), read_only=False) as duck_client:
                parquet_pattern = str(self.catalog_path / "**" / "*.parquet")
                
                self.logger.info("ğŸ“Š Catalog í…Œì´ë¸” ìƒì„± ì¤‘...")
                duck_client.create_table_from_parquet(
                    "catalog",
                    parquet_pattern,
                    hive_partitioning=True,
                    union_by_name=True
                )
                
                # ê²°ê³¼ ê²€ì¦
                count_result = duck_client.execute_query("SELECT COUNT(*) as total FROM catalog")
                total_rows = count_result['total'].iloc[0]
                
                self.logger.info(f"âœ… Catalog DB êµ¬ì¶• ì™„ë£Œ!")
                self.logger.info(f"ğŸ“Š ì´ {total_rows:,}ê°œ í–‰")
                self.logger.info(f"ğŸ’¾ DB íŒŒì¼: {self.duckdb_path}")
                self.logger.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {self.duckdb_path.stat().st_size / 1024 / 1024:.1f}MB")
                
            # ê¶Œí•œ ì„¤ì •
            self.duckdb_path.chmod(0o666)
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Catalog DB êµ¬ì¶• ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
            if self.duckdb_path.exists():
                try:
                    self.duckdb_path.unlink()
                except:
                    pass
            return False
    
    def get_catalog_partitions(self) -> pd.DataFrame:
        """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒí‹°ì…˜ ëª©ë¡ ì¡°íšŒ"""
        self.logger.info("ğŸ” Catalog íŒŒí‹°ì…˜ ì¡°íšŒ ì¤‘...")
        
        try:
            if not self.duckdb_path.exists():
                raise FileNotFoundError("Catalog DBê°€ ì—†ìŠµë‹ˆë‹¤. build_catalog_db()ë¡œ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
                
            with DuckDBClient(str(self.duckdb_path), read_only=True) as duck_client:
                self._validate_catalog_db(duck_client)
                partitions_df = duck_client.retrieve_partitions("catalog")
                
                self.logger.info(f"ğŸ“Š ì´ {len(partitions_df)}ê°œ íŒŒí‹°ì…˜ ì¡°íšŒë¨")
                return partitions_df
                
        except Exception as e:
            self.logger.error(f"âŒ íŒŒí‹°ì…˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
    
    def search_catalog(
        self,
        providers: Optional[List[str]] = None,
        datasets: Optional[List[str]] = None,
        tasks: Optional[List[str]] = None,
        variants: Optional[List[str]] = None,
        text_search: Optional[Dict] = None,
        limit: Optional[int] = None
    ) -> pd.DataFrame:
        """
        Catalogì—ì„œ ë°ì´í„° ê²€ìƒ‰
        
        Args:
            providers: Provider ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            datasets: Dataset ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            tasks: Task ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            variants: Variant ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            text_search: í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì„¤ì • {"column": str, "text": str, "json_path": str}
            limit: ê²°ê³¼ ì œí•œ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ DataFrame
        """
        self.logger.info("ğŸ” Catalog ê²€ìƒ‰ ì‹œì‘")
        
        try:
            if not self.duckdb_path.exists():
                raise FileNotFoundError("Catalog DBê°€ ì—†ìŠµë‹ˆë‹¤. build_catalog_db()ë¡œ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
            
            with DuckDBClient(str(self.duckdb_path), read_only=True) as duck_client:
                self._validate_catalog_db(duck_client)
                
                if text_search:
                    # í…ìŠ¤íŠ¸ ê²€ìƒ‰
                    results = self._perform_text_search(duck_client, text_search, limit)
                else:
                    # íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰
                    results = self._perform_partition_search(
                        duck_client, providers, datasets, tasks, variants, limit
                    )
                
                self.logger.info(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(results):,}ê°œ í•­ëª©")
                return results
                
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise
    
    def _prepare_dataframe(
        self, 
        search_results: pd.DataFrame, 
        absolute_paths: bool = True,
    ) -> pd.DataFrame:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬ìš© DataFrameìœ¼ë¡œ ì¤€ë¹„"""
        df_copy = search_results.copy()
        
        if absolute_paths and 'path' in df_copy.columns:
            df_copy['path'] = df_copy['path'].apply(
                lambda x: str(self.assets_path / x) if isinstance(x, str) and x else x
            )
            self.logger.debug("ğŸ“ ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜")
        
        return df_copy

    def to_pandas(
        self, 
        search_results: pd.DataFrame, 
        absolute_paths: bool = True,
    ) -> pd.DataFrame:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜"""
        self.logger.info("ğŸ“Š Pandas DataFrame ë³€í™˜ ì‹œì‘...")
        
        df_copy = self._prepare_dataframe(search_results, absolute_paths)
        
        self.logger.info(f"âœ… DataFrame ë³€í™˜ ì™„ë£Œ: {len(df_copy):,}ê°œ í•­ëª©")
        return df_copy

    def to_dataset(
        self,
        search_results: pd.DataFrame,
        include_images: bool = False,
        absolute_paths: bool = True,
    ):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ HuggingFace Dataset ê°ì²´ë¡œ ë³€í™˜"""
        self.logger.info("ğŸ“¥ Dataset ê°ì²´ ìƒì„± ì‹œì‘...")
        
        df_copy = self._prepare_dataframe(search_results, absolute_paths)
        dataset = Dataset.from_pandas(df_copy)
        
        if include_images:
            dataset = self._add_images_to_dataset(dataset)
            
        self.logger.info(f"âœ… Dataset ê°ì²´ ìƒì„± ì™„ë£Œ: {len(dataset):,}ê°œ í•­ëª©") 
        return dataset

    def download_as_parquet(
        self, 
        search_results: pd.DataFrame, 
        output_path: Union[str, Path],
        absolute_paths: bool = True,
    ) -> Path:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ Parquetìœ¼ë¡œ ì €ì¥"""
        self.logger.info("ğŸ’¾ Parquet ì €ì¥ ì‹œì‘...")
        
        df_copy = self._prepare_dataframe(search_results, absolute_paths)
        
        output_path = Path(output_path).with_suffix('.parquet')
        output_path.parent.mkdir(parents=True, exist_ok=True)
        df_copy.to_parquet(output_path, index=False)
        
        file_size = output_path.stat().st_size / 1024 / 1024
        self.logger.info(f"âœ… Parquet ì €ì¥ ì™„ë£Œ: {output_path}")
        self.logger.info(f"ğŸ“Š {len(df_copy):,}ê°œ í•­ëª©, {file_size:.1f}MB")
        
        return output_path

    def download_as_dataset(
        self,
        search_results: pd.DataFrame,
        output_path: Union[str, Path], 
        include_images: bool = False,
        absolute_paths: bool = True,
    ) -> Path:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ HuggingFace Datasetìœ¼ë¡œ ì €ì¥"""
        self.logger.info("ğŸ“¥ Dataset ì €ì¥ ì‹œì‘...")
        
        # Dataset ê°ì²´ ìƒì„± (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
        dataset = self.to_dataset(search_results, include_images, absolute_paths)
        
        # ì €ì¥
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        dataset.save_to_disk(str(output_path))
        
        total_size = sum(f.stat().st_size for f in output_path.rglob('*') if f.is_file()) / 1024 / 1024
        
        self.logger.info(f"âœ… Dataset ì €ì¥ ì™„ë£Œ: {output_path}")
        self.logger.info(f"ğŸ“Š {len(dataset):,}ê°œ í•­ëª©, {total_size:.1f}MB")
        
        return output_path
    
    def validate_data_integrity(
        self, 
        search_results: pd.DataFrame,
        sample_percent: Optional[float] = None
    ) -> Dict:
        """
        ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
        
        Args:
            search_results: ê²€ì‚¬í•  ë°ì´í„° (Noneì´ë©´ ì „ì²´ catalog ê²€ì‚¬)
            sample_percent: ìƒ˜í”Œë§ ë¹„ìœ¨ (0.1 = 10%)
            
        Returns:
            ê²€ì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.logger.info("ğŸ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì‹œì‘...")
        
        try:
            if search_results.empty:
                self.logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¬´ê²°ì„± ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return {
                    'total_items': 0,
                    'missing_files': [],
                    'errors': ["ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."]
                }
            else:
                self.logger.info(f"ğŸ“Š ê²€ì‚¬ ëŒ€ìƒ í•­ëª©: {len(search_results):,}ê°œ")
            
            # ìƒ˜í”Œë§
            if sample_percent:
                sample_size = int(len(search_results) * sample_percent)
                search_results = search_results.sample(n=sample_size, random_state=42)
                self.logger.info(f"ğŸ“Š ìƒ˜í”Œ ê²€ì‚¬: {len(search_results):,}ê°œ í•­ëª© ({sample_percent*100:.1f}%)")
            
            dataset = Dataset.from_pandas(search_results)
            dataset = dataset.filter(
                lambda x: x.get('hash') and x.get('path'), 
                desc="í•„ìˆ˜ í•„ë“œ í•„í„°ë§"
            )
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬
            def check_file_exists(example):
                path_val = example.get('path')
                if not path_val:
                    example['file_exists'] = None
                    return example
                
                file_path = self.assets_path / path_val
                exists = file_path.exists()
                example['file_exists'] = exists
                
                return example
            
            # ë³‘ë ¬ ê²€ì‚¬
            dataset_with_check = dataset.map(
                check_file_exists,
                desc="íŒŒì¼ ì¡´ì¬ í™•ì¸",
                num_proc=min(self.num_proc, 8),
                load_from_cache_file=False
            )
            
            # ëˆ„ë½ëœ íŒŒì¼ ì°¾ê¸°
            missing_files_data = dataset_with_check.filter(
                lambda x: not x['file_exists'],
                desc="ëˆ„ë½ íŒŒì¼ í•„í„°ë§"
            )
            
            missing_files = missing_files_data.to_list()
            
            result = {
                'total_items': len(search_results),
                'checked_items': len(dataset),
                'missing_files': missing_files,
                'missing_count': len(missing_files),
                'integrity_rate': (len(dataset) - len(missing_files)) / len(dataset) * 100 if len(dataset) > 0 else 0
            }
            
            self.logger.info(f"ğŸ“Š ë¬´ê²°ì„± ê²€ì‚¬ ì™„ë£Œ:")
            self.logger.info(f"  ì´ í•­ëª©: {result['total_items']:,}ê°œ")
            self.logger.info(f"  ê²€ì‚¬ í•­ëª©: {result['checked_items']:,}ê°œ")
            self.logger.info(f"  ëˆ„ë½ íŒŒì¼: {result['missing_count']:,}ê°œ")
            self.logger.info(f"  ë¬´ê²°ì„± ë¹„ìœ¨: {result['integrity_rate']:.1f}%")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return {
                'total_items': 0,
                'missing_files': [],
                'errors': [str(e)]
            }
            
    def check_db_processes(self):
        """DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
        print("\nğŸ” DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸")
        print("="*50)
        
        db_path = Path(self.duckdb_path).resolve()  # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
        
        try:
            using_processes = []
            
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    # 1. cmdline ê²€ì‚¬ (ê¸°ì¡´ ë°©ì‹)
                    cmdline_match = False
                    if proc.info['cmdline']:
                        cmdline = ' '.join(proc.info['cmdline'])
                        if str(db_path) in cmdline or 'catalog.duckdb' in cmdline:
                            cmdline_match = True
                    
                    # 2. ì—´ë¦° íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ê²€ì‚¬ (ìƒˆë¡œìš´ ë°©ì‹)
                    file_match = False
                    try:
                        process = psutil.Process(proc.info['pid'])
                        open_files = process.open_files()
                        for f in open_files:
                            file_path = Path(f.path).resolve()
                            # DB íŒŒì¼ì´ë‚˜ ê´€ë ¨ íŒŒì¼ë“¤ í™•ì¸
                            if (file_path == db_path or 
                                file_path.name == db_path.name or
                                str(file_path).endswith('.duckdb') or
                                str(file_path).endswith('.duckdb.wal') or
                                str(file_path).endswith('.duckdb.tmp')):
                                file_match = True
                                break
                    except (psutil.AccessDenied, psutil.NoSuchProcess):
                        # ê¶Œí•œì´ ì—†ê±°ë‚˜ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ë¼ì§„ ê²½ìš°
                        pass
                    
                    # 3. ë©”ëª¨ë¦¬ ë§¤í•‘ ê²€ì‚¬ (ì¶”ê°€)
                    memory_match = False
                    try:
                        process = psutil.Process(proc.info['pid'])
                        memory_maps = process.memory_maps()
                        for m in memory_maps:
                            if str(db_path) in m.path:
                                memory_match = True
                                break
                    except (psutil.AccessDenied, psutil.NoSuchProcess, AttributeError):
                        # ì¼ë¶€ ì‹œìŠ¤í…œì—ì„œëŠ” memory_maps()ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                        pass
                    
                    # í•˜ë‚˜ë¼ë„ ë§¤ì¹˜ë˜ë©´ DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤
                    if cmdline_match or file_match or memory_match:
                        match_type = []
                        if cmdline_match: match_type.append("cmdline")
                        if file_match: match_type.append("open_files")
                        if memory_match: match_type.append("memory_map")
                        
                        using_processes.append({
                            'pid': proc.info['pid'],
                            'name': proc.info['name'],
                            'cmdline': cmdline[:100] + '...' if len(cmdline) > 100 else cmdline,
                            'match_type': ', '.join(match_type)
                        })
                        
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            # DB íŒŒì¼ ìƒíƒœ ì •ë³´ë„ í¬í•¨
            db_info = {
                'path': str(db_path),
                'exists': db_path.exists()
            }
            
            if db_path.exists():
                stat = db_path.stat()
                db_info.update({
                    'size': stat.st_size,
                    'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat()
                })
                
                # WAL íŒŒì¼ í™•ì¸
                wal_file = db_path.with_suffix('.duckdb.wal')
                db_info['has_wal'] = wal_file.exists()
            
            result = {
                'processes': using_processes,
                'db_info': db_info
            }
            
            self.logger.info(f"ğŸ“Š DB í”„ë¡œì„¸ìŠ¤ í™•ì¸ ì™„ë£Œ: {len(using_processes)}ê°œ í”„ë¡œì„¸ìŠ¤ ë°œê²¬")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ í”„ë¡œì„¸ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
        
    def _validate_catalog_db(self, duck_client):
        """Catalog DB ìœ íš¨ì„± ê²€ì‚¬"""
        tables = duck_client.list_tables()
        if tables.empty or 'catalog' not in tables['name'].values:
            raise ValueError("catalog í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. build_catalog_db()ë¡œ ìƒì„±í•˜ì„¸ìš”.")
    
    def _is_db_outdated(self) -> bool:
        """DBê°€ ìµœì‹  ìƒíƒœì¸ì§€ í™•ì¸"""
        if not self.duckdb_path.exists():
            return True
            
        db_mtime = self.duckdb_path.stat().st_mtime
        
        # ê°€ì¥ ìµœê·¼ Parquet íŒŒì¼ í™•ì¸
        latest_parquet_mtime = 0
        for parquet_file in self.catalog_path.rglob("*.parquet"):
            file_mtime = parquet_file.stat().st_mtime
            if file_mtime > latest_parquet_mtime:
                latest_parquet_mtime = file_mtime
        
        return latest_parquet_mtime > db_mtime
    
    def _cleanup_db_files(self):
        """DB ê´€ë ¨ íŒŒì¼ë“¤ ì •ë¦¬"""
        files_to_remove = [
            self.duckdb_path,
            self.duckdb_path.with_suffix('.duckdb-wal'),
            self.duckdb_path.with_suffix('.duckdb-shm'),
            self.duckdb_path.with_suffix('.duckdb.wal'),
            self.duckdb_path.with_suffix('.duckdb.tmp'),
            self.duckdb_path.with_suffix('.duckdb.lock')
        ]
        
        for file_path in files_to_remove:
            if file_path.exists():
                try:
                    file_path.unlink()
                    self.logger.debug(f"ğŸ—‘ï¸ ì‚­ì œ: {file_path}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
    
    def _perform_partition_search(
        self, 
        duck_client, 
        providers, 
        datasets, 
        tasks, 
        variants, 
        limit
    ):
        """íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰"""
        return duck_client.retrieve_with_existing_cols(
            providers=providers,
            datasets=datasets, 
            tasks=tasks,
            variants=variants,
            table="catalog",
            limit=limit
        )

    def _perform_text_search(self, duck_client, text_search, limit):
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰"""
        column = text_search.get("column")
        text = text_search.get("text")
        json_path = text_search.get("json_path")
        
        if json_path:
            # JSON ê²€ìƒ‰
            sql = duck_client.json_queries.search_text_in_column(
                table="catalog",
                column=column,
                search_text=text,
                search_type="json",
                json_loc=json_path,
                engine="duckdb"
            )
        else:
            # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰
            sql = duck_client.json_queries.search_text_in_column(
                table="catalog", 
                column=column,
                search_text=text,
                search_type="simple",
                engine="duckdb"
            )
        
        if limit is not None:
            sql += f" LIMIT {limit}"
            
        return duck_client.execute_query(sql)
    
    def _add_images_to_dataset(self, dataset):
        def load_image(example):
            try:
                if example.get('path'):
                    image_path = Path(example['path'])
                
                    if image_path.exists():
                        pil_image = Image.open(image_path)
                        example['image'] = pil_image
                        example['has_valid_image'] = True
                        return example
            except Exception as e:
                self.logger.warning(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {example.get('path', 'unknown')} - {e}")
            
            example['image'] = None
            example['has_valid_image'] = False
            return example
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        self.logger.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...")
        dataset_with_images = dataset.map(
            load_image,
            desc="ì´ë¯¸ì§€ ë¡œë”©",
            num_proc=self.num_proc
        )
        
        # ìœ íš¨í•œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
        valid_dataset = dataset_with_images.filter(
            lambda x: x,
            desc="ìœ íš¨ ì´ë¯¸ì§€ í•„í„°ë§",
            input_columns=['has_valid_image'],
            num_proc=self.num_proc
        )
        
        # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
        valid_dataset = valid_dataset.remove_columns(['has_valid_image'])
        
        total_items = len(dataset)
        valid_items = len(valid_dataset)
        
        self.logger.info(f"ğŸ“Š ì´ë¯¸ì§€ ë¡œë”© ê²°ê³¼: {valid_items:,}/{total_items:,} ì„±ê³µ")
        
        return valid_dataset
    
    def _check_raw_data_exists(self, provider: str, dataset: str) -> bool:
        """í•´ë‹¹ provider/datasetì˜ raw ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        raw_task_path = self.catalog_path / f"provider={provider}" / f"dataset={dataset}" / "task=raw"
        
        # raw task ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ê³ , ê·¸ ì•ˆì— variantê°€ í•˜ë‚˜ ì´ìƒ ìˆëŠ”ì§€ í™•ì¸
        if not raw_task_path.exists():
            return False
        
        # raw ë””ë ‰í† ë¦¬ ì•ˆì— variant í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸ (variant=image, variant=text, variant=mixed ë“±)
        variant_dirs = [d for d in raw_task_path.iterdir() 
                    if d.is_dir() and d.name.startswith("variant=")]
        return len(variant_dirs) > 0     
    
    def _check_nas_api_connection(self):
        """NAS API ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(f"{self.nas_api_url}/health", timeout=5)
            if response.status_code == 200:
                self.logger.info(f"âœ… NAS API ì„œë²„ ì—°ê²° í™•ì¸: {self.nas_api_url}")
            else:
                self.logger.warning(f"âš ï¸ NAS API ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"âš ï¸ NAS API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise ConnectionError(f"NAS API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def _create_metadata(
        self,
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        total_rows: int,
        data_type: str,
        has_images: bool = False,
        has_files: bool = False,
        dataset_description: str = "",
        original_source: str = "",
        meta: Optional[Dict] = None,
    ) -> Dict:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata = {
            'provider': provider,
            'dataset': dataset,
            'task': task,
            'variant': variant,
            'data_type': data_type,
            'dataset_description': dataset_description,
            'original_source': original_source,
            'has_images': has_images,
            'has_files': has_files,
            'total_rows': total_rows,
            'uploaded_by': os.getenv('USER', 'unknown'),
            'uploaded_at': datetime.now().isoformat(),
            'file_id': str(uuid.uuid4())[:8],
            
        }
        if meta:
            metadata.update(meta)
        self.logger.debug(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {metadata}")
        return metadata

    def _cleanup_existing_pending(
        self, 
        provider: str, 
        dataset: str, 
        task: str, 
        variant: str = None, 
        is_raw: bool = True,
    ):
        """ê°™ì€ provider/dataset/task ì¡°í•©ì˜ ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬"""
        pending_path = self.staging_path / "pending"
        
        if not pending_path.exists():
            return
        
        existing_dirs = []
        
        for pending_dir in pending_path.iterdir():
            if not pending_dir.is_dir():
                continue
                
            # ë©”íƒ€ë°ì´í„°ë¡œ ì •í™•íˆ í™•ì¸
            try:
                metadata_file = pending_dir / "upload_metadata.json"
                if metadata_file.exists():
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    basic_match = (
                        metadata.get('provider') == provider and 
                        metadata.get('dataset') == dataset and 
                        metadata.get('task') == task
                    )
                    
                    should_cleanup = False
                    if is_raw:
                        should_cleanup = basic_match
                    else:
                        should_cleanup = basic_match and metadata.get('variant') == variant
                        
                    if should_cleanup:
                        existing_dirs.append(pending_dir)
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {pending_dir} - {e}")
                continue
        return existing_dirs
    
    def _load_data(self, data_file) -> tuple[Dataset, dict]:
        
        if isinstance(data_file, pd.DataFrame):
            self.logger.info(f"ğŸ“Š pandas DataFrame ë¡œë“œ ì¤‘: {len(data_file)} í–‰")
            try:
                dataset_obj = Dataset.from_pandas(data_file)
                self.logger.info(f"âœ… pandas DataFrame ë¡œë“œ ì™„ë£Œ: {len(data_file)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ pandas DataFrame ë³€í™˜ ì‹¤íŒ¨: {e}")
                
        # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        elif isinstance(data_file, (str, Path)):
            data_path = Path(data_file).resolve()
            if not data_path.exists():
                raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
            
            self.logger.info(f"ğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {data_path}")   
            
            if data_path.is_dir():
                try:
                    dataset_obj = load_from_disk(str(data_path))
                    self.logger.info(f"âœ… datasets í´ë” ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)} í–‰")
                except Exception as e:
                    raise ValueError(f"âŒ datasets í´ë” ë¡œë“œ ì‹¤íŒ¨: {e}")   
            elif data_path.suffix == '.parquet':
                try:
                    df = pd.read_parquet(data_path)
                    dataset_obj = Dataset.from_pandas(df)
                    self.logger.info(f"âœ… Parquet íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
                except Exception as e:
                    raise ValueError(f"âŒ Parquet íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {data_path.suffix}")
                
            self.logger.info(f"âœ… ë°ì´í„° íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {data_file}")
            
        else:
            raise TypeError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(data_file)}. "
                        f"íŒŒì¼ ê²½ë¡œ(str/Path) ë˜ëŠ” pandas.DataFrameì„ ì‚¬ìš©í•˜ì„¸ìš”.")
                
        column_names = dataset_obj.column_names
        self.logger.info(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {column_names}")
                
        metadata_columns_to_remove = [
            'provider', 'dataset', 'task', 'variant', 
            'data_type', 'uploaded_by', 'uploaded_at', 'file_id'
        ]
        
        columns_to_remove = [col for col in metadata_columns_to_remove 
                            if col in dataset_obj.column_names]
        
        if columns_to_remove:
            dataset_obj = dataset_obj.remove_columns(columns_to_remove)
            self.logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œê±°: {columns_to_remove}")
            
       # í†µí•©ëœ ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ì²˜ë¦¬ (JSON dumps + ì´ë¯¸ì§€)
        dataset_obj = self._process_cast_columns(dataset_obj)
        file_info = self._detect_file_columns_and_type(dataset_obj)
        if file_info['process_assets']:
            dataset_obj = self._normalize_column_names(dataset_obj, file_info)

            self.logger.info(f"ğŸ“„ íŒŒì¼ ë¶„ì„ ê²°ê³¼: variant={file_info['type']}, "
                           f"ì´ë¯¸ì§€ì»¬ëŸ¼={file_info['image_columns']}, "
                           f"íŒŒì¼ì»¬ëŸ¼={file_info['file_columns']}, "
                           f"í™•ì¥ì={file_info['extensions']}")
        else:
            self.logger.debug("ğŸ“„ Assets ì»¬ëŸ¼ ì²˜ë¦¬ ìƒëµ")
        
        return dataset_obj, file_info
    
    def _detect_file_columns_and_type(self, dataset_obj: Dataset) -> Dict:
        """íŒŒì¼ ì»¬ëŸ¼ë“¤ì„ ì°¾ê³  í™•ì¥ì ê¸°ë°˜ìœ¼ë¡œ type ê²°ì •"""
        result = {
            'image_columns': [],
            'file_columns': [],
            'extensions': set(),
            'type': 'text'
        }
        for key in dataset_obj.column_names:
            sample_value = dataset_obj[0][key]
            
            # PIL Imageë‚˜ bytes ë°ì´í„°ì¸ ê²½ìš°
            if key in self.image_data_candidates:
                if hasattr(sample_value, 'save') or isinstance(sample_value, bytes):
                    result['image_columns'].append(key)
                    continue
            
            # ê²½ë¡œ ê¸°ë°˜ íŒŒì¼ì¸ ê²½ìš°
            if key in self.file_path_candidates:
                if isinstance(sample_value, str) and Path(sample_value).exists():
                    ext = Path(sample_value).suffix.lower()
                    result['extensions'].add(ext)
                    result['file_columns'].append(key)
        if len(result['image_columns']) > 1:
            raise ValueError(f"âŒ ì´ë¯¸ì§€ ì»¬ëŸ¼ì´ 2ê°œ ì´ìƒì…ë‹ˆë‹¤: {result['image_columns']}. "
                             f"í•˜ë‚˜ì˜ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        if len(result['file_columns']) > 1:
            raise ValueError(f"âŒ íŒŒì¼ ì»¬ëŸ¼ì´ 2ê°œ ì´ìƒì…ë‹ˆë‹¤: {result['file_columns']}. "
                             f"í•˜ë‚˜ì˜ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        result['image_columns'] = result['image_columns'][:1]
        result['file_columns'] = result['file_columns'][:1]
        result['has_image_data'] = bool(result['image_columns'])
        result['has_file_paths'] = bool(result['file_columns'])
        
        has_image_data = result['has_image_data']
        has_file_paths = result['has_file_paths']
        extensions = result['extensions']
        
        result['process_assets'] = has_image_data or has_file_paths
        
        if has_image_data and has_file_paths:
            result['type'] = "mixed"
        elif has_image_data:
            result['type'] = "image"
        elif has_file_paths:
            # í™•ì¥ì ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì  ë¶„ë¥˜
            if any(ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'] for ext in extensions):
                result['type'] = "image"
            else:
                result['type'] = "files"
                
        
        return result

    def _normalize_column_names(self, dataset_obj: Dataset, file_info: Dict) -> Dataset:
        """ì»¬ëŸ¼ëª…ì„ í‘œì¤€í™” (image_columns â†’ image, file_columns â†’ file_path)"""
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ í‘œì¤€í™”
        if len(file_info['image_columns']):
            image_col = file_info['image_columns'][0]
            self.logger.info(f"ğŸ”„ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‘œì¤€í™”: {image_col} â†’ {self.image_data_key}")
            
            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ì„ í‘œì¤€ ì»¬ëŸ¼ìœ¼ë¡œ ì‚¬ìš©
            
            if image_col != self.image_data_key:
                # ì»¬ëŸ¼ëª… ë³€ê²½
                dataset_obj = dataset_obj.rename_column(image_col, self.image_data_key)
        
        # íŒŒì¼ ì»¬ëŸ¼ í‘œì¤€í™”
        if len(file_info['file_columns']):
            file_col = file_info['file_columns'][0]
            self.logger.info(f"ğŸ”„ íŒŒì¼ ì»¬ëŸ¼ í‘œì¤€í™”: {file_col} â†’ {self.file_path_key}")
            
            
            # ì»¬ëŸ¼ëª… ë³€ê²½ (í•„ìš”í•œ ê²½ìš°)
            if file_col != self.file_path_key:
                dataset_obj = dataset_obj.rename_column(file_col, self.file_path_key)
        return dataset_obj
    
    def _process_cast_columns(self, dataset_obj: Dataset):
        
        self.logger.info("ğŸ” JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ê²€ì‚¬ ì‹œì‘")
        
        json_cast_columns = []
        
        for key in dataset_obj.column_names:
            sample_value = dataset_obj[0][key]
            
            if isinstance(sample_value, (dict, list)):
                json_cast_columns.append(key)
                self.logger.info(f"ğŸ“ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ë°œê²¬: '{key}' (íƒ€ì…: {type(sample_value).__name__})")
        
        # JSON dumps ì²˜ë¦¬
        if json_cast_columns:
            dataset_obj = self._apply_json_transform(dataset_obj, json_cast_columns)
        else:
            self.logger.info("ğŸ“„ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ì—†ìŒ")
        
        return dataset_obj
    
    def _apply_json_transform(self, dataset_obj: Dataset, json_cast_columns: list) -> Dataset:
        """JSON ë³€í™˜ ì ìš©"""
        self.logger.info(f"ğŸ”„ {len(json_cast_columns)}ê°œ ì»¬ëŸ¼ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘: {json_cast_columns}")
        
        def json_transform(x):
            if isinstance(x, (dict, list)):
                return json.dumps(x, ensure_ascii=False)
            return x
        
        try:
            dataset_obj = dataset_obj.map(
                lambda x: {col: json_transform(x[col]) for col in json_cast_columns},
                num_proc=self.num_proc,
                desc="JSON ë³€í™˜ ì¤‘",
            )
            self.logger.info(f"âœ… JSON ë³€í™˜ ì™„ë£Œ: {json_cast_columns}")
            return dataset_obj
        except Exception as e:
            self.logger.error(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise ValueError(f"âŒ JSON ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _save_to_staging(self, dataset_obj: Dataset, metadata: dict, has_file: bool = False) -> str:
        """ë°ì´í„°ì…‹ì„ staging í´ë”ì— ì €ì¥í•˜ê³  ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±"""
        """ë°ì´í„°ë¥¼ staging í´ë”ì— ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        file_id = metadata['file_id']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        user = metadata['uploaded_by']
        
        staging_dirname = f"{dataset_name}_{task}_{variant}_{file_id}_{timestamp}_{user}"
        staging_dir= self.staging_path / "pending" / staging_dirname
        try:
            if has_file:
                staging_assets_dir = staging_dir / "assets"
                dataset_obj  = self._copy_file_path_to_staging(
                    dataset_obj, staging_assets_dir
                )
                
            if metadata.get('data_type') == 'task':
                dataset_obj = self._add_metadata_columns(dataset_obj, metadata)
                
            dataset_obj.save_to_disk(str(staging_dir))
            
            metadata_file = staging_dir / "upload_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=4)
                
            self.logger.info(f"ğŸ“¦ datasets ì €ì¥ ì™„ë£Œ: {staging_dir}")
            return str(staging_dir)
        except Exception as e:
            if staging_dir.exists():
                shutil.rmtree(staging_dir)
            raise 
    
    def _copy_file_path_to_staging(self, dataset_obj: Dataset, staging_assets_dir: Path):
        """íŒŒì¼ ê²½ë¡œë¥¼ stagingìœ¼ë¡œ ë³µì‚¬"""
        sample_value = dataset_obj[0][self.file_path_key]
        
        if isinstance(sample_value, str) and Path(sample_value).exists():
            sample_path = Path(sample_value).resolve()
            staging_device = os.stat(staging_assets_dir.parent).st_dev
            source_device = os.stat(sample_path.parent).st_dev
            same_device = (source_device == staging_device)
            
            copy_method = "OS ë ˆë²¨ cp" if same_device else "Python copy2"
            self.logger.debug(f"ğŸ“¤ íŒŒì¼ ë³µì‚¬ ëª¨ë“œ: {copy_method} (device: {source_device}â†’{staging_device})")
            def copy_file(example, idx):
                original_path = Path(example[self.file_path_key]).resolve()
                if original_path.exists():
                    ext = original_path.suffix or ""
                    prefix = "file"
                    
                    folder_num = idx // 1000
                    folder_name = f"batch_{folder_num:04d}"
                    new_filename = f"{prefix}_{idx:06d}{ext}"
                    target_dir = staging_assets_dir / folder_name
                    target_path = target_dir / new_filename
                    target_path.parent.mkdir(mode=0o775,parents=True, exist_ok=True)
                    
                    if same_device:
                        result = subprocess.run(
                            ["cp", str(original_path), str(target_path)], 
                            capture_output=True, text=True
                        )
                        if result.returncode != 0:
                            raise RuntimeError(f"cp ëª…ë ¹ ì‹¤íŒ¨: {result.stderr}")
                    else:
                        shutil.copy2(original_path, target_path)
                    relative_path = target_path.relative_to(self.staging_pending_path)
                    example[self.file_path_key] = str(relative_path)
                    
                return example
            
            dataset_obj = dataset_obj.map(
                copy_file, 
                with_indices=True,
                num_proc=self.num_proc,
                desc="íŒŒì¼ ê²½ë¡œ ë³µì‚¬ ì¤‘",)
            return dataset_obj
        else:
            self.logger.warning(f"âš ï¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ '{self.file_path_key}'ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {sample_value}")
            raise ValueError(f"íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ '{self.file_path_key}'ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    def _check_path_and_setup_logging(self, log_level: str):
        
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
        }
        
        missing_paths = []
        for path_name, path_obj in required_paths.items():
            if not path_obj.exists():
                missing_paths.append(f"  - {path_name}: {path_obj}")
        
        if missing_paths:
            missing_list = '\n'.join(missing_paths)
            raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
        setup_logging(log_level=log_level, base_path=str(self.base_path))
        self.logger = logging.getLogger(__name__)
        self.logger.debug("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")
        
    def _add_metadata_columns(self, dataset_obj: Dataset, metadata: Dict):
        """Task ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€"""
        self.logger.info("ğŸ“ Task ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€ ì¤‘")
        
        required_fields = self.schema_manager.get_required_fields(metadata['task'])
        self.logger.debug(f"í•„ìˆ˜ í•„ë“œ: {required_fields}")
        self.logger.debug(f"í˜„ì¬ columns: {dataset_obj.column_names}")
        
        # ë°ì´í„°ì…‹ ê¸¸ì´
        num_rows = len(dataset_obj)
        # í•„ìˆ˜ í•„ë“œë“¤ë§Œ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
        added_columns = []
        
        for field in required_fields:
            value = metadata.get(field)
            if value is None:
                self.logger.warning(f"âš ï¸ í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ë©”íƒ€ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            column_data = [value] * num_rows
            dataset_obj = dataset_obj.add_column(field, column_data)
            added_columns.append(f"{field}={value}")
            self.logger.debug(f"ğŸ“ ì»¬ëŸ¼ ì¶”ê°€: {field} = {value}")
        if added_columns:
            self.logger.info(f"âœ… í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ: {', '.join(added_columns)}")
        else:
            self.logger.info("ğŸ“ ì¶”ê°€í•  í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì—†ìŒ")
            
        return dataset_obj
    
            
if __name__ == "__main__":
    from utils.config import Config
    config = Config()
    manager = DatalakeClient(
        log_level="DEBUG",
    )
    
    manager.show_nas_dashboard()
    manager.show_schema_info()
    
    manager.add_provider("example_provider")
    manager.remove_provider("example_provider")
    manager.add_provider("example_provider")
    manager.add_task("example_task", required_fields=["field1", "field2"], allowed_values={"field1": ["value1", "value2"]})
    staging_dir, job_id = manager.upload_raw_data(
        data_file="/home/kai/workspace/DeepDocs_Project/datalake/managers/sample_data_1",
        provider="example_provider",
        dataset="example_dataset",
        dataset_description="This is a sample dataset for testing.",
        original_source="https://example.com/original_source"
    )
    
    if job_id:
        try:
            job_status = manager.wait_for_job_completion(job_id, timeout=600)
            print(f"ì‘ì—… ì™„ë£Œ: {job_status}")
        except Exception as e:
            print(f"ì‘ì—… ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    
    
    
