import logging
import os
import uuid
import json
import shutil
import pandas as pd
import requests 
import time 
import subprocess
import psutil
from pathlib import Path
from datetime import datetime
from datasets import Dataset, load_from_disk
from datasets import Image as DatasetImage
from typing import Dict, Optional, List, Union
from PIL import Image

from core.schema import SchemaManager
from core.collections import CollectionManager
from utils.logging import setup_logging
from clients.duckdb_client import DuckDBClient


class DatalakeClient:
    def __init__(
        self, 
        user_id: str = None, # ì‚¬ìš©ì ID (í•„ìˆ˜)
        base_path: str = "/mnt/AI_NAS/datalake/",
        server_url: str = "http://192.168.20.62:8091",
        log_level: str = "INFO",
        num_proc: int = 8, # ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        table_name: str = "catalog",
        create_dirs: bool = False, # ì´ˆê¸° ë””ë ‰í† ë¦¬ ìƒì„± ì—¬ë¶€
    ):
        if not user_id:
            raise ValueError("user_idëŠ” í•„ìˆ˜ ì…ë‹ˆë‹¤. ì˜ˆ: DatalakeClient(user_id='user_123')")
        
        self.user_id = user_id
        self.server_url = server_url.rstrip('/')
        
        self.base_path = Path(base_path)
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        self.catalog_path = self.base_path / "catalog"
        self.assets_path  = self.base_path / "assets"
        self.collections_path = self.base_path / "collections"
        self.config_path = self.base_path / "config" / "schema.yaml"
        self.duckdb_path = self.base_path / "users" / f"{self.user_id}.duckdb"
        
        self.num_proc = num_proc
        self.table_name = table_name
        self.image_data_candidates = ['image', 'image_bytes']
        self.image_data_key = 'image'  # ê¸°ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‚¤
        self.file_path_candidates = ['image_path', 'file', 'file_path']
        self.file_path_key = 'file_path'  # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ í‚¤
        
    
        
        self._initialize(log_level, create_dirs=create_dirs)
        self._check_server_connection()
               
        self.schema_manager = SchemaManager(
            config_path=self.config_path,
            create_default=True
        )
        self.collection_manager = CollectionManager(
            collections_path=self.collections_path,
        )
        
    def _initialize(self, log_level: str, create_dirs: bool = False):
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
            'collections': self.collections_path,
        }
        if create_dirs:
            for path_name, path_obj in required_paths.items():
                if not path_obj.exists():
                    path_obj.mkdir(mode=0o777, parents=True, exist_ok=True)
                    self.logger.debug(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {path_name}")
        else:
            missing_paths = []
            for path_name, path_obj in required_paths.items():
                if not path_obj.exists():
                    missing_paths.append(f"  - {path_name}: {path_obj}")
            
            if missing_paths:
                missing_list = '\n'.join(missing_paths)
                raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
            
        setup_logging(
            user_id=self.user_id,
            log_level=log_level, 
            base_path=str(self.base_path)
        )
        self.logger = logging.getLogger(__name__)
        self.logger.debug("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")

    def _check_server_connection(self):
        """ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(f"{self.server_url}/health", timeout=5)
            if response.status_code == 200:
                self.logger.debug(f"âœ… ì„œë²„ ì—°ê²° ì„±ê³µ: {self.server_url}")
            else:
                self.logger.warning(f"âš ï¸ ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"âš ï¸ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise ConnectionError(f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        
    def upload_raw(
        self,
        data_file: Union[str, Path, pd.DataFrame, Dataset],
        provider: str,
        dataset: str,
        dataset_description: str = "", # ë°ì´í„°ì…‹ ì„¤ëª…
        original_source: str = "", # ì›ë³¸ ì†ŒìŠ¤ URL 
        overwrite: bool = False, # ê¸°ì¡´ pending ë°ì´í„° ì œê±° ì—¬ë¶€
    ) -> bool:
        task = "raw"

        self.logger.info(f"ğŸ“¥ Raw data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}")

        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")

        if not self._handle_existing_data(provider, dataset, task, overwrite=overwrite, is_raw=True):
            self.logger.warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤: {provider}/{dataset}/{task}")
            self.logger.info("ğŸ’¡ overwrite=Trueë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë®ì–´ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            return False
        
        dataset_obj, file_info = self._load_data(data_file)

        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=file_info['type'],
            total_rows=len(dataset_obj),
            data_type="raw",
            has_images=file_info['has_image_data'],
            has_files= file_info['has_file_paths'],
            dataset_description=dataset_description,
            original_source=original_source,
        )
        
        staging_dir = self._save_to_staging(dataset_obj, metadata)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        return True
    
    def upload_task(
        self,
        data_file: Union[str, Path, pd.DataFrame, Dataset],
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        dataset_description: str = "",
        overwrite: bool = False,
        meta: Optional[Dict] = None,
    ) -> bool:
        self.logger.info(f"ğŸ“¥ Task data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}/{task}/{variant}")
        
        if not self._check_raw_data_exists(provider, dataset):
            self.logger.warning(f"âš ï¸ Raw ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {provider}/{dataset}")
            self.logger.info("ğŸ’¡ upload_raw()ë¡œ ì›ë³¸ ë°ì´í„°ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        is_valid, error_msg = self.schema_manager.validate_task_metadata(task, meta)
        if not is_valid:
            raise ValueError(f"âŒ Task ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error_msg}")

        if not self._handle_existing_data(provider, dataset, task, variant, overwrite=overwrite, is_raw=False):
            self.logger.warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤: {provider}/{dataset}/{task}/{variant}")
            self.logger.info("ğŸ’¡ overwrite=Trueë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë®ì–´ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            return False
        
        dataset_obj, file_info = self._load_data(data_file)

        columns_to_remove = [key for key in meta.keys()
                            if key in dataset_obj.column_names]
        
        if columns_to_remove:
            dataset_obj = dataset_obj.remove_columns(columns_to_remove)
            self.logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œê±°: {columns_to_remove}")
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=variant,
            dataset_description=dataset_description,
            has_images=file_info['has_image_data'],
            has_files=file_info['has_file_paths'],
            total_rows=len(dataset_obj),
            data_type='task',
            meta=meta,
        )
        
        # Stagingì— ì €ì¥
        staging_dir = self._save_to_staging(dataset_obj, metadata)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        return True
    
    def get_server_status(self) -> Optional[Dict]:
        """ì„œë²„ ìƒíƒœ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.server_url}/status", timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def list_server_jobs(self) -> Optional[List[Dict]]:
        """ëª¨ë“  ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.server_url}/jobs", timeout=10)
            if response.status_code == 200:
                return response.json().get('jobs', [])
            else:
                self.logger.error(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def show_server_dashboard(self):
        """ì„œë²„ ìƒíƒœ ëŒ€ì‹œë³´ë“œ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š Data Processing Server Dashboard")
        print("="*60)

        # ìƒíƒœ ì¡°íšŒ
        status = self.get_server_status()
        if status:
            print(f"ğŸ“¦ Pending: {status['pending']}ê°œ")
            print(f"ğŸ”„ Processing: {status['processing']}ê°œ")
            print(f"âŒ Failed: {status['failed']}ê°œ")
            print(f"ğŸ–¥ï¸ Server Status: {status['server_status']}")
            print(f"â° Last Updated: {status['last_updated']}")
        else:
            print("âŒ ì„œë²„ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨")
        
        # ì‘ì—… ëª©ë¡
        jobs = self.list_server_jobs()
        if jobs:
            print(f"\nğŸ“‹ Recent Jobs ({len(jobs)}ê°œ):")
            for job in jobs[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(job['status'], "â“")
                print(f"  {status_emoji} {job['job_id']} - {job['status']} ({job['started_at']})")

        print("="*60 + "\n")
        
    def trigger_processing(self) -> Optional[str]:
        """ì„œë²„ ì²˜ë¦¬ ìš”ì²­"""
        self.logger.info("ğŸ”„ ì„œë²„ ì²˜ë¦¬ ìš”ì²­ ì¤‘...")
        start_time = time.time()
        try:
            response = requests.post(
                f"{self.server_url}/process", 
                timeout=30,
                headers={'Content-Type': 'application/json'}
            )
            elapsed = time.time() - start_time
            self.logger.debug(f"â±ï¸ ì„œë²„ ì²˜ë¦¬ ìš”ì²­ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            if response.status_code == 200:
                result = response.json()
                job_id = result.get('job_id')
                status = result.get('status')
                message = result.get('message', '')
                
                if status == 'already_running':
                    self.logger.info("ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤")
                    return job_id
                elif status == 'started':
                    self.logger.info(f"âœ… ì²˜ë¦¬ ì‘ì—… ì‹œì‘ë¨: {job_id}")
                    return job_id
                else:
                    self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ: {status}, ë©”ì‹œì§€: {message}")
                    return job_id
            else:                    
                self.logger.error(f"âŒ ì²˜ë¦¬ ì‹œì‘ ì‹¤íŒ¨: {response.status_code}")
                try:
                    error_detail = response.json().get('detail', response.text)
                    self.logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {error_detail}")
                except:
                    self.logger.error(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
                return None
            
        except requests.exceptions.Timeout:
            elapsed = time.time() - start_time
            self.logger.error(f"âŒ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ({elapsed:.2f}ì´ˆ)")
            return None
        except requests.exceptions.ConnectionError:
            self.logger.error(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {self.server_url}")
            return None
        except requests.exceptions.RequestException as e:
            elapsed = time.time() - start_time
            self.logger.error(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e} ({elapsed:.2f}ì´ˆ)")
            return None
    
    def get_job_status(self, job_id: str) -> Optional[dict]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.server_url}/jobs/{job_id}", timeout=10)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 404:
                self.logger.warning(f"âš ï¸ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {job_id}")
                return None
            else:
                self.logger.error(f"âŒ ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def wait_for_job_completion(self, job_id: str, polling_interval: int = 10, timeout: int = 3600) -> dict:
        """ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (í´ë§)"""
        self.logger.info(f"â³ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì¤‘: {job_id}")
        
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            job_status = self.get_job_status(job_id)
            if not job_status:
                raise RuntimeError(f"ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {job_id}")
            
            status = job_status.get('status')
            
            if status == 'completed':
                result = job_status.get('result', {})
                self.logger.info(f"âœ… ì‘ì—… ì™„ë£Œ: {job_id}")
                self.logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì„±ê³µ={result.get('success', 0)}, ì‹¤íŒ¨={result.get('failed', 0)}")
                return job_status
                
            elif status == 'failed':
                error = job_status.get('error', 'Unknown error')
                self.logger.error(f"âŒ ì‘ì—… ì‹¤íŒ¨: {job_id}, ì˜¤ë¥˜: {error}")
                raise RuntimeError(f"ì‘ì—… ì‹¤íŒ¨: {error}")
                
            elif status == 'running':
                self.logger.debug(f"ğŸ”„ ì‘ì—… ì§„í–‰ ì¤‘: {job_id}")
                time.sleep(polling_interval)
            else:
                self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… ìƒíƒœ: {status}")
                time.sleep(polling_interval)
        
        raise TimeoutError(f"ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {job_id}")    

    def get_db_info(self) -> Dict:
        """DB ì •ë³´ ì¡°íšŒ"""
        self.logger.info("ğŸ“Š DB ì •ë³´ ì¡°íšŒ ì¤‘...")
        
        try:
            if not self.duckdb_path.exists():
                return {
                    'exists': False,
                    'message': 'DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. build_db()ë¡œ ìƒì„±í•˜ì„¸ìš”.'
                }
            
            # DB ê¸°ë³¸ ì •ë³´
            db_size = self.duckdb_path.stat().st_size / 1024 / 1024
            db_mtime = datetime.fromtimestamp(self.duckdb_path.stat().st_mtime)
            
            info = {
                'user_id': self.user_id,
                'exists': True,
                'path': str(self.duckdb_path),
                'size_mb': round(db_size, 1),
                'modified_time': db_mtime.strftime('%Y-%m-%d %H:%M:%S'),
                'is_outdated': self._is_db_outdated(),
            }

            with DuckDBClient(str(self.duckdb_path), read_only=True) as duck_client:
                tables = duck_client.list_tables()
                info['tables'] = tables['name'].tolist()
                
                if self.table_name in info['tables']:
                    count_result = duck_client.execute_query(f"SELECT COUNT(*) as total FROM {self.table_name}")
                    total_rows = count_result['total'].iloc[0]
                    info['total_rows'] = total_rows
                    info['partitions'] = 0
                    info['provider_stats'] = {}
                    info['dataset_stats'] = {}
                    info['task_stats'] = {}
                    info['variant_stats'] = {}
                    
                    # íŒŒí‹°ì…˜ ì •ë³´
                    try:
                        partitions_df = duck_client.retrieve_partitions(self.table_name)
                        info['partitions'] = len(partitions_df)

                        # Providerë³„ í†µê³„
                        if not partitions_df.empty:
                            provider_stats = partitions_df.groupby('provider').size().to_dict()
                            dataset_stats = partitions_df.groupby('dataset').size().to_dict()
                            task_stats = partitions_df.groupby('task').size().to_dict()
                            variant_stats = partitions_df.groupby('variant').size().to_dict()
                            info['provider_stats'] = provider_stats
                            info['dataset_stats'] = dataset_stats
                            info['task_stats'] = task_stats
                            info['variant_stats'] = variant_stats
                    except Exception as e:
                        self.logger.warning(f"íŒŒí‹°ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
            return info

        except Exception as e:
            self.logger.error(f"âŒDB ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'exists': False,
                'error': str(e)
            }
    
    def build_db(self, force_rebuild: bool = False) -> bool:
        """DB êµ¬ì¶• ë˜ëŠ” ì¬êµ¬ì¶•"""
        self.logger.info("ğŸ”¨ DB êµ¬ì¶• ì‹œì‘...")
        
        try:
            if not self.catalog_path.exists():
                raise FileNotFoundError(f"Catalog ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.catalog_path}")

            # ê¸°ì¡´ DB íŒŒì¼ ì²˜ë¦¬
            if self.duckdb_path.exists():
                if force_rebuild:
                    self.logger.info("ğŸ—‘ï¸ ê¸°ì¡´ DB íŒŒì¼ ì‚­ì œ ì¤‘...")
                    self._cleanup_db_files()
                else:
                    self.logger.info("âš ï¸ ê¸°ì¡´ DB íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. force_rebuild=Trueë¡œ ì¬êµ¬ì¶•í•˜ì„¸ìš”.")
                    return False

            # ë””ë ‰í† ë¦¬ ìƒì„±
            self.duckdb_path.parent.mkdir(mode=0o777, parents=True, exist_ok=True)

            # Parquet íŒŒì¼ë“¤ í™•ì¸
            parquet_files = list(self.catalog_path.rglob("*.parquet"))
            if not parquet_files:
                raise FileNotFoundError("Parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            self.logger.info(f"ğŸ“‚ ë°œê²¬ëœ Parquet íŒŒì¼: {len(parquet_files)}ê°œ")

            # ìƒˆ DB ìƒì„±
            with DuckDBClient(str(self.duckdb_path), read_only=False) as duck_client:
                parquet_pattern = str(self.catalog_path / "**" / "*.parquet")
                
                self.logger.info("ğŸ“Š í…Œì´ë¸” ìƒì„± ì¤‘...")
                duck_client.create_table_from_parquet(
                    self.table_name,
                    parquet_pattern,
                    hive_partitioning=True,
                    union_by_name=True
                )

                # ê²°ê³¼ ê²€ì¦
                count_result = duck_client.execute_query(f"SELECT COUNT(*) as total FROM {self.table_name}")
                total_rows = count_result['total'].iloc[0]
                
                self.logger.info(f"âœ… DB êµ¬ì¶• ì™„ë£Œ!")
                self.logger.info(f"ğŸ“Š ì´ {total_rows:,}ê°œ í–‰")
                self.logger.info(f"ğŸ’¾ DB íŒŒì¼: {self.duckdb_path}")
                self.logger.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {self.duckdb_path.stat().st_size / 1024 / 1024:.1f}MB")

            # ê¶Œí•œ ì„¤ì •
            self.duckdb_path.chmod(0o777)
            return True

        except Exception as e:
            self.logger.error(f"âŒ DB êµ¬ì¶• ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
            if self.duckdb_path.exists():
                try:
                    self.duckdb_path.unlink()
                except:
                    pass
            return False
    
    def get_partitions(self) -> pd.DataFrame:
        """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒí‹°ì…˜ ëª©ë¡ ì¡°íšŒ"""
        self.logger.debug("ğŸ” íŒŒí‹°ì…˜ ì¡°íšŒ ì¤‘...")
        
        try:
            if not self.duckdb_path.exists():
                raise FileNotFoundError("DBê°€ ì—†ìŠµë‹ˆë‹¤. build_db()ë¡œ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
                
            with DuckDBClient(str(self.duckdb_path), read_only=True) as duck_client:
                self._validate_db(duck_client)
                partitions_df = duck_client.retrieve_partitions(self.table_name)
                
                self.logger.debug(f"ğŸ“Š ì´ {len(partitions_df)}ê°œ íŒŒí‹°ì…˜ ì¡°íšŒë¨")
                return partitions_df

        except Exception as e:
            self.logger.error(f"âŒ íŒŒí‹°ì…˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise

    def search(
        self,
        providers: Optional[List[str]] = None,
        datasets: Optional[List[str]] = None,
        tasks: Optional[List[str]] = None,
        variants: Optional[List[str]] = None,
        text_search: Optional[Dict] = None,
        limit: Optional[int] = None
    ) -> pd.DataFrame:
        """
        DBì—ì„œ ë°ì´í„° ê²€ìƒ‰
        
        Args:
            providers: Provider ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            datasets: Dataset ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            tasks: Task ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            variants: Variant ëª©ë¡ (Noneì´ë©´ ì „ì²´)
            text_search: í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì„¤ì • {"column": str, "text": str, "json_path": str}
            limit: ê²°ê³¼ ì œí•œ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ DataFrame
        """
        self.logger.info("ğŸ” ê²€ìƒ‰ ì‹œì‘")
        
        try:
            if not self.duckdb_path.exists():
                raise FileNotFoundError("DBê°€ ì—†ìŠµë‹ˆë‹¤. build_db()ë¡œ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
            
            with DuckDBClient(str(self.duckdb_path), read_only=True) as duck_client:
                self._validate_db(duck_client)
                
                if text_search:
                    # í…ìŠ¤íŠ¸ ê²€ìƒ‰
                    results = self._perform_text_search(duck_client, text_search, limit)
                else:
                    # íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰
                    results = self._perform_partition_search(
                        duck_client, providers, datasets, tasks, variants, limit
                    )

                self.logger.info(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(results):,}ê°œ í•­ëª©")
                return results

        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise

    def to_pandas(
        self, 
        search_results: pd.DataFrame, 
        absolute_paths: bool = True,
    ) -> pd.DataFrame:
        self.logger.info("ğŸ“Š Pandas DataFrame ë³€í™˜ ì‹œì‘...")
        
        df_copy = search_results.copy()
        
        if absolute_paths and 'path' in df_copy.columns.tolist():
            df_copy['path'] = df_copy['path'].apply(
                lambda x: (self.assets_path / x).as_posix() if isinstance(x, str) and x else x
            )
            self.logger.debug("ğŸ“ ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜")
            
        self.logger.info(f"âœ… DataFrame ë³€í™˜ ì™„ë£Œ: {len(df_copy):,}ê°œ í•­ëª©")
        return df_copy

    def to_dataset(
        self,
        search_results: pd.DataFrame,
        absolute_paths: bool = True,
        check_path_exists: bool = True,
        include_images: bool = False,
    ):
        self.logger.info("ğŸ“¥ Dataset ê°ì²´ ìƒì„± ì‹œì‘...")
        
        df_copy = self.to_pandas(search_results, absolute_paths)
        dataset = Dataset.from_pandas(df_copy)
        print(dataset.column_names)
        if check_path_exists and 'path' in dataset.column_names:
            print("?")
            dataset = self._check_file_exist(dataset)
        if include_images:
            dataset = self._add_images_to_dataset(dataset)

        self.logger.info(f"âœ… Dataset ê°ì²´ ìƒì„± ì™„ë£Œ: {len(dataset):,}ê°œ í•­ëª©") 
        return dataset
    
    def download(
        self,
        search_results: pd.DataFrame,
        output_path: Union[str, Path],
        format: str = "auto",
        absolute_paths: bool = True,
        check_path_exists: bool = True,
        include_images: bool = False,  # dataset ì „ìš©
    ) -> Path:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ DataFrame
            output_path: ì¶œë ¥ ê²½ë¡œ
            format: ì¶œë ¥ í˜•ì‹ ("parquet", "dataset", "auto")
            absolute_paths: ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš© ì—¬ë¶€
            include_images: ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€ (datasetë§Œ í•´ë‹¹)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼/ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        output_path = Path(output_path)
        
        # autoì¸ ê²½ìš° í™•ì¥ìë¡œ í˜•ì‹ ê²°ì •
        if format == "auto":
            if output_path.suffix.lower() == ".parquet":
                format = "parquet"
            else:
                format = "dataset"  # ê¸°ë³¸ê°’
        
        self.logger.info(f"ğŸ’¾ {format.upper()} í˜•ì‹ìœ¼ë¡œ ì €ì¥ ì‹œì‘...")
        
        if format == "parquet":
            return self._save_as_parquet(search_results, output_path, absolute_paths)
        elif format == "dataset":
            return self._save_as_dataset(search_results, output_path, include_images, check_path_exists, absolute_paths)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤: {format}")
    
    def save_collection(
        self,
        search_results,
        name,
        version = None,
        description: str = "",
    ):
        dataset = self.to_dataset(search_results, absolute_paths=True, check_path_exists=True)
        return self.dataset_manager.save_collection(
            dataset=dataset,
            name=name,
            version=version,
            description=description,
            user_id=self.user_id,
        )
        
    def import_collection(
        self,
        data_file: Union[str, Path, pd.DataFrame, Dataset],
        name: str,
        version: Optional[str] = None,
        description: str = "",
    ) -> str:
        
        dataset = self._load_to_dataset(data_file)
        
        return self.collection_manager.save_collection(
            collection=dataset,
            name=name,
            version=version,
            description=description,
            user_id=self.user_id,
        )
            
    def request_asset_validation(
        self,
        search_results: pd.DataFrame,
        sample_percent: Optional[float] = None
    ) -> Optional[str]:
        try:
            required_columns = ['hash', 'path']
            self.logger.debug(f"í•„ìˆ˜ ì»¬ëŸ¼: {required_columns}")
            search_results = search_results[required_columns].dropna(subset=['hash', 'path'])
            search_results = search_results.drop_duplicates(subset=['hash', 'path'])
            if search_results.empty:
                self.logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìœ íš¨ì„± ê²€ì‚¬ ìš”ì²­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return None
            self.logger.debug(f"ìœ íš¨ì„± ê²€ì‚¬ ëŒ€ìƒ ë°ì´í„°: {len(search_results):,}ê°œ")
            search_data = search_results.to_dict('records')
            self.logger.info(f"ğŸ” ìœ íš¨ì„± ê²€ì‚¬ ìš”ì²­: {len(search_data):,}ê°œ í•­ëª©")
            response = requests.post(
                f"{self.server_url}/validate-assets",  
                json={
                    "user_id": self.user_id,
                    "search_data": search_data,
                    "sample_percent": sample_percent
                },
                timeout=120,
            )
            
            if response.status_code == 200:
                result = response.json()
                job_id = result.get('job_id')
                status = result.get('status')
                
                if status == 'already_running':
                    self.logger.info("ğŸ”„ ì´ë¯¸ ìœ íš¨ì„± ê²€ì‚¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
                    return job_id
                elif status == 'started':
                    self.logger.info(f"âœ… íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ ì‹œì‘ë¨: {job_id}")
                    return job_id
                else:
                    self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ: {status}")
                    return job_id
            else:
                self.logger.error(f"âŒ ê²€ì‚¬ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return None

    def check_db_processes(self):
        """DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
        print("\nğŸ” DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸")
        print("="*50)
        
        db_path = Path(self.duckdb_path).resolve()  # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜

        try:
            using_processes = []

            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    # 1. cmdline ê²€ì‚¬ (ê¸°ì¡´ ë°©ì‹)
                    cmdline_match = False
                    if proc.info['cmdline']:
                        cmdline = ' '.join(proc.info['cmdline'])
                        if str(db_path) in cmdline:
                            cmdline_match = True

                    # 2. ì—´ë¦° íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ê²€ì‚¬ (ìƒˆë¡œìš´ ë°©ì‹)
                    file_match = False
                    try:
                        process = psutil.Process(proc.info['pid'])
                        open_files = process.open_files()
                        for f in open_files:
                            file_path = Path(f.path).resolve()
                            # DB íŒŒì¼ì´ë‚˜ ê´€ë ¨ íŒŒì¼ë“¤ í™•ì¸
                            if (file_path == db_path or 
                                file_path.name == db_path.name or
                                str(file_path).endswith('.duckdb') or
                                str(file_path).endswith('.duckdb.wal') or
                                str(file_path).endswith('.duckdb.tmp')):
                                file_match = True
                                break
                    except (psutil.AccessDenied, psutil.NoSuchProcess):
                        # ê¶Œí•œì´ ì—†ê±°ë‚˜ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ë¼ì§„ ê²½ìš°
                        pass

                    # 3. ë©”ëª¨ë¦¬ ë§¤í•‘ ê²€ì‚¬ (ì¶”ê°€)
                    memory_match = False
                    try:
                        process = psutil.Process(proc.info['pid'])
                        memory_maps = process.memory_maps()
                        for m in memory_maps:
                            if str(db_path) in m.path:
                                memory_match = True
                                break
                    except (psutil.AccessDenied, psutil.NoSuchProcess, AttributeError):
                        # ì¼ë¶€ ì‹œìŠ¤í…œì—ì„œëŠ” memory_maps()ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                        pass

                    # í•˜ë‚˜ë¼ë„ ë§¤ì¹˜ë˜ë©´ DB ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤
                    if cmdline_match or file_match or memory_match:
                        match_type = []
                        if cmdline_match: match_type.append("cmdline")
                        if file_match: match_type.append("open_files")
                        if memory_match: match_type.append("memory_map")
                        
                        using_processes.append({
                            'pid': proc.info['pid'],
                            'name': proc.info['name'],
                            'cmdline': cmdline[:100] + '...' if len(cmdline) > 100 else cmdline,
                            'match_type': ', '.join(match_type)
                        })
                        
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue

            # DB íŒŒì¼ ìƒíƒœ ì •ë³´ë„ í¬í•¨
            db_info = {
                'path': str(db_path),
                'exists': db_path.exists()
            }

            if db_path.exists():
                stat = db_path.stat()
                db_info.update({
                    'size': stat.st_size,
                    'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat()
                })

                # WAL íŒŒì¼ í™•ì¸
                wal_file = db_path.with_suffix('.duckdb.wal')
                db_info['has_wal'] = wal_file.exists()

            result = {
                'processes': using_processes,
                'db_info': db_info
            }

            self.logger.info(f"ğŸ“Š DB í”„ë¡œì„¸ìŠ¤ í™•ì¸ ì™„ë£Œ: {len(using_processes)}ê°œ í”„ë¡œì„¸ìŠ¤ ë°œê²¬")
            return result

        except Exception as e:
            self.logger.error(f"âŒ í”„ë¡œì„¸ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
     
    def _check_file_exist(self, dataset):
        """Datasetì˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ ë³‘ë ¬ë¡œ í™•ì¸"""
        def check_exists(example):
            try:
                if example.get('path'):
                    file_path = Path(example['path'])
                    example['exists'] = file_path.exists()
                else:
                    example['exists'] = False
            except Exception as e:
                self.logger.warning(f"íŒŒì¼ ì¡´ì¬ í™•ì¸ ì‹¤íŒ¨: {example.get('path', 'unknown')} - {e}")
                example['exists'] = False
            return example

        self.logger.info("ğŸ“ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì¤‘...")
        dataset_with_exists = dataset.map(
            check_exists,
            desc="íŒŒì¼ ì¡´ì¬ í™•ì¸",
            num_proc=self.num_proc
        )

        # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
        valid_dataset = dataset_with_exists.filter(
            lambda x: x,
            input_columns=['exists'],
            desc="ì¡´ì¬í•˜ëŠ” íŒŒì¼ í•„í„°ë§",
            num_proc=self.num_proc
        )

        # exists ì»¬ëŸ¼ ì œê±°
        valid_dataset = valid_dataset.remove_columns(['exists'])

        total_items = len(dataset)
        valid_items = len(valid_dataset)
        self.logger.info(f"ğŸ“Š íŒŒì¼ ì¡´ì¬ í™•ì¸ ê²°ê³¼: {valid_items:,}/{total_items:,} ì¡´ì¬")

        return valid_dataset
    
    def _save_as_parquet(
        self, 
        search_results: pd.DataFrame, 
        output_path: Union[str, Path],
        absolute_paths: bool = True,
    ) -> Path:
        
        df_copy = self.to_pandas(search_results, absolute_paths)
        
        output_path = Path(output_path).with_suffix('.parquet')
        output_path.parent.mkdir(parents=True, exist_ok=True)
        df_copy.to_parquet(output_path, index=False)
        
        file_size = output_path.stat().st_size / 1024 / 1024
        self.logger.info(f"âœ… Parquet ì €ì¥ ì™„ë£Œ: {output_path}")
        self.logger.info(f"ğŸ“Š {len(df_copy):,}ê°œ í•­ëª©, {file_size:.1f}MB")
        
        return output_path

    def _save_as_dataset(
        self,
        search_results: pd.DataFrame,
        output_path: Union[str, Path], 
        include_images: bool = False,
        check_path_exists: bool = True,
        absolute_paths: bool = True,
    ) -> Path:
    
        dataset = self.to_dataset(search_results, absolute_paths, check_path_exists, include_images)
        
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        dataset.save_to_disk(str(output_path))
        
        total_size = sum(f.stat().st_size for f in output_path.rglob('*') if f.is_file()) / 1024 / 1024
        
        self.logger.info(f"âœ… Dataset ì €ì¥ ì™„ë£Œ: {output_path}")
        self.logger.info(f"ğŸ“Š {len(dataset):,}ê°œ í•­ëª©, {total_size:.1f}MB")
        
        return output_path
    
    def _handle_existing_data(self, provider: str, dataset_name: str, task: str = None, 
                            variant: str = None, overwrite: bool = False, is_raw: bool = True):
        """ê¸°ì¡´ ë°ì´í„° ì²˜ë¦¬ ë¡œì§"""
        existing_dirs = self._cleanup_existing_pending(
            provider, dataset_name, task, variant=variant, is_raw=is_raw
        )
        
        if existing_dirs and not overwrite:
            self.logger.warning(f"âš ï¸ ì´ë¯¸ pending ë°ì´í„°ê°€ ìˆì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {len(existing_dirs)}ê°œ")
            self.logger.info("ğŸ’¡ ë®ì–´ì“°ë ¤ë©´ overwrite=Trueë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
            return False
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        for existing_dir in existing_dirs:
            try:
                shutil.rmtree(existing_dir)
                self.logger.info(f"ğŸ—‘ï¸ ì‚­ì œ ì™„ë£Œ: {existing_dir.name}")
            except Exception as e:
                self.logger.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {existing_dir.name} - {e}")
        
        return True

    def _validate_db(self, duck_client):
        """DB ìœ íš¨ì„± ê²€ì‚¬"""
        tables = duck_client.list_tables()
        if tables.empty or self.table_name not in tables['name'].values:
            raise ValueError(f"âŒ '{self.table_name}' í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. DBê°€ ì˜¬ë°”ë¥´ê²Œ êµ¬ì¶•ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

    def _is_db_outdated(self) -> bool:
        """DBê°€ ìµœì‹  ìƒíƒœì¸ì§€ í™•ì¸"""
        if not self.duckdb_path.exists():
            return True

        db_mtime = self.duckdb_path.stat().st_mtime

        # ê°€ì¥ ìµœê·¼ Parquet íŒŒì¼ í™•ì¸
        latest_parquet_mtime = 0
        for parquet_file in self.catalog_path.rglob("*.parquet"):
            file_mtime = parquet_file.stat().st_mtime
            if file_mtime > latest_parquet_mtime:
                latest_parquet_mtime = file_mtime
        
        return latest_parquet_mtime > db_mtime

    def _cleanup_db_files(self):
        """DB ê´€ë ¨ íŒŒì¼ë“¤ ì •ë¦¬"""
        files_to_remove = [
            self.duckdb_path,
            self.duckdb_path.with_suffix('.duckdb-wal'),
            self.duckdb_path.with_suffix('.duckdb-shm'),
            self.duckdb_path.with_suffix('.duckdb.wal'),
            self.duckdb_path.with_suffix('.duckdb.tmp'),
            self.duckdb_path.with_suffix('.duckdb.lock')
        ]

        for file_path in files_to_remove:
            if file_path.exists():
                try:
                    file_path.unlink()
                    self.logger.debug(f"ğŸ—‘ï¸ ì‚­ì œ: {file_path}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")

    def _perform_partition_search(
        self, 
        duck_client, 
        providers, 
        datasets, 
        tasks, 
        variants, 
        limit
    ):
        """íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰"""
        return duck_client.retrieve_with_existing_cols(
            providers=providers,
            datasets=datasets, 
            tasks=tasks,
            variants=variants,
            table=self.table_name,
            limit=limit
        )

    def _perform_text_search(
        self,
        duck_client,
        text_search,
        limit,
    ):
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰"""
        column = text_search.get("column")
        text = text_search.get("text")
        json_path = text_search.get("json_path")

        if json_path:
            # JSON ê²€ìƒ‰
            sql = duck_client.json_queries.search_text_in_column(
                table=self.table_name,
                column=column,
                search_text=text,
                search_type="json",
                json_loc=json_path,
                engine="duckdb"
            )
        else:
            # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰
            sql = duck_client.json_queries.search_text_in_column(
                table=self.table_name,
                column=column,
                search_text=text,
                search_type="simple",
                engine="duckdb"
            )

        if limit is not None:
            sql += f" LIMIT {limit}"
            
        return duck_client.execute_query(sql)

    def _add_images_to_dataset(self, dataset):
        def load_image(example):
            try:
                if example.get('path'):
                    image_path = Path(example['path'])
                    pil_image = Image.open(image_path)
                    example['image'] = pil_image
                    example['has_valid_image'] = True
                    return example
            except Exception as e:
                self.logger.warning(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {example.get('path', 'unknown')} - {e}")

            example['image'] = None
            example['has_valid_image'] = False
            return example

        self.logger.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...")
        dataset_with_images = dataset.map(
            load_image,
            desc="ì´ë¯¸ì§€ ë¡œë”©",
            num_proc=self.num_proc
        )
        if len(dataset_with_images) == 0:
            self.logger.warning("âš ï¸ ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return dataset_with_images
        
        valid_dataset = dataset_with_images.filter(
            lambda x: x,
            desc="ìœ íš¨ ì´ë¯¸ì§€ í•„í„°ë§",
            input_columns=['has_valid_image'],
            num_proc=self.num_proc
        )

        # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
        valid_dataset = valid_dataset.remove_columns(['has_valid_image'])

        total_items = len(dataset)
        valid_items = len(valid_dataset)
        self.logger.info(f"ğŸ“Š ì´ë¯¸ì§€ ë¡œë”© ê²°ê³¼: {valid_items:,}/{total_items:,} ì„±ê³µ")

        return valid_dataset

    def _check_raw_data_exists(self, provider: str, dataset: str) -> bool:
        try:
            results = self.search(
                providers=[provider],
                datasets=[dataset], 
                tasks=["raw"]
            )
            return not results.empty
        except:
            return False    
    
    def _create_metadata(
        self,
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        total_rows: int,
        data_type: str,
        has_images: bool = False,
        has_files: bool = False,
        dataset_description: str = "",
        original_source: str = "",
        meta: Optional[Dict] = None,
    ) -> Dict:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata = {
            'provider': provider,
            'dataset': dataset,
            'task': task,
            'variant': variant,
            'data_type': data_type,
            'dataset_description': dataset_description,
            'original_source': original_source,
            'has_images': has_images,
            'has_files': has_files,
            'total_rows': total_rows,
            'uploaded_by': self.user_id,
            'uploaded_at': datetime.now().isoformat(),
            'file_id': str(uuid.uuid4())[:8],
            
        }
        if meta:
            metadata.update(meta)
        self.logger.debug(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {metadata}")
        return metadata

    def _cleanup_existing_pending(
        self, 
        provider: str, 
        dataset: str, 
        task: str, 
        variant: str = None, 
        is_raw: bool = True,
    ):
        """ê°™ì€ provider/dataset/task ì¡°í•©ì˜ ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬"""
        pending_path = self.staging_path / "pending"
        
        if not pending_path.exists():
            return
        
        existing_dirs = []
        
        for pending_dir in pending_path.iterdir():
            if not pending_dir.is_dir():
                continue
                
            # ë©”íƒ€ë°ì´í„°ë¡œ ì •í™•íˆ í™•ì¸
            try:
                metadata_file = pending_dir / "upload_metadata.json"
                if metadata_file.exists():
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    basic_match = (
                        metadata.get('provider') == provider and 
                        metadata.get('dataset') == dataset and 
                        metadata.get('task') == task
                    )
                    
                    should_cleanup = False
                    if is_raw:
                        should_cleanup = basic_match
                    else:
                        should_cleanup = basic_match and metadata.get('variant') == variant
                        
                    if should_cleanup:
                        existing_dirs.append(pending_dir)
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {pending_dir} - {e}")
                continue
        return existing_dirs
    
    def _get_file_type(self, data_file) -> str:
        if isinstance(data_file, Dataset):
            return "dataset"
        elif isinstance(data_file, pd.DataFrame):
            return "dataframe"
        elif isinstance(data_file, (str, Path)):
            data_path = Path(data_file).resolve()
            if not data_path.exists():
                raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
            
            if data_path.is_dir():
                return "datasets_folder"
            elif data_path.suffix == '.parquet':
                return "parquet"
            else:
                raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {data_path.suffix}")
        else:
            raise TypeError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(data_file)}. "
                            "Dataset, pandas DataFrame, str ë˜ëŠ” Path ê°ì²´ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
            
    def _load_to_dataset(self, data_file) -> Dataset:
        data_type = self._get_file_type(data_file)
        if data_type == "dataset":
            self.logger.info(f"ğŸ¤— Hugging Face Dataset ë¡œë“œ ì¤‘: {len(data_file)} í–‰")
            try:
                dataset_obj = data_file  # ì´ë¯¸ Dataset ê°ì²´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                self.logger.info(f"âœ… Hugging Face Dataset ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)} í–‰")
                
                # Dataset ì •ë³´ ë¡œê¹…
                if hasattr(dataset_obj, 'features'):
                    features = list(dataset_obj.features.keys())
                    self.logger.info(f"ğŸ“‹ Dataset features: {features}")
                    
            except Exception as e:
                raise ValueError(f"âŒ Hugging Face Dataset ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        elif data_type == "dataframe":
            self.logger.info(f"ğŸ“Š pandas DataFrame ë¡œë“œ ì¤‘: {len(data_file)} í–‰")
            try:
                dataset_obj = Dataset.from_pandas(data_file, preserve_index=False)
                self.logger.info(f"âœ… pandas DataFrame ë¡œë“œ ì™„ë£Œ: {len(data_file)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ pandas DataFrame ë³€í™˜ ì‹¤íŒ¨: {e}")
                
        elif data_type == "datasets_folder":
            data_path = Path(data_file).resolve()
            self.logger.info(f"ğŸ“‚ datasets í´ë” ë¡œë“œ ì¤‘: {data_path}")
            try:
                dataset_obj = load_from_disk(str(data_path))
                self.logger.info(f"âœ… datasets í´ë” ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ datasets í´ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        elif data_type == "parquet_file":
            data_path = Path(data_file).resolve()
            self.logger.info(f"ğŸ“‚ Parquet íŒŒì¼ ë¡œë“œ ì¤‘: {data_path}")
            try:
                df = pd.read_parquet(data_path)
                dataset_obj = Dataset.from_pandas(df)
                self.logger.info(f"âœ… Parquet íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ Parquet íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        else:            
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {data_type}. "
                "Dataset, pandas DataFrame, datasets í´ë” ë˜ëŠ” Parquet íŒŒì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
            
        return dataset_obj
        
    def _load_data(self, data_file) -> tuple[Dataset, dict]:
        
        dataset_obj = self._load_to_dataset(data_file)
        
        self.logger.info(f"âœ… ë°ì´í„° íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {data_file}")
        column_names = dataset_obj.column_names
        self.logger.info(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {column_names}")

        metadata_columns_to_remove = [
            'provider', 'dataset', 'task', 'variant', 
            'data_type', 'uploaded_by', 'uploaded_at', 'file_id'
        ]

        columns_to_remove = [col for col in metadata_columns_to_remove 
                            if col in dataset_obj.column_names]

        if columns_to_remove:
            dataset_obj = dataset_obj.remove_columns(columns_to_remove)
            self.logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œê±°: {columns_to_remove}")
            
       # í†µí•©ëœ ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ì²˜ë¦¬ (JSON dumps + ì´ë¯¸ì§€)
        
        file_info = self._detect_file_columns_and_type(dataset_obj)
        self.logger.debug(f"ğŸ“‚ íŒŒì¼ ì •ë³´: {file_info}")
        if file_info['process_assets']:
            dataset_obj = self._normalize_column_names(dataset_obj, file_info)

            self.logger.info(f"ğŸ“„ íŒŒì¼ ë¶„ì„ ê²°ê³¼: variant={file_info['type']}, "
                           f"ì´ë¯¸ì§€ì»¬ëŸ¼={file_info['image_columns']}, "
                           f"íŒŒì¼ì»¬ëŸ¼={file_info['file_columns']}, "
                           f"í™•ì¥ì={file_info['extensions']}")
        else:
            self.logger.debug("ğŸ“„ Assets ì»¬ëŸ¼ ì²˜ë¦¬ ìƒëµ")
            
        dataset_obj = self._process_cast_columns(dataset_obj)
        return dataset_obj, file_info

    def _detect_file_columns_and_type(
        self,
        dataset_obj: Dataset,
    ) -> Dict:
        """íŒŒì¼ ì»¬ëŸ¼ë“¤ì„ ì°¾ê³  í™•ì¥ì ê¸°ë°˜ìœ¼ë¡œ type ê²°ì •"""
        result = {
            'image_columns': [],
            'file_columns': [],
            'extensions': set(),
            'type': 'text'
        }
        for key in dataset_obj.column_names:
            sample_value = dataset_obj[0][key]
            
            # PIL Imageë‚˜ bytes ë°ì´í„°ì¸ ê²½ìš°
            if key in self.image_data_candidates:
                result['image_columns'].append(key)
            # ê²½ë¡œ ê¸°ë°˜ íŒŒì¼ì¸ ê²½ìš°
            elif key in self.file_path_candidates:
                if isinstance(sample_value, str) and Path(sample_value).exists():
                    ext = Path(sample_value).suffix.lower()
                    result['extensions'].add(ext)
                    result['file_columns'].append(key)
                    
        if len(result['image_columns']) > 1:
            raise ValueError(f"âŒ ì´ë¯¸ì§€ ì»¬ëŸ¼ì´ 2ê°œ ì´ìƒì…ë‹ˆë‹¤: {result['image_columns']}. "
                             f"í•˜ë‚˜ì˜ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        if len(result['file_columns']) > 1:
            raise ValueError(f"âŒ íŒŒì¼ ì»¬ëŸ¼ì´ 2ê°œ ì´ìƒì…ë‹ˆë‹¤: {result['file_columns']}. "
                             f"í•˜ë‚˜ì˜ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        result['image_columns'] = result['image_columns'][:1]
        result['file_columns'] = result['file_columns'][:1]
        result['has_image_data'] = bool(result['image_columns'])
        result['has_file_paths'] = bool(result['file_columns'])
        
        has_image_data = result['has_image_data']
        has_file_paths = result['has_file_paths']
        extensions = result['extensions']
        
        result['process_assets'] = has_image_data or has_file_paths
        
        if has_image_data and has_file_paths:
            result['type'] = "mixed"
        elif has_image_data:
            result['type'] = "image"
        elif has_file_paths:
            # í™•ì¥ì ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì  ë¶„ë¥˜
            if any(ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'] for ext in extensions):
                result['type'] = "image"
            else:
                result['type'] = "files"
                
        
        return result

    def _normalize_column_names(
        self,
        dataset_obj: Dataset,
        file_info: Dict,
    ) -> Dataset:
        """ì»¬ëŸ¼ëª…ì„ í‘œì¤€í™” (image_columns â†’ image, file_columns â†’ file_path)"""
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ í‘œì¤€í™”
        if len(file_info['image_columns']):
            image_col = file_info['image_columns'][0]
            self.logger.info(f"ğŸ”„ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‘œì¤€í™”: {image_col} â†’ {self.image_data_key}")
            
            if image_col != self.image_data_key:
                dataset_obj = dataset_obj.rename_column(image_col, self.image_data_key)
                
            dataset_obj = dataset_obj.cast_column(self.image_data_key, DatasetImage())
        
        # íŒŒì¼ ì»¬ëŸ¼ í‘œì¤€í™”
        if len(file_info['file_columns']):
            file_col = file_info['file_columns'][0]
            self.logger.info(f"ğŸ”„ íŒŒì¼ ì»¬ëŸ¼ í‘œì¤€í™”: {file_col} â†’ {self.file_path_key}")
            
            
            # ì»¬ëŸ¼ëª… ë³€ê²½ (í•„ìš”í•œ ê²½ìš°)
            if file_col != self.file_path_key:
                dataset_obj = dataset_obj.rename_column(file_col, self.file_path_key)
        return dataset_obj

    def _process_cast_columns(self, dataset_obj: Dataset):
        
        self.logger.info("ğŸ” JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ê²€ì‚¬ ì‹œì‘")
        json_cast_columns = []
        
        for key in dataset_obj.column_names:
            sample_value = dataset_obj[0][key]
            
            if isinstance(sample_value, (dict, list)):
                json_cast_columns.append(key)
                self.logger.info(f"ğŸ“ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ë°œê²¬: '{key}' (íƒ€ì…: {type(sample_value).__name__})")
        
        # JSON dumps ì²˜ë¦¬
        if json_cast_columns:
            dataset_obj = self._apply_json_transform(dataset_obj, json_cast_columns)
        else:
            self.logger.info("ğŸ“„ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ì—†ìŒ")
        
        return dataset_obj

    def _apply_json_transform(self, dataset_obj: Dataset, json_cast_columns: list) -> Dataset:
        """JSON ë³€í™˜ ì ìš©"""
        self.logger.info(f"ğŸ”„ {len(json_cast_columns)}ê°œ ì»¬ëŸ¼ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘: {json_cast_columns}")
        
        def json_transform(x):
            if isinstance(x, (dict, list)):
                return json.dumps(x, ensure_ascii=False)
            return x
        
        try:
            dataset_obj = dataset_obj.map(
                lambda x: {col: json_transform(x[col]) for col in json_cast_columns},
                num_proc=self.num_proc,
                desc="JSON ë³€í™˜ ì¤‘",
            )
            self.logger.info(f"âœ… JSON ë³€í™˜ ì™„ë£Œ: {json_cast_columns}")
            return dataset_obj
        except Exception as e:
            self.logger.error(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise ValueError(f"âŒ JSON ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _save_to_staging(
        self,
        dataset_obj: Dataset,
        metadata: dict,
    ) -> str:
        """ë°ì´í„°ì…‹ì„ staging í´ë”ì— ì €ì¥í•˜ê³  ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±"""
        """ë°ì´í„°ë¥¼ staging í´ë”ì— ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        file_id = metadata['file_id']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        user = metadata['uploaded_by']
        
        staging_dirname = f"{dataset_name}_{task}_{variant}_{file_id}_{timestamp}_{user}"
        staging_dir= self.staging_path / "pending" / staging_dirname
        try:
            has_file = metadata.get('has_files', False)
            if has_file:
                staging_assets_dir = staging_dir / "assets"
                staging_assets_dir.mkdir(mode=0o775, parents=True, exist_ok=True)
                dataset_obj  = self._copy_file_path_to_staging(
                    dataset_obj, staging_assets_dir
                )
                
            if metadata.get('data_type') == 'task':
                dataset_obj = self._add_metadata_columns(dataset_obj, metadata)
            dataset_obj.save_to_disk(str(staging_dir))
            
            metadata_file = staging_dir / "upload_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=4)
                
            self.logger.info(f"ğŸ“¦ datasets ì €ì¥ ì™„ë£Œ: {staging_dir}")
            return str(staging_dir)
        except Exception as e:
            if staging_dir.exists():
                shutil.rmtree(staging_dir)
            raise 
    
    def _copy_file_path_to_staging(self, dataset_obj: Dataset, staging_assets_dir: Path):
        """íŒŒì¼ ê²½ë¡œë¥¼ stagingìœ¼ë¡œ ë³µì‚¬"""
        sample_value = dataset_obj[0][self.file_path_key]
        
        if isinstance(sample_value, str) and Path(sample_value).exists():
            sample_path = Path(sample_value).resolve()
            staging_device = os.stat(staging_assets_dir.parent).st_dev
            source_device = os.stat(sample_path.parent).st_dev
            same_device = (source_device == staging_device)
            
            copy_method = "OS ë ˆë²¨ cp" if same_device else "Python copy2"
            self.logger.debug(f"ğŸ“¤ íŒŒì¼ ë³µì‚¬ ëª¨ë“œ: {copy_method} (device: {source_device}â†’{staging_device})")
            def copy_file(example, idx):
                original_path = Path(example[self.file_path_key]).resolve()
                if original_path.exists():
                    ext = original_path.suffix or ""
                    prefix = "file"
                    
                    folder_num = idx // 1000
                    folder_name = f"batch_{folder_num:04d}"
                    new_filename = f"{prefix}_{idx:06d}{ext}"
                    target_dir = staging_assets_dir / folder_name
                    target_path = target_dir / new_filename
                    target_path.parent.mkdir(mode=0o775,parents=True, exist_ok=True)
                    
                    if same_device:
                        result = subprocess.run(
                            ["cp", str(original_path), str(target_path)], 
                            capture_output=True, text=True
                        )
                        if result.returncode != 0:
                            raise RuntimeError(f"cp ëª…ë ¹ ì‹¤íŒ¨: {result.stderr}")
                    else:
                        shutil.copy2(original_path, target_path)
                    relative_path = target_path.relative_to(self.staging_pending_path)
                    example[self.file_path_key] = str(relative_path)
                    
                return example
            
            dataset_obj = dataset_obj.map(
                copy_file, 
                with_indices=True,
                num_proc=self.num_proc,
                desc="íŒŒì¼ ê²½ë¡œ ë³µì‚¬ ì¤‘",)
            return dataset_obj
        else:
            self.logger.warning(f"âš ï¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ '{self.file_path_key}'ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {sample_value}")
            raise ValueError(f"íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ '{self.file_path_key}'ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
    def _add_metadata_columns(self, dataset_obj: Dataset, metadata: Dict):
        """Task ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€"""
        self.logger.info("ğŸ“ Task ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€ ì¤‘")
        
        required_fields = self.schema_manager.get_required_fields(metadata['task'])
        self.logger.debug(f"í•„ìˆ˜ í•„ë“œ: {required_fields}")
        self.logger.debug(f"í˜„ì¬ columns: {dataset_obj.column_names}")
        
        # ë°ì´í„°ì…‹ ê¸¸ì´
        num_rows = len(dataset_obj)
        # í•„ìˆ˜ í•„ë“œë“¤ë§Œ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
        added_columns = []
        
        for field in required_fields:
            value = metadata.get(field)
            if value is None:
                self.logger.warning(f"âš ï¸ í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ë©”íƒ€ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            column_data = [value] * num_rows
            dataset_obj = dataset_obj.add_column(field, column_data)
            added_columns.append(f"{field}={value}")
            self.logger.debug(f"ğŸ“ ì»¬ëŸ¼ ì¶”ê°€: {field} = {value}")
        if added_columns:
            self.logger.info(f"âœ… í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ: {', '.join(added_columns)}")
        else:
            self.logger.info("ğŸ“ ì¶”ê°€í•  í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì—†ìŒ")
            
        return dataset_obj
    
            
if __name__ == "__main__":
    from utils.config import Config
    config = Config()
    manager = DatalakeClient(
        user_id="user",
        log_level="DEBUG",
    )
    
    manager.show_server_dashboard()
    
    manager.schema_manager.show_schema_info()
    manager.schema_manager.add_provider("example_provider")
    manager.schema_manager.remove_provider("example_provider")
    manager.schema_manager.add_provider("example_provider")
    manager.schema_manager.add_task("example_task", required_fields=["field1", "field2"], allowed_values={"field1": ["value1", "value2"]})
    staging_dir, job_id = manager.upload_raw(
        data_file="test.parquet",
        provider="example_provider",
        dataset="example_dataset",
        dataset_description="This is a sample dataset for testing.",
        original_source="https://example.com/original_source"
    )
    
    if job_id:
        try:
            job_status = manager.wait_for_job_completion(job_id, timeout=600)
            print(f"ì‘ì—… ì™„ë£Œ: {job_status}")
        except Exception as e:
            print(f"ì‘ì—… ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
