{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2c7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duck db\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ce8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from client import DuckDBClient\n",
    "from managers.datalake_client import DatalakeClient\n",
    "from datasets import load_from_disk, Dataset\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4dcbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 09:44:50,481 - managers.datalake_client - INFO - âœ… NAS API ì„œë²„ ì—°ê²° í™•ì¸: http://192.168.20.62:8091\n"
     ]
    }
   ],
   "source": [
    "datalake_client = DatalakeClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d742a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 09:47:49,132 - managers.datalake_client - INFO - ğŸ” Catalog ê²€ìƒ‰ ì‹œì‘\n",
      "2025-06-20 09:47:49,658 - managers.datalake_client - INFO - ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: 27,968ê°œ í•­ëª©\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DuckDB ì—°ê²° ì¢…ë£Œ\n"
     ]
    }
   ],
   "source": [
    "search_results = datalake_client.search_catalog(\n",
    "    providers=[\"opensource\"],\n",
    "    variants=[\"base_kie\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe4628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 09:48:16,329 - managers.datalake_client - INFO - ğŸ“¥ Dataset ê°ì²´ ìƒì„± ì‹œì‘...\n",
      "2025-06-20 09:48:16,850 - managers.datalake_client - INFO - ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3d02f9770e4638b1ef8ad95f09d151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ì´ë¯¸ì§€ ë¡œë”© (num_proc=8):   0%|          | 0/27968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f3b4dbde0841d8901892c421625046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ìœ íš¨ ì´ë¯¸ì§€ í•„í„°ë§ (num_proc=8):   0%|          | 0/27968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 09:48:44,366 - managers.datalake_client - INFO - ğŸ“Š ì´ë¯¸ì§€ ë¡œë”© ê²°ê³¼: 27,968/27,968 ì„±ê³µ\n",
      "2025-06-20 09:48:44,368 - managers.datalake_client - INFO - âœ… Dataset ê°ì²´ ìƒì„± ì™„ë£Œ: 27,968ê°œ í•­ëª©\n"
     ]
    }
   ],
   "source": [
    "ds = datalake_client.to_dataset(search_results, include_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18f8d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 09:48:49,071 - managers.datalake_client - INFO - ğŸ“Š Pandas DataFrame ë³€í™˜ ì‹œì‘...\n",
      "2025-06-20 09:48:49,382 - managers.datalake_client - INFO - âœ… DataFrame ë³€í™˜ ì™„ë£Œ: 27,968ê°œ í•­ëª©\n"
     ]
    }
   ],
   "source": [
    "db = datalake_client.to_pandas(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.DataFrame({\n",
    "    \"id\": [1],\n",
    "    \"title\": [1],\n",
    "    \"text\": [1],\n",
    "    \"url\": [1],\n",
    "    \"source\": [1],\n",
    "    \"timestamp\": [1],\n",
    "    \"language\": [1],\n",
    "    \"type\": [1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f703d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalake_client.download_as_parquet(\n",
    "    db,\n",
    "    \"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e086a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = DuckDBClient(\n",
    "    database_path=\"/mnt/AI_NAS/datalake/db/catalog.duckdb\",\n",
    "    read_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parquet = conn.retrieve_partitions(\n",
    "    table=\"catalog\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9894a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taskê°€ rawì´ê³  variantê°€ textê°€ ì•„ë‹Œê²ƒë§Œ \n",
    "all_parquet = all_parquet[\n",
    "    (all_parquet[\"task\"] == \"raw\") & \n",
    "    (all_parquet[\"variant\"] != \"text\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65759fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = \"huggingface\"\n",
    "dataset = \"doclaynet_val\"\n",
    "paths = glob(f\"/mnt/AI_NAS/datalake/catalog/provider={provider}/dataset={dataset}/**/*.parquet\", recursive=True)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = paths[1]\n",
    "db = pd.read_parquet(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed45a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[0]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[db['path'].str.contains(f\"f105ff5c\")]['path'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_shard_config(total_images: int):\n",
    "    \n",
    "    if total_images < 10000:\n",
    "        # ìƒ¤ë”© ì—†ìŒ\n",
    "        return {\"levels\": 0, \"dirs\": 1}\n",
    "    elif total_images < 2500000:  # 256 * 10000\n",
    "        # 1ë‹¨ê³„: xx/ (256ê°œ í´ë”)\n",
    "        return {\"levels\": 1, \"dirs\": 256}\n",
    "    else:\n",
    "        # 2ë‹¨ê³„: xx/xx/ (65536ê°œ í´ë”)  \n",
    "        return {\"levels\": 2, \"dirs\": 65536}\n",
    "\n",
    "def _get_level_path(base_path, shard_config, image_hash: str) :\n",
    "    levels = shard_config[\"levels\"]\n",
    "    \n",
    "    if levels == 0:\n",
    "        return base_path / f\"{image_hash}.jpg\"\n",
    "    elif levels == 1:\n",
    "        return base_path / image_hash[:2] / f\"{image_hash}.jpg\"\n",
    "    elif levels == 2:  \n",
    "        return base_path / image_hash[:2] / image_hash[2:4] / f\"{image_hash}.jpg\"\n",
    "\n",
    "# dbì˜ path basenameì„ ì–»ê³  shard ë‹¤ì‹œ êµ¬í•˜ê¸° base_pathëŠ” f\"provider={provider}/dataset={dataset}/\"\n",
    "base_path = Path(f\"provider={provider}/dataset={dataset}/\")\n",
    "temp_path = Path(\"/mnt/AI_NAS/datalake/assets\")\n",
    "shard_config = _get_shard_config(len(db))\n",
    "new_db = db.copy()\n",
    "new_db[\"new_path\"] = new_db[\"path\"].apply(\n",
    "    lambda x: str(_get_level_path(base_path, shard_config, Path(x).stem))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d6655",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db.loc[0]['new_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = Dataset.from_pandas(new_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b46c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_source_exists(row):\n",
    "    \"\"\"ì†ŒìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\"\"\"\n",
    "    src_path = str(temp_path / row[\"path\"])\n",
    "    new_path = str(temp_path / row[\"new_path\"])\n",
    "    \n",
    "    src_bool = Path(src_path).exists()\n",
    "    new_bool = Path(new_path).exists()\n",
    "    return src_bool or new_bool\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57952fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"/home/kai/workspace/DeepDocs_Project/datalake/downloads/post_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a580ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds ì˜ pathì— 07ì´ ë“¤ì–´ê°„ê±° ì°¾ê°€\n",
    "\n",
    "ds = ds.filter(lambda x: \"a782d\" in x[\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42738749",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists('/mnt/AI_NAS/datalake/assetsprovider=inhouse/dataset=postoffice_label/a782dbc7824fa84d0d86b99ab0f5c9c70ca30ce1b2382bec45233468f90aa81f.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1ë‹¨ê³„: ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§\n",
    "print(\"ğŸ” ì¡´ì¬í•˜ëŠ” íŒŒì¼ í•„í„°ë§ ì¤‘...\")\n",
    "valid_ds = new_ds.filter(\n",
    "    check_source_exists,\n",
    "    num_proc=16,\n",
    "    desc=\"íŒŒì¼ ì¡´ì¬ í™•ì¸\"\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(new_ds)} â†’ {len(valid_ds)}ê°œ\")\n",
    "new_db = valid_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_file(row):\n",
    "    src_path = str(temp_path / row[\"path\"])\n",
    "    dst_path = str(temp_path / row[\"new_path\"])\n",
    "    dst_path = dst_path.replace(f\"dataset={dataset}/\", f\"dataset={dataset}_1/\")\n",
    "    dst_path = Path(dst_path)\n",
    "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if dst_path.exists():\n",
    "        return\n",
    "    elif not Path(src_path).exists():\n",
    "        print(f\"Source {src_path} does not exist, skipping.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        shutil.move(src_path, dst_path)\n",
    "    except FileNotFoundError:\n",
    "        # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ì´ë¯¸ ì´ë™í–ˆì„ ìˆ˜ ìˆìŒ\n",
    "        print(f\"File already moved: {src_path}\")\n",
    "\n",
    "# path ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "print(\"ğŸ” ì¤‘ë³µ path ì œê±° ì¤‘...\")\n",
    "unique_db = new_db.drop_duplicates(subset=['path']).reset_index(drop=True)\n",
    "unique_ds = Dataset.from_pandas(unique_db)\n",
    "\n",
    "print(f\"ğŸ“Š ì¤‘ë³µ ì œê±°: {len(valid_ds)} â†’ {len(unique_ds)}ê°œ\")\n",
    "\n",
    "print(\"ğŸ“ íŒŒì¼ ì´ë™ ì¤‘...\")\n",
    "\n",
    "unique_ds.map(\n",
    "    move_file, \n",
    "    num_proc=16, \n",
    "    desc=\"íŒŒì¼ ì´ë™\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dbì˜ pathê°€ ëª¨ë‘ existsí•œì§€ check\n",
    "def check_path_exists(path):\n",
    "    path = temp_path / path\n",
    "    return path.exists()\n",
    "\n",
    "new_db[:100][\"new_path\"].apply(check_path_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d80297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dbì— new_pathë¥¼ pathë¡œ ë³€ê²½\n",
    "new_db[\"path\"] = new_db[\"new_path\"]\n",
    "new_db = new_db.drop(columns=[\"new_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59200bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db[:10]['path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd539a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db.to_parquet(db_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5da4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"id\": 1,\n",
    "    \"text\": \"Title\",\n",
    "    \"label\": \"key\",\n",
    "    \"linking\": [[1,2]]\n",
    "},\n",
    "{\n",
    "    \"id\": 2,\n",
    "    \"text\": \"subTitle\",\n",
    "    \"label\": \"key\",\n",
    "    \"linking\": [[1,2],[2,3]]\n",
    "},\n",
    "{\n",
    "    \"id\": 3,\n",
    "    \"text\": \"Content\",\n",
    "    \"label\": \"value\",\n",
    "    \"linking\": [[2,3]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb806c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.create_table_from_parquet(\n",
    "    \"test\",\n",
    "    \"/mnt/AI_NAS/datalake/**/*.parquet\",\n",
    "    hive_partitioning=True,\n",
    "    union_by_name=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a171358",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DuckDBClient(\n",
    "    database_path=\"/mnt/AI_NAS/datalake/db/catalog.duckdb\",\n",
    "    read_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fead3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('/home/kai/workspace/DeepDocs_Project/datalake/downloads/export_11283_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# widthê°€ 1000 ì´ìƒì¸ ê²ƒë§Œ í•„í„°ë§\n",
    "f = dataset.filter(\n",
    "    lambda x: x >= 1000,\n",
    "    num_proc=4,\n",
    "    input_columns=['width'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argparse\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = parser.add_subparsers(dest='command')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conn.execute(\"\"\"\n",
    "    SELECT * FROM read_parquet('/mnt/AI_NAS/datalake/catalog/provider=*/dataset=*/task=*/variant=*/**/data.parquet', union_by_name=True)\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14136466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:9064/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model\n",
    "\n",
    "models = client.models.list()\n",
    "for model in models.data:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def base64_encode(\n",
    "    image_path,\n",
    "):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "image_path = \"/home/kai/workspace/DeepDocs_Project/datalake/managers/test4.jpg\"\n",
    "base64_image = base64_encode(\n",
    "    image_path,\n",
    ")\n",
    "    \n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Extract all layout elements. Reading order does not matter.\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"adapter\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "tokens = completion.usage.total_tokens\n",
    "tokens_per_second = tokens / elapsed_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a319ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Qwen2VLImageProcessor.from_pretrained(\"/mnt/AI_NAS/AI_MODEL/checkpoints/Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image = Image.open(image_path)\n",
    "inputs = processor(\n",
    "    images=[image],\n",
    "    return_tensors=\"np\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41795c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "image = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(image)\n",
    "target = json.loads(result)\n",
    "\n",
    "for item in target:\n",
    "    class_ = item.get(\"type\", \"unknown\")\n",
    "    bbox = item.get(\"bbox\", [0, 0, 0, 0])\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "    # \n",
    "    x0 -= 14\n",
    "    x1 += 14\n",
    "    y0 -= 14\n",
    "    y1 += 14\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=2)\n",
    "    draw.text((x0, y0), class_, fill=\"red\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/kai/workspace/DeepDocs_Project/datalake\")\n",
    "from client.src.core.duckdb_client import DuckDBClient\n",
    "from datalake_client import DatalakeClient\n",
    "from datasets import Dataset, load_from_disk\n",
    "import os\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# datalake_client.py\n",
    "manager = DatalakeClient(\n",
    "    nas_api_url=\"http://192.168.20.62:8091\",\n",
    "    num_proc=32)\n",
    "duckdb = DuckDBClient(\n",
    "    database_path=manager.base_path / \"catalog.duckdb\",\n",
    "    read_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.show_nas_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bebf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_db = duckdb.retrieve_partitions()\n",
    "all_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 30\n",
    "provider = all_db.iloc[index]['provider']\n",
    "dataset = all_db.iloc[index]['dataset']\n",
    "print(f\"provider: {provider}, dataset: {dataset}\")\n",
    "db = duckdb.retrieve_with_existing_cols(\n",
    "    providers=provider,\n",
    "    datasets=dataset,\n",
    ")\n",
    "\n",
    "print(\"db:\")\n",
    "\n",
    "db = db.drop(columns=['provider', 'dataset', 'task', 'variant'], errors='ignore')\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"kie\"\n",
    "variant = \"base_kie\"\n",
    "meta = {\n",
    "    \"lang\": \"ko\",\n",
    "    \"src\": \"real\",\n",
    "}\n",
    "staging_dir, job_id = manager.upload_task_data(\n",
    "    data_file=db,\n",
    "    provider=provider,\n",
    "    dataset=dataset,\n",
    "    task=task,\n",
    "    variant=variant,\n",
    "    dataset_description=\"ì„¤ëª…ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.\",\n",
    "    overwrite=False,\n",
    "    meta=meta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11160f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.trigger_nas_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.wait_for_job_completion(\"job_20250617_060301_119\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = []\n",
    "for parquet_path in tqdm(parquets):\n",
    "    try:\n",
    "        db = Dataset.from_parquet(parquet_path)\n",
    "        if \"image_path\" in db.column_names:\n",
    "            db = db.map(lambda x: {\"image_path\": apply_image_path(x)})\n",
    "        # if dbì— dateê°€ ì—†ì„ê²½ìš°\n",
    "        if \"date\" not in db.column_names:\n",
    "            db = db.map(lambda x: {\"date\": datetime.now().strftime(\"%Y-%m-%d\")})\n",
    "        \n",
    "        provider, dataset, task, variant, *etc = parquet_path.replace(\"/mnt/AI_NAS/datalake/catalog/\",\"\").split(\"/\")[:-1]\n",
    "        provider = provider.replace(\"provider=\", \"\")\n",
    "        dataset = dataset.replace(\"dataset=\", \"\")\n",
    "        task = task.replace(\"task=\", \"\")\n",
    "        variant = variant.replace(\"variant=\", \"\")\n",
    "        meta = {}\n",
    "        for et in etc:\n",
    "            key, value = et.split(\"=\")\n",
    "            meta[key] = value\n",
    "        if \"table\" in variant:\n",
    "            meta[\"mod\"] = \"table\"        \n",
    "\n",
    "        print(f\"provider: {provider}, dataset: {dataset}, task: {task}, variant: {variant}, meta: {meta}\")\n",
    "        temp_path = f\"./temp/{provider}/{dataset}/{task}/{variant}\"\n",
    "        db.save_to_disk(temp_path)\n",
    "        staging_dir, job_id = manager.upload_raw_data(\n",
    "            data_file=temp_path,\n",
    "            provider=provider,\n",
    "            dataset=dataset,\n",
    "            dataset_description=\"ì„¤ëª…ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.\",\n",
    "            original_source=\"ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\",\n",
    "            overwrite=False,\n",
    "        )\n",
    "        # manager.trigger_nas_processing()\n",
    "        # manager.wait_for_job_completion(job_id, timeout=1280000)\n",
    "        # uploaded_db = Dataset.from_parquet(f\"/mnt/AI_NAS/datalake/catalog/provider={provider}/dataset={dataset}/task=raw/variant=*/data.parquet\")\n",
    "        # uploaded_db.save_to_disk(temp_path)\n",
    "        # manager.upload_task_data(\n",
    "        #     data_file=temp_path,\n",
    "        #     provider=provider,\n",
    "        #     dataset=dataset,\n",
    "        #     task=task,\n",
    "        #     variant=variant,\n",
    "        #     dataset_description=\"ì„¤ëª…ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.\",\n",
    "        #     original_source=\"ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\",\n",
    "        #     **meta,  # unpack meta dictionary\n",
    "        # )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {parquet_path}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        error_list.append({\n",
    "            \"parquet_path\": parquet_path,\n",
    "            \"error\": str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e00dba",
   "metadata": {},
   "source": [
    "parquets = glob(\"/mnt/AI_NAS/datalake/catalog/provider=*/dataset=*/task=*/**/*.parquet\", recursive=True)\n",
    "all_parquets = parquets.copy()\n",
    "print(len(parquets))\n",
    "parquets = [p for p in parquets if \"synthtabnet\" not in p]\n",
    "parquets = [p for p in parquets if \"pubtables\" not in p]\n",
    "parquets = [p for p in parquets if \"pubtabnet\" not in p]\n",
    "parquets = [p for p in parquets if \"fintabnet\" not in p]\n",
    "parquets = [p for p in parquets if \"vis_qa\" not in p]\n",
    "parquets = [p for p in parquets if \"table_image_text_pair\" not in p]\n",
    "parquets = [p for p in parquets if \"diverse_ocr_word\" not in p]\n",
    "parquets = [p for p in parquets if \"diverse_ocr_char\" not in p]\n",
    "parquets = [p for p in parquets if \"tourism_food_menu_board\" not in p]\n",
    "print(len(parquets))\n",
    "\n",
    "# ì—­ìœ¼ë¡œ êµ¬í•˜ê¸°\n",
    "\n",
    "reverse_parquets = [p for p in all_parquets if p not in parquets]\n",
    "reverse_parquets = [p for p in all_parquets if p not in parquets]\n",
    "parquets = reverse_parquets.copy()\n",
    "parquets = [p for p in parquets if \"pubtabnet_otsl\" not in p]\n",
    "parquets = [p for p in parquets if \"tourism_food_menu_board\" not in p]\n",
    "parquets = [p for p in parquets if \"table_image_text_pair\" not in p]\n",
    "print(len(parquets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_image_path(row):\n",
    "    if row['image_path']:\n",
    "        path = f\"/mnt/AI_NAS/datalake/{row['image_path']}\"\n",
    "        # check path exists\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        raise FileNotFoundError(f\"Image path does not exist: {path}\")\n",
    "    return None\n",
    "error_list = []\n",
    "for parquet_path in tqdm(parquets):\n",
    "    try:\n",
    "        db = Dataset.from_parquet(parquet_path)\n",
    "        if \"image_path\" in db.column_names:\n",
    "            db = db.map(lambda x: {\"image_path\": apply_image_path(x)})\n",
    "        # if dbì— dateê°€ ì—†ì„ê²½ìš°\n",
    "        if \"date\" not in db.column_names:\n",
    "            db = db.map(lambda x: {\"date\": datetime.now().strftime(\"%Y-%m-%d\")})\n",
    "        \n",
    "        provider, dataset, task, variant, *etc = parquet_path.replace(\"/mnt/AI_NAS/datalake/catalog/\",\"\").split(\"/\")[:-1]\n",
    "        provider = provider.replace(\"provider=\", \"\")\n",
    "        dataset = dataset.replace(\"dataset=\", \"\")\n",
    "        task = task.replace(\"task=\", \"\")\n",
    "        variant = variant.replace(\"variant=\", \"\")\n",
    "        meta = {}\n",
    "        for et in etc:\n",
    "            key, value = et.split(\"=\")\n",
    "            meta[key] = value\n",
    "        if \"table\" in variant:\n",
    "            meta[\"mod\"] = \"table\"        \n",
    "\n",
    "        print(f\"provider: {provider}, dataset: {dataset}, task: {task}, variant: {variant}, meta: {meta}\")\n",
    "        temp_path = f\"./temp/{provider}/{dataset}/{task}/{variant}\"\n",
    "        db.save_to_disk(temp_path)\n",
    "        staging_dir, job_id = manager.upload_raw_data(\n",
    "            data_file=temp_path,\n",
    "            provider=provider,\n",
    "            dataset=dataset,\n",
    "            dataset_description=\"ì„¤ëª…ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.\",\n",
    "            original_source=\"ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\",\n",
    "            overwrite=False,\n",
    "        )\n",
    "        # manager.trigger_nas_processing()\n",
    "        # manager.wait_for_job_completion(job_id, timeout=1280000)\n",
    "        # uploaded_db = Dataset.from_parquet(f\"/mnt/AI_NAS/datalake/catalog/provider={provider}/dataset={dataset}/task=raw/variant=*/data.parquet\")\n",
    "        # uploaded_db.save_to_disk(temp_path)\n",
    "        # manager.upload_task_data(\n",
    "        #     data_file=temp_path,\n",
    "        #     provider=provider,\n",
    "        #     dataset=dataset,\n",
    "        #     task=task,\n",
    "        #     variant=variant,\n",
    "        #     dataset_description=\"ì„¤ëª…ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.\",\n",
    "        #     original_source=\"ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\",\n",
    "        #     **meta,  # unpack meta dictionary\n",
    "        # )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {parquet_path}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        error_list.append({\n",
    "            \"parquet_path\": parquet_path,\n",
    "            \"error\": str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b888aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_dir, job_id = manager.upload_raw_data(\n",
    "    data_file=temp_path,\n",
    "    provider=provider,\n",
    "    dataset=dataset,\n",
    "    dataset_description=\"ì„¤ëª…ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.\",\n",
    "    original_source=\"ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b984c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.show_nas_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.trigger_nas_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.wait_for_job_completion(\"job_20250616_074217_948\", timeout=1280000, polling_interval=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.move(\"/data.parquet\", \"/mnt/AI_NAS/datalake/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.show_nas_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0633a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.trigger_nas_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341155cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.upload_raw_data(\n",
    "    data_file=\"arrowê²½ë¡œ\",\n",
    "    provider=\"provider_name\",\n",
    "    dataset=\"dataset_name\",\n",
    "    dataset_description=\"ì„¤ëª…ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.\",\n",
    "    original_source=\"ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.upload_task_data(\n",
    "    data_file=parquet_path,\n",
    "    provider=provider,\n",
    "    dataset=dataset,\n",
    "    task=task,\n",
    "    variant=variant,\n",
    "    dataset_description=\"ì„¤ëª…ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.\",\n",
    "    original_source=\"ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\",\n",
    "    **meta,  # unpack meta dictionary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = client.execute(\"\"\"\n",
    "    SELECT * FROM read_parquet('/mnt/AI_NAS/datalake/catalog/provider=*/dataset=*/task=*/variant=*/data.parquet', union_by_name=True, filename=True, hive_partitioning=True)\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset.from_pandas(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0638af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathì— /mnt/AI_NASd/datalake/assets ì¶”ê°€\n",
    "d = d.map(lambda x: {\"path\": f\"/mnt/AI_NAS/datalake/assets/{x['path']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dì˜ filenameì´ exstsí•œì§€ í™•ì¸\n",
    "def check_image_exists(row):\n",
    "    if row['path']:\n",
    "        path = row['path']\n",
    "        return os.path.exists(path)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.map(lambda x: {\"image_exists\": check_image_exists(x)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaiocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
