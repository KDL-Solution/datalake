import zipfile
import json
import pdfplumber
import hashlib
import pandas as pd
from pathlib import Path
from collections import defaultdict
from io import BytesIO
from PIL import Image
from tqdm import tqdm

from utils import NAS_ROOT


def unzip_all_zips_in_dir(
    target_dir: str,
    save_dir: str,  # ì¶”ê°€: ì••ì¶• í•´ì œí•  ë£¨íŠ¸ ë””ë ‰í† ë¦¬
) -> None:
    target_dir = Path(target_dir).resolve()
    save_dir = Path(save_dir).resolve()
    save_dir.mkdir(parents=True, exist_ok=True)

    # ëª¨ë“  í•˜ìœ„ í´ë”ë¥¼ í¬í•¨í•´ì„œ .zip íŒŒì¼ ì°¾ê¸°
    for zip_path in target_dir.rglob("*.zip"):
        # zip ì´ë¦„ë§Œ ê°€ì ¸ì™€ì„œ í´ë”ëª… ìƒì„± (í™•ì¥ì ì œê±°)
        extract_subdir = save_dir / zip_path.stem
        extract_subdir.mkdir(parents=True, exist_ok=True)

        print(f"ğŸ“¦ Extracting {zip_path} -> {extract_subdir}")

        # ì••ì¶• í•´ì œ
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_subdir)


def sha256_pil_image(
    image: Image.Image,
) -> str:
    h = hashlib.sha256()
    with BytesIO() as buffer:
        image.save(buffer, format="JPEG")
        h.update(buffer.getvalue())
    return h.hexdigest()


def main(
    unzip: bool = True,
    save_images: bool = True,
    dpi: int = 144,
) -> None:
    script_dir = Path(__file__).resolve().parent
    data_dir = script_dir / "data"
    images_dir = script_dir / "images"

    if unzip:
        unzip_all_zips_in_dir(
            target_dir=NAS_ROOT / f"source/{script_dir.parents[2].stem}/{script_dir.parents[1].stem}",
            save_dir=script_dir / "data",
        )

    gt_paths = [
        i for i in list(data_dir.glob("**/*.json"))
        if "ë¼ë²¨ë§ë°ì´í„°" in i.parent.as_posix()
    ]
    doc_ids = [gt_path.stem.rsplit("_", 1)[-1]  for gt_path in gt_paths]
    # GTì—ì„œ doc idì— ì¤‘ë³µì´ ìˆëŠ”ì§€ í™•ì¸:
    assert len(doc_ids) == len(set(doc_ids))

    # doc idì™€ pageë³„ë¡œ ì´ë¯¸ì§€ì˜ ê²½ë¡œë¥¼ ì €ì¥:
    rows = []
    data_dict = defaultdict(dict)
    for pdf_path in data_dir.glob("**/*.pdf"):
        if "ì›ì²œë°ì´í„°" not in pdf_path.parent.as_posix():
            continue

        doc_id = pdf_path.stem.rsplit("_", 1)[-1]
        # .pdfì—ì„œ doc idì— ì¤‘ë³µì´ ìˆëŠ”ì§€ í™•ì¸:
        if doc_id in data_dict:
            print(f"Duplication in doc id: {doc_id}")

        with pdfplumber.open(pdf_path) as pdf:
            for page_no, page in enumerate(
                pdf.pages,
                start=1,
            ):
                image = page.to_image(
                    resolution=dpi,
                    antialias=False,
                )
                pil_image = image.original.convert("RGB")
                width, height = pil_image.size
                sha256 = sha256_pil_image(
                    pil_image,
                )
                save_path = images_dir / f"{sha256[: 2]}/{sha256}.jpg"
                if save_images and not save_path.exists():
                    save_path.parent.mkdir(
                        parents=True,
                        exist_ok=True,
                    )
                    pil_image.save(
                        save_path,
                        format="JPEG",
                    )

                data_dict[doc_id][page_no] = {
                    "image_path": save_path.relative_to(
                        images_dir,
                    ).as_posix(),
                    "width": width,
                    "height": height,
                }

    for gt_path in tqdm(gt_paths):
        with open(gt_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        doc_id = gt_path.stem.rsplit("_", 1)[-1]

        for row in data["source_data_info"]:
            pages = row["page_no"]

            # QAì™€ ì§ì§€ì–´ì§„ í˜ì´ì§€ê°€ ì—¬ëŸ¿ì¸ ê²½ìš°ëŠ” ì œì™¸:
            if len(pages) != 1:
                continue

            for qa in row["qa_data"]:
                user_prompt = qa["question"]
                assistant_prompt = qa["answer"]

                rows.append(
                    {
                        "query": user_prompt,
                        "label": assistant_prompt,
                    } | data_dict[doc_id][pages[0]]
                )

    df = pd.DataFrame(rows)
    df.to_parquet(
        script_dir / "data.parquet",
        index=False,
    )


if __name__ == "__main__":
    # datalake/datalake-prepì—ì„œ ì‹¤í–‰í•˜ì‹œì˜¤: e.g., `python -m provider=aihub.vis_qa.qa.parquet`.
    import fire

    fire.Fire(main)
