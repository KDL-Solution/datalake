{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.trigger_nas_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duck db\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e086a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect(database=\"/mnt/AI_NAS/datalake/catalog/**/*.parquet\", read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conn.execute(\"\"\"\n",
    "    SELECT * FROM read_parquet('/mnt/AI_NAS/datalake/catalog/provider=*/dataset=*/task=*/variant=*/**/data.parquet', union_by_name=True)\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14136466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:9064/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model\n",
    "\n",
    "models = client.models.list()\n",
    "for model in models.data:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def base64_encode(\n",
    "    image_path,\n",
    "):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "image_path = \"/home/kai/workspace/DeepDocs_Project/datalake/managers/test4.jpg\"\n",
    "base64_image = base64_encode(\n",
    "    image_path,\n",
    ")\n",
    "    \n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Extract all layout elements. Reading order does not matter.\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"adapter\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "tokens = completion.usage.total_tokens\n",
    "tokens_per_second = tokens / elapsed_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a319ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Qwen2VLImageProcessor.from_pretrained(\"/mnt/AI_NAS/AI_MODEL/checkpoints/Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image = Image.open(image_path)\n",
    "inputs = processor(\n",
    "    images=[image],\n",
    "    return_tensors=\"np\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41795c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "image = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(image)\n",
    "target = json.loads(result)\n",
    "\n",
    "for item in target:\n",
    "    class_ = item.get(\"type\", \"unknown\")\n",
    "    bbox = item.get(\"bbox\", [0, 0, 0, 0])\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "    # \n",
    "    x0 -= 14\n",
    "    x1 += 14\n",
    "    y0 -= 14\n",
    "    y1 += 14\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=2)\n",
    "    draw.text((x0, y0), class_, fill=\"red\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0170da83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 09:37:23,177 - datalake_client - INFO - ‚úÖ NAS API ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌôïÏù∏: http://192.168.20.62:8000\n"
     ]
    }
   ],
   "source": [
    "from datalake_client import DatalakeClient\n",
    "from datasets import Dataset, load_from_disk\n",
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "# datalake_client.py\n",
    "manager = DatalakeClient(\n",
    "    nas_api_url=\"http://192.168.20.62:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e1f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä NAS Data Processing Dashboard\n",
      "============================================================\n",
      "üì¶ Pending: 0Í∞ú\n",
      "üîÑ Processing: 0Í∞ú\n",
      "‚ùå Failed: 5Í∞ú\n",
      "üñ•Ô∏è Server Status: running\n",
      "‚è∞ Last Updated: 2025-06-16T09:37:23.858488\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manager.show_nas_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e00dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquets = glob(\"/mnt/AI_NAS/datalake/catalog/provider=*/dataset=*/task=*/**/*.parquet\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b468b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provider: huggingface, dataset: funsd_plus, task: vqa, variant: base_vqa, meta: {'lang': 'en', 'src': 'real'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3251f9a3f2e9480297fead95ff6ae5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 09:37:28,844 - datalake_client - INFO - üì• Raw data ÏóÖÎ°úÎìú ÏãúÏûë: huggingface/funsd_plus\n",
      "2025-06-16 09:37:28,855 - datalake_client - INFO - üìÇ Îç∞Ïù¥ÌÑ∞ ÌååÏùº Î°úÎìú Ï§ë: /home/kai/workspace/DeepDocs_Project/datalake/managers/temp/huggingface/funsd_plus/vqa/base_vqa\n",
      "2025-06-16 09:37:28,859 - datalake_client - INFO - ‚úÖ datasets Ìè¥Îçî Î°úÎìú ÏôÑÎ£å: 1112 Ìñâ\n",
      "2025-06-16 09:37:28,860 - datalake_client - INFO - ‚úÖ Îç∞Ïù¥ÌÑ∞ ÌååÏùº Î°úÎìú ÏôÑÎ£å: ./temp/huggingface/funsd_plus/vqa/base_vqa\n",
      "2025-06-16 09:37:28,860 - datalake_client - INFO - Îç∞Ïù¥ÌÑ∞ÏÖã Ïª¨Îüº: ['image_path', 'query', 'label', 'width', 'height', 'date']\n",
      "2025-06-16 09:37:28,861 - datalake_client - INFO - üîç JSON Î≥ÄÌôò ÎåÄÏÉÅ Ïª¨Îüº Í≤ÄÏÇ¨ ÏãúÏûë\n",
      "2025-06-16 09:37:28,863 - datalake_client - INFO - üìÑ JSON Î≥ÄÌôò ÎåÄÏÉÅ Ïª¨Îüº ÏóÜÏùå\n",
      "2025-06-16 09:37:28,864 - datalake_client - INFO - üîÑ ÌååÏùº Ïª¨Îüº ÌëúÏ§ÄÌôî: image_path ‚Üí file_path\n",
      "2025-06-16 09:37:28,866 - datalake_client - INFO - üìÑ ÌååÏùº Î∂ÑÏÑù Í≤∞Í≥º: variant=image, Ïù¥ÎØ∏ÏßÄÏª¨Îüº=[], ÌååÏùºÏª¨Îüº=['image_path'], ÌôïÏû•Ïûê={'.jpg'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24e86d9c73b44b494d1b4a35f6a39e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd74c12dc52d4d8b853c56cdba44cd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 09:37:31,767 - datalake_client - INFO - üì¶ datasets Ï†ÄÏû• ÏôÑÎ£å: /mnt/AI_NAS/datalake/migrate_test/staging/pending/funsd_plus_raw_image_8891f826_20250616093728_kai\n",
      "2025-06-16 09:37:31,768 - datalake_client - INFO - ‚úÖ Task Îç∞Ïù¥ÌÑ∞ ÏóÖÎ°úÎìú ÏôÑÎ£å: /mnt/AI_NAS/datalake/migrate_test/staging/pending/funsd_plus_raw_image_8891f826_20250616093728_kai\n",
      "2025-06-16 09:37:31,769 - datalake_client - INFO - üîÑ NAS Ï≤òÎ¶¨ ÏöîÏ≤≠ Ï§ë...\n",
      "2025-06-16 09:37:31,774 - datalake_client - INFO - ‚úÖ Ï≤òÎ¶¨ ÏûëÏóÖ ÏãúÏûëÎê®: job_20250616_093731_773\n",
      "2025-06-16 09:37:31,775 - datalake_client - INFO - ‚è≥ ÏûëÏóÖ ÏôÑÎ£å ÎåÄÍ∏∞ Ï§ë: None\n",
      "2025-06-16 09:37:31,779 - datalake_client - WARNING - ‚ö†Ô∏è ÏûëÏóÖÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /mnt/AI_NAS/datalake/catalog/provider=huggingface/dataset=funsd_plus/task=vqa/variant=base_vqa/lang=en/src=real/data.parquet: ÏûëÏóÖ ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: None\n"
     ]
    }
   ],
   "source": [
    "def apply_image_path(row):\n",
    "    if row['image_path']:\n",
    "        path = f\"/mnt/AI_NAS/datalake/{row['image_path']}\"\n",
    "        # check path exists\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        raise FileNotFoundError(f\"Image path does not exist: {path}\")\n",
    "    return None\n",
    "error_list = []\n",
    "for parquet_path in parquets:\n",
    "    try:\n",
    "        db = Dataset.from_parquet(parquet_path)\n",
    "        db = db.map(lambda x: {\"image_path\": apply_image_path(x)})\n",
    "        # if dbÏóê dateÍ∞Ä ÏóÜÏùÑÍ≤ΩÏö∞\n",
    "        if \"date\" not in db.column_names:\n",
    "            db = db.map(lambda x: {\"date\": datetime.now().strftime(\"%Y-%m-%d\")})\n",
    "            \n",
    "        sample_image_path = db[0]['image_path']\n",
    "        \n",
    "        provider, dataset, task, variant, *etc = parquet_path.replace(\"/mnt/AI_NAS/datalake/catalog/\",\"\").split(\"/\")[:-1]\n",
    "        provider = provider.replace(\"provider=\", \"\")\n",
    "        dataset = dataset.replace(\"dataset=\", \"\")\n",
    "        task = task.replace(\"task=\", \"\")\n",
    "        variant = variant.replace(\"variant=\", \"\")\n",
    "        meta = {}\n",
    "        for et in etc:\n",
    "            key, value = et.split(\"=\")\n",
    "            meta[key] = value\n",
    "        if \"table\" in variant:\n",
    "            meta[\"mod\"] = \"table\"        \n",
    "\n",
    "        print(f\"provider: {provider}, dataset: {dataset}, task: {task}, variant: {variant}, meta: {meta}\")\n",
    "        temp_path = f\"./temp/{provider}/{dataset}/{task}/{variant}\"\n",
    "        db.save_to_disk(temp_path)\n",
    "        staging_dir, job_id = manager.upload_raw_data(\n",
    "            data_file=temp_path,\n",
    "            provider=provider,\n",
    "            dataset=dataset,\n",
    "            dataset_description=\"ÏÑ§Î™ÖÏùÑ Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî.\",\n",
    "            original_source=\"ÎßÅÌÅ¨Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.\",\n",
    "        )\n",
    "        # manager.trigger_nas_processing()\n",
    "        # manager.wait_for_job_completion(job_id, timeout=1280000)\n",
    "        # uploaded_db = Dataset.from_parquet(f\"/mnt/AI_NAS/datalake/migrate_test/catalog/provider={provider}/dataset={dataset}/task=raw/variant=*/data.parquet\")\n",
    "        # uploaded_db.save_to_disk(temp_path)\n",
    "        # manager.upload_task_data(\n",
    "        #     data_file=temp_path,\n",
    "        #     provider=provider,\n",
    "        #     dataset=dataset,\n",
    "        #     task=task,\n",
    "        #     variant=variant,\n",
    "        #     dataset_description=\"ÏÑ§Î™ÖÏùÑ Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî.\",\n",
    "        #     original_source=\"ÎßÅÌÅ¨Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.\",\n",
    "        #     **meta,  # unpack meta dictionary\n",
    "        # )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {parquet_path}: {e}\")\n",
    "        error_list.append({\n",
    "            \"parquet_path\": parquet_path,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Dataset.from_parquet(f\"/mnt/AI_NAS/datalake/migrate_test/catalog/provider={provider}/dataset={dataset}/task=vqa/variant=*/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327cb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_db = Dataset.from_parquet(f\"/mnt/AI_NAS/datalake/migrate_test/catalog/provider={provider}/dataset={dataset}/task=raw/variant=*/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d80329",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341155cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.upload_raw_data(\n",
    "    data_file=\"arrowÍ≤ΩÎ°ú\",\n",
    "    provider=\"provider_name\",\n",
    "    dataset=\"dataset_name\",\n",
    "    dataset_description=\"ÏÑ§Î™ÖÏùÑ Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî.\",\n",
    "    original_source=\"ÎßÅÌÅ¨Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.upload_task_data(\n",
    "    data_file=parquet_path,\n",
    "    provider=provider,\n",
    "    dataset=dataset,\n",
    "    task=task,\n",
    "    variant=variant,\n",
    "    dataset_description=\"ÏÑ§Î™ÖÏùÑ Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî.\",\n",
    "    original_source=\"ÎßÅÌÅ¨Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.\",\n",
    "    **meta,  # unpack meta dictionary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaiocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
