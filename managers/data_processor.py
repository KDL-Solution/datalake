import logging
import json
import shutil
import hashlib
import io
import threading
import time
import os
from datetime import datetime
from tqdm import tqdm
from pathlib import Path
from typing import Dict, Optional, List
import gc

from datasets import Dataset, load_from_disk
from datasets.features import Image as ImageFeature
from PIL import Image

class NASDataProcessor:
    
    def __init__(
        self,
        base_path: str = "/mnt/AI_NAS/datalake/migrate_test",
        log_level: str = "INFO",
        num_proc: int = 4,
        batch_size: int = 1000,  # map()ì˜ ë°°ì¹˜ í¬ê¸°
    ):
        # ê²½ë¡œ ì„¤ì •
        self.base_path = Path(base_path)
        
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        self.catalog_path = self.base_path / "catalog"
        self.assets_path = self.base_path / "assets"
        
        self.num_proc = num_proc
        self.batch_size = batch_size
        self.image_column_candidates = ["image", "image_bytes"]
        
        self._setup_console_logging(log_level)
        self._check_path_and_setup_logging()
        
        self.existing_hashes = set()
        self.cache_built = False
        self.cache_lock = threading.Lock()
        
        # ì²˜ë¦¬ ì‹¤íŒ¨ ì¶”ì ìš©
        self.processing_failed = False
        self.failure_lock = threading.Lock()
        self.error_messages = []
        
        self.logger.info(f"ğŸš€ OptimizedNASDataProcessor ì´ˆê¸°í™” (ë³‘ë ¬: {self.num_proc}, ë°°ì¹˜: {batch_size})")

    def _setup_console_logging(self, log_level: str) -> logging.Logger:
        """ê¸°ë³¸ ë¡œê¹… ì„¤ì •"""
        
        self.formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(self.formatter)
        self.logger = logging.getLogger(__name__)
        self.logger.addHandler(console_handler)
        
        if log_level.upper() == "DEBUG":
            self.logger.setLevel(logging.DEBUG)
        else:
            self.logger.setLevel(logging.INFO)

    def _check_path_and_setup_logging(self):
        
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
        }
        
        missing_paths = []
        for path_name, path_obj in required_paths.items():
            if not path_obj.exists():
                missing_paths.append(f"  - {path_name}: {path_obj}")
        
        if missing_paths:
            missing_list = '\n'.join(missing_paths)
            raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
            
        self.logger.info("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")
        
        log_dir = self.base_path / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        date_str = datetime.now().strftime("%Y%m%d")
        user = os.getenv('USER', 'unknown')
        log_file = log_dir / f"DataProcessor_{date_str}_{user}.log"
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(self.formatter)
        self.logger.addHandler(file_handler)
        self.logger.info(f"ğŸ“ íŒŒì¼ ë¡œê¹… í™œì„±í™”: {log_file}")
        self.logger.info(f"ğŸš€ DataProcessor ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_status(self) -> Dict:
        """ê°„ë‹¨í•œ ìƒíƒœ ì¡°íšŒ"""
        return {
            "pending": len(list(self.staging_pending_path.glob("*"))) if self.staging_pending_path.exists() else 0,
            "processing": len(list(self.staging_processing_path.glob("*"))) if self.staging_processing_path.exists() else 0,
            "failed": len(list(self.staging_failed_path.glob("*"))) if self.staging_failed_path.exists() else 0
        }
    
    def process_all_pending(self) -> Dict:
        """ëª¨ë“  Pending ë°ì´í„° ì²˜ë¦¬"""
        self.logger.info("ğŸ”„ Pending ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        if not self.staging_pending_path.exists():
            return {"success": 0, "failed": 0, "message": "Pending ë””ë ‰í† ë¦¬ ì—†ìŒ"}
        
        pending_dirs = [d for d in self.staging_pending_path.iterdir() if d.is_dir()]
        
        if not pending_dirs:
            return {"success": 0, "failed": 0, "message": "ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ"}
        
        self.logger.info(f"ğŸ“¦ ì²˜ë¦¬ ëŒ€ìƒ: {len(pending_dirs)}ê°œ")
        
        success_count = 0
        failed_count = 0
        
        for pending_dir in pending_dirs:
            processing_dir = None
            try:
                # processingìœ¼ë¡œ ì´ë™
                processing_dir = self.staging_processing_path / pending_dir.name
                shutil.move(str(pending_dir), str(processing_dir))
                
                # ì²˜ë¦¬ ì‹¤íŒ¨ í”Œë˜ê·¸ ì´ˆê¸°í™”
                self.processing_failed = False
                self.error_messages = []
                
                # ì²˜ë¦¬
                self._process_single_directory(processing_dir)
                
                # ì„±ê³µ ì‹œ ì •ë¦¬
                shutil.rmtree(processing_dir)
                success_count += 1
                
                self.logger.info(f"âœ… ì™„ë£Œ: {pending_dir.name}")
                
            except Exception as e:
                failed_count += 1
                self.logger.error(f"âŒ ì‹¤íŒ¨: {pending_dir.name} - {e}")
                
                # ì‹¤íŒ¨ ì‹œ failedë¡œ ì´ë™
                if processing_dir and processing_dir.exists():
                    failed_dir = self.staging_failed_path / pending_dir.name
                    failed_dir.parent.mkdir(exist_ok=True)
                    try:
                        shutil.move(str(processing_dir), str(failed_dir))
                    except Exception as move_error:
                        self.logger.error(f"Failed ë””ë ‰í† ë¦¬ ì´ë™ ì‹¤íŒ¨: {move_error}")
                        if processing_dir.exists():
                            shutil.rmtree(processing_dir)
        
        result = {"success": success_count, "failed": failed_count}
        self.logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {result}")
        return result
    
    def _process_single_directory(self, processing_dir: Path):
        """ë‹¨ì¼ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ - datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©"""
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        metadata_file = processing_dir / "upload_metadata.json"
        if not metadata_file.exists():
            raise ValueError("ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        
        with open(metadata_file, encoding='utf-8') as f:
            metadata = json.load(f)
        
        # datasetsë¡œ ë¡œë“œ
        dataset_obj = load_from_disk(str(processing_dir))
        self.logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ: {len(dataset_obj)}í–‰")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ (Raw ë°ì´í„°ì¸ ê²½ìš°)
        if metadata.get('data_type') == 'raw' and metadata.get('has_images', False):
            dataset_obj = self._process_images_with_map(dataset_obj, metadata)
        
        # Catalogì— ì €ì¥
        self._save_to_catalog(dataset_obj, metadata)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del dataset_obj
        gc.collect()
    
    def _process_images_with_map(self, dataset_obj: Dataset, metadata: Dict) -> Dataset:
        """datasets.map()ì„ í™œìš©í•œ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ ì°¾ê¸°
        image_column = None
        for col in dataset_obj.column_names:
            if col.lower() in self.image_column_candidates:
                image_column = col
                break
        
        if not image_column:
            raise ValueError("ì´ë¯¸ì§€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        total_images = len(dataset_obj)
        self.logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘: {image_column} ({total_images}ê°œ)")
        
        # Assets ê²½ë¡œ ì„¤ì •
        provider = metadata['provider']
        dataset_name = metadata['dataset']
        self.assets_base = self.assets_path / f"provider={provider}" / f"dataset={dataset_name}"
        self.assets_base.mkdir(parents=True, exist_ok=True)
        
        self.shard_config = self._get_shard_config(total_images)
        self.logger.info(f"ğŸ”§ ìƒ¤ë”© ì„¤ì •: {self.shard_config['info']}")
        
        # í•´ì‹œ ìºì‹œ êµ¬ì¶•
        self._build_hash_cache(self.assets_base)
        
        # Image featureë¡œ ìºìŠ¤íŒ…
        dataset_obj = dataset_obj.cast_column(image_column, ImageFeature())
        
        # datasets.map()ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        try:
            processed_dataset = dataset_obj.map(
                self._process_image_batch,
                batched=True,
                batch_size=self.batch_size,
                num_proc=self.num_proc,
                remove_columns=[image_column],  # ì›ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì œê±°
                desc="ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬",
                load_from_cache_file=False,  # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            )
            
            # ì²˜ë¦¬ ì¤‘ ì‹¤íŒ¨ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
            if self.processing_failed:
                error_summary = f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {'; '.join(self.error_messages[:5])}"
                raise RuntimeError(error_summary)
                
            self.logger.info(f"âœ… ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {len(processed_dataset)}ê°œ")
            return processed_dataset
            
        except Exception as e:
            self.logger.error(f"âŒ datasets.map() ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _process_image_batch(self, batch: Dict) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (datasets.mapìš©)"""
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ ì´ë¦„ ì°¾ê¸°
        image_column = None
        for col in batch.keys():
            if col.lower() in self.image_column_candidates:
                image_column = col
                break
        
        if not image_column:
            raise ValueError("ì´ë¯¸ì§€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        images = batch[image_column]
        batch_size = len(images)
        
        # ê²°ê³¼ ì €ì¥ìš©
        image_hashes = []
        image_paths = []
        
        saved_count = 0
        duplicate_count = 0
        
        for idx, image_data in enumerate(images):
            try:
                # ì‹¤íŒ¨ í”Œë˜ê·¸ í™•ì¸
                if self.processing_failed:
                    break
                
                if image_data is None:
                    image_hashes.append(None)
                    image_paths.append(None)
                    continue
                
                # PIL Image ì²˜ë¦¬
                pil_image = image_data if hasattr(image_data, 'save') else Image.open(io.BytesIO(image_data))
                
                # í•´ì‹œ ê³„ì‚°
                image_hash = self._get_image_hash(pil_image)
                
                # ì¤‘ë³µ ì²´í¬
                if image_hash in self.existing_hashes:
                    duplicate_count += 1
                    image_path = self._get_image_path(self.assets_base, image_hash, self.shard_config)
                    relative_path = str(image_path.relative_to(self.assets_path))
                else:
                    # ìƒˆ ì´ë¯¸ì§€ ì €ì¥
                    image_path = self._get_image_path(self.assets_base, image_hash, self.shard_config)
                    image_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    if pil_image.mode != 'RGB':
                        pil_image = pil_image.convert('RGB')
                    pil_image.save(str(image_path), 'JPEG', quality=95)
                    
                    # ìºì‹œì— ì¶”ê°€ (thread-safe)
                    with self.cache_lock:
                        self.existing_hashes.add(image_hash)
                    
                    saved_count += 1
                    relative_path = str(image_path.relative_to(self.assets_path))
                
                image_hashes.append(image_hash)
                image_paths.append(relative_path)
                
            except Exception as e:
                # ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì‹¤íŒ¨ë¡œ ë§ˆí‚¹
                with self.failure_lock:
                    if not self.processing_failed:
                        self.processing_failed = True
                        error_msg = f"ì´ë¯¸ì§€ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                        self.error_messages.append(error_msg)
                        self.logger.error(f"âŒ {error_msg}")
                
                # ì‹¤íŒ¨ ì¦‰ì‹œ ì¤‘ë‹¨
                raise RuntimeError(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        # ë¡œê·¸ ì¶œë ¥ (ë°°ì¹˜ë³„)
        if saved_count > 0 or duplicate_count > 0:
            self.logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬: ì €ì¥={saved_count}, ì¤‘ë³µ={duplicate_count}")
        
        return {
            "image_hash": image_hashes,
            "image_path": image_paths
        }
    
    def _build_hash_cache(self, assets_base: Path):
        """ê¸°ì¡´ ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ êµ¬ì¶•"""
        if self.cache_built:
            return
            
        with self.cache_lock:
            if self.cache_built:
                return
            
            self.logger.info("ğŸ” ê¸°ì¡´ ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ êµ¬ì¶• ì¤‘...")
            start_time = time.time()
            
            # ëª¨ë“  .jpg íŒŒì¼ì—ì„œ í•´ì‹œ ì¶”ì¶œ
            for image_file in assets_base.rglob("*.jpg"):
                hash_from_filename = image_file.stem
                if len(hash_from_filename) == 64:  # SHA256 ê¸¸ì´ ê²€ì¦
                    self.existing_hashes.add(hash_from_filename)
            
            build_time = time.time() - start_time
            self.logger.info(f"âœ… í•´ì‹œ ìºì‹œ êµ¬ì¶• ì™„ë£Œ: {len(self.existing_hashes)}ê°œ ({build_time:.2f}ì´ˆ)")
            self.cache_built = True
            
    def _get_image_hash(self, pil_image: Image.Image) -> str:
        """ì´ë¯¸ì§€ í•´ì‹œ ê³„ì‚°"""
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        # JPEG ë³€í™˜ í›„ í•´ì‹œ
        img_buffer = io.BytesIO()
        pil_image.save(img_buffer, format='JPEG', quality=95)
        jpeg_bytes = img_buffer.getvalue()
        
        return hashlib.sha256(jpeg_bytes).hexdigest()
    
    def _get_shard_config(self, total_images: int) -> Dict:
        """ìƒ¤ë”© ì„¤ì •"""
        if total_images < 1000:
            return {"levels": 0, "info": "ìƒ¤ë”© ì—†ìŒ"}
        elif total_images < 50000:
            return {"levels": 1, "info": "1ë‹¨ê³„ ìƒ¤ë”© (xx/)"}
        else:
            return {"levels": 2, "info": "2ë‹¨ê³„ ìƒ¤ë”© (xx/xx/)"}

    def _get_image_path(self, base_path: Path, image_hash: str, shard_config: Dict) -> Path:
        """ìƒ¤ë”© ì„¤ì •ì— ë”°ë¥¸ ê²½ë¡œ"""
        levels = shard_config["levels"]
        if levels == 0:
            return base_path / f"{image_hash}.jpg"
        elif levels == 1:
            return base_path / image_hash[:2] / f"{image_hash}.jpg"
        elif levels == 2:  
            return base_path / image_hash[:2] / image_hash[2:4] / f"{image_hash}.jpg"
        else:
            raise ValueError(f"ì˜ëª»ëœ ìƒ¤ë”© ë ˆë²¨: {levels}")
    
    def _save_to_catalog(self, dataset_obj: Dataset, metadata: Dict):
        """Catalogì— ì €ì¥"""
        provider = metadata['provider']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        
        # Catalog ê²½ë¡œ
        catalog_dir = (
            self.catalog_path /
            f"provider={provider}" /
            f"dataset={dataset_name}" /
            f"task={task}" /
            f"variant={variant}"
        )
        catalog_dir.mkdir(parents=True, exist_ok=True)
        
        # Parquet ì €ì¥ (datasets ë‚´ì¥ ìµœì í™”)
        parquet_file = catalog_dir / "data.parquet"
        dataset_obj.to_parquet(str(parquet_file))
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_file = catalog_dir / "_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # íŒŒì¼ í¬ê¸° ë¡œê·¸
        file_size_mb = parquet_file.stat().st_size / (1024 * 1024)
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {parquet_file.name} ({file_size_mb:.1f}MB, {len(dataset_obj)}í–‰)")
        
if __name__ == "__main__":
    # datasets.map() í™œìš© ë²„ì „
    processor = NASDataProcessor(
        batch_size=1000,    # map()ì˜ ë°°ì¹˜ í¬ê¸°
        num_proc=4         # ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜
    )
    
    # ìƒíƒœ í™•ì¸
    status = processor.get_status()
    print(f"í˜„ì¬ ìƒíƒœ: {status}")
    
    # ì²˜ë¦¬ ì‹¤í–‰
    result = processor.process_all_pending()
    print(f"ì²˜ë¦¬ ê²°ê³¼: {result}")