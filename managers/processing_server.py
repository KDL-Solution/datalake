import concurrent.futures
import asyncio
import logging
import uvicorn
import sys
import os
from pathlib import Path
from typing import Dict, List
from datetime import datetime
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

sys.path.append(str(Path(__file__).resolve().parent.parent))  # ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€
from managers.nas_processor import NASDataProcessor

# Request/Response ëª¨ë¸ë“¤
class ProcessRequest(BaseModel):
    """ì²˜ë¦¬ ìš”ì²­ ëª¨ë¸"""
    pass  # body ì—†ì´ ëª¨ë“  pending ì²˜ë¦¬

class ProcessResponse(BaseModel):
    """ì²˜ë¦¬ ì‘ë‹µ ëª¨ë¸"""
    success: int
    failed: int
    message: str = ""

class StatusResponse(BaseModel):
    """ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""
    pending: int
    processing: int
    failed: int
    server_status: str
    last_updated: str

class ProcessingJob(BaseModel):
    """ì²˜ë¦¬ ì‘ì—… ìƒíƒœ"""
    job_id: str
    status: str  # "running", "completed", "failed"
    started_at: str
    completed_at: str = None
    result: Dict = None
    error: str = None


# ì „ì—­ ë³€ìˆ˜ë“¤
processor = None
logger = None
BASE_PATH = None
LOG_LEVEL = None
BATCH_SIZE = None
NUM_PROC = None
current_jobs: Dict[str, ProcessingJob] = {}
job_lock = asyncio.Lock()
@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    global processor, logger, BASE_PATH, LOG_LEVEL, BATCH_SIZE, NUM_PROC
    
    BASE_PATH = os.environ["BASE_PATH"]
    LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
    BATCH_SIZE = int(os.environ.get("BATCH_SIZE", 1000))
    NUM_PROC = int(os.environ.get("NUM_PROC", 4))    
    try:
        processor = NASDataProcessor(
            base_path=BASE_PATH,
            log_level=LOG_LEVEL,
            num_proc=NUM_PROC,
            batch_size=BATCH_SIZE,
        )
        logger = processor.logger
        logger.info("âœ… NASDataProcessor ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ Processor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logger.info("ğŸ”„ ì„œë²„ ì¢…ë£Œ ì¤‘...")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="NAS Data Processing API",
    description="NASì—ì„œ ë°ì´í„° ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” API ì„œë²„",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/test-async")
async def test_async():
    """ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸"""
    async def long_task():
        await asyncio.sleep(5)  # 5ì´ˆ ëŒ€ê¸°
        return "ì™„ë£Œ"
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
    asyncio.create_task(long_task())
    
    # ì¦‰ì‹œ ì‘ë‹µ
    return {"message": "ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ë¨", "timestamp": datetime.now().isoformat()}

@app.get("/status", response_model=StatusResponse)
async def get_status():
    """í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        if not processor:
            raise HTTPException(status_code=503, detail="Processor not initialized")
        
        status = processor.get_status()
        
        return StatusResponse(
            pending=status["pending"],
            processing=status["processing"],
            failed=status["failed"],
            server_status="running",
            last_updated=datetime.now().isoformat()
        )
    except Exception as e:
        logger.error(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/process", response_model=Dict)
async def process_pending_data(background_tasks: BackgroundTasks):
    """Pending ë°ì´í„° ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
    try:
        
        job_id = f"job_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:19]}"
        
        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…ì´ ìˆëŠ”ì§€ í™•ì¸
        async with job_lock:
            running_jobs = [job for job in current_jobs.values() if job.status == "running"]
            
            if running_jobs:
                return {
                    "job_id": running_jobs[0].job_id,
                    "status": "already_running",
                    "message": "ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤"
                }
        
            job = ProcessingJob(
                job_id=job_id,
                status="running",
                started_at=datetime.now().isoformat()
            )
            current_jobs[job_id] = job
        
        asyncio.create_task(run_processing_job(job_id))
        
        return {
            "job_id": job_id,
            "status": "started",
            "message": "ì²˜ë¦¬ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/jobs/{job_id}")
async def get_job_status(job_id: str):
    async with job_lock:
        job = current_jobs.get(job_id)
    if job is None:
        raise HTTPException(status_code=404, detail="Job not found")
    
    return {
        "job_id": job.job_id,
        "status": job.status,
        "started_at": job.started_at,
        "completed_at": job.completed_at,
        "result": job.result,
        "error": job.error
    }


@app.get("/jobs")
async def list_jobs():
    """ëª¨ë“  ì‘ì—… ëª©ë¡"""
    async with job_lock:
        return {
            "jobs": [
                {
                    "job_id": job.job_id,
                    "status": job.status,
                    "started_at": job.started_at,
                    "completed_at": job.completed_at
                }
                for job in current_jobs.values()
            ]
        }


@app.delete("/jobs/{job_id}")
async def delete_job(job_id: str):
    """ì‘ì—… ì‚­ì œ (ì™„ë£Œëœ ì‘ì—…ë§Œ)"""
    async with job_lock:
        if job_id not in current_jobs:
            raise HTTPException(status_code=404, detail="Job not found")
        
        job = current_jobs[job_id]
        if job.status == "running":
            raise HTTPException(status_code=400, detail="Cannot delete running job")
        
        del current_jobs[job_id]
        logger.info(f"âœ… ì‘ì—… {job_id} ì‚­ì œë¨")
        return {"message": f"Job {job_id} deleted"}


async def run_processing_job(job_id: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•  ì²˜ë¦¬ ì‘ì—…"""
    try:
        logger.info(f"ğŸ”„ ì²˜ë¦¬ ì‘ì—… ì‹œì‘: {job_id}")
        
        # ì„±ê³µ ì‹œ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            # ì‹¤ì œ ì²˜ë¦¬ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            result = await loop.run_in_executor(
                executor, 
                processor.process_all_pending
            )
        async with job_lock:
            if job_id in current_jobs:
                current_jobs[job_id].status = "completed"
                current_jobs[job_id].completed_at = datetime.now().isoformat()
                current_jobs[job_id].result = result
        logger.info(f"âœ… ì²˜ë¦¬ ì‘ì—… ì™„ë£Œ: {job_id}, ê²°ê³¼: {result}")
        
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
        error_msg = str(e)
        logger.error(f"âŒ ì²˜ë¦¬ ì‘ì—… ì‹¤íŒ¨: {job_id}, ì˜¤ë¥˜: {error_msg}")
        
        async with job_lock:
            if job_id in current_jobs:
                current_jobs[job_id].status = "failed"
                current_jobs[job_id].completed_at = datetime.now().isoformat()
                current_jobs[job_id].error = error_msg


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="NAS Data Processing API Server")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind to")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    parser.add_argument("--log-level", default="INFO", help="Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)")
    parser.add_argument("--base-path", default="/mnt/AI_NAS/datalake/migrate_test", help="Base path for NAS data")
    parser.add_argument("--num-proc", type=int, default=4, help="Number of processing threads")
    parser.add_argument("--batch-size", type=int, default=1000, help="Batch size for processing")
    
    args = parser.parse_args()
    os.environ["BASE_PATH"] = args.base_path
    os.environ["LOG_LEVEL"] = args.log_level
    os.environ["NUM_PROC"] = str(args.num_proc)
    print(f"ğŸš€ Starting NAS Data Processing API Server on {args.host}:{args.port}")

    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=args.workers,
        reload=False
    )