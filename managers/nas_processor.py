import logging
import json
import shutil
import hashlib
import io
import threading
import time
import os
import gc
from datetime import datetime
from tqdm import tqdm
from pathlib import Path
from typing import Dict, Optional, List
from PIL import Image
from datasets import Dataset, load_from_disk
from datasets.features import Image as ImageFeature
from functools import partial

from managers.logging_setup import setup_logging

class NASDataProcessor:
    
    def __init__(
        self,
        base_path: str = "/mnt/AI_NAS/datalake/migrate_test",
        log_level: str = "INFO",
        num_proc: int = 4,
        batch_size: int = 1000,  # map()ì˜ ë°°ì¹˜ í¬ê¸°
    ):
        # ê²½ë¡œ ì„¤ì •
        self.base_path = Path(base_path)
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        
        self.catalog_path = self.base_path / "catalog"
        self.assets_path = self.base_path / "assets"
        
        self.num_proc = num_proc
        self.batch_size = batch_size
        
        # LocalDataManagerì™€ ë™ì¼
        self.image_data_key = 'image'  # ê¸°ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‚¤
        self.file_path_key = 'file_path'  # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ í‚¤
        
        self._check_path_and_setup_logging(log_level)
        
        self.existing_hashes = set()
        self.cache_built = False
        self.cache_lock = threading.Lock()
        
        # ì²˜ë¦¬ ì‹¤íŒ¨ ì¶”ì ìš©
        self.processing_failed = False
        self.failure_lock = threading.Lock()
        self.error_messages = []
        
        self.logger.info(f"ğŸš€ NASDataProcessor ì´ˆê¸°í™” (ë³‘ë ¬: {self.num_proc}, ë°°ì¹˜: {batch_size})")
 
    def get_status(self) -> Dict:
        """ê°„ë‹¨í•œ ìƒíƒœ ì¡°íšŒ"""
        return {
            "pending": len(list(self.staging_pending_path.glob("*"))) if self.staging_pending_path.exists() else 0,
            "processing": len(list(self.staging_processing_path.glob("*"))) if self.staging_processing_path.exists() else 0,
            "failed": len(list(self.staging_failed_path.glob("*"))) if self.staging_failed_path.exists() else 0
        }
    
    def process_all_pending(self) -> Dict:
        """ëª¨ë“  Pending ë°ì´í„° ì²˜ë¦¬"""
        self.logger.info("ğŸ”„ Pending ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        if not self.staging_pending_path.exists():
            return {"success": 0, "failed": 0, "message": "Pending ë””ë ‰í† ë¦¬ ì—†ìŒ"}
        
        pending_dirs = [
            d for d in self.staging_pending_path.iterdir()
            if d.is_dir() and (d / "upload_metadata.json").exists()
        ]
        
        if not pending_dirs:
            return {"success": 0, "failed": 0, "message": "ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ"}
        
        self.logger.info(f"ğŸ“¦ ì²˜ë¦¬ ëŒ€ìƒ: {len(pending_dirs)}ê°œ")
        
        success_count = 0
        failed_count = 0
        
        for pending_dir in pending_dirs:
            processing_dir = None
            try:
                # processingìœ¼ë¡œ ì´ë™
                processing_dir = self.staging_processing_path / pending_dir.name
                shutil.move(str(pending_dir), str(processing_dir))
                
                # ì²˜ë¦¬ ì‹¤íŒ¨ í”Œë˜ê·¸ ì´ˆê¸°í™”
                self.processing_failed = False
                self.error_messages = []
                
                # ì²˜ë¦¬
                self._process_single_directory(processing_dir)
                
                # ì„±ê³µ ì‹œ ì •ë¦¬
                shutil.rmtree(processing_dir)
                success_count += 1
                
                self.logger.info(f"âœ… ì™„ë£Œ: {pending_dir.name}")
                
            except Exception as e:
                failed_count += 1
                self.logger.error(f"âŒ ì‹¤íŒ¨: {pending_dir.name} - {e}")
                
                if processing_dir and processing_dir.exists():
                    failed_dir = self.staging_failed_path / pending_dir.name
                    failed_dir.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
                    try:
                        shutil.move(str(processing_dir), str(failed_dir))
                    except Exception as move_error:
                        self.logger.error(f"Failed ë””ë ‰í† ë¦¬ ì´ë™ ì‹¤íŒ¨: {move_error}")
                        if processing_dir.exists():
                            shutil.rmtree(processing_dir)
        
        result = {"success": success_count, "failed": failed_count}
        return result
    
    def _check_path_and_setup_logging(self, log_level: str = "INFO"):
        
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
        }
        
        missing_paths = []
        for path_name, path_obj in required_paths.items():
            if not path_obj.exists():
                missing_paths.append(f"  - {path_name}: {path_obj}")
        
        if missing_paths:
            missing_list = '\n'.join(missing_paths)
            raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
        setup_logging(log_level=log_level, base_path=str(self.base_path))
        self.logger = logging.getLogger(__name__)
        self.logger.debug("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")
   
    def _process_single_directory(self, processing_dir: Path):
        """ë‹¨ì¼ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ - datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©"""
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        metadata_file = processing_dir / "upload_metadata.json"
        if not metadata_file.exists():
            raise ValueError("ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        
        with open(metadata_file, encoding='utf-8') as f:
            metadata = json.load(f)
        
        # datasetsë¡œ ë¡œë“œ
        dataset_obj = load_from_disk(str(processing_dir))
        self.logger.info(f"{processing_dir.name} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)}ê°œ í–‰")
        self.logger.debug(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {dataset_obj.column_names}")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ (Raw ë°ì´í„°ì¸ ê²½ìš°)
        if metadata.get('data_type') == 'raw':
            provider = metadata['provider']
            dataset_name = metadata['dataset']
            assets_base = self.assets_path / f"provider={provider}" / f"dataset={dataset_name}"
            # í•´ì‹œ ìºì‹œ êµ¬ì¶• (ê³µí†µ)
            self._build_hash_cache(assets_base)

            # ì´ë¯¸ì§€ ì²˜ë¦¬
            if metadata.get('has_images', False) and self.image_data_key in dataset_obj.column_names:
                assets_base.mkdir(mode=0o775, parents=True, exist_ok=True)
                dataset_obj = self._process_images_with_map(dataset_obj, metadata, assets_base)
            
            # íŒŒì¼ ì²˜ë¦¬
            if metadata.get('has_files', False) and self.file_path_key in dataset_obj.column_names:
                assets_base.mkdir(mode=0o775, parents=True, exist_ok=True)
                dataset_obj = self._process_files_with_map(dataset_obj, metadata, assets_base)
        
        # Catalogì— ì €ì¥
        self._save_to_catalog(dataset_obj, metadata)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del dataset_obj
        gc.collect()
    
    def _process_images_with_map(self, dataset_obj: Dataset, metadata: Dict, assets_base: Path) -> Dataset:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ (PIL Image/bytes â†’ hash.jpg)"""
        print(metadata)
        total_images = len(dataset_obj)
        self.logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘: {self.image_data_key} ({total_images}ê°œ)")

        shard_config = self._get_shard_config(total_images)
        self.logger.info(f"ğŸ”§ ìƒ¤ë”© ì„¤ì •: {shard_config['info']}")
        
        dataset_obj = dataset_obj.cast_column(self.image_data_key, ImageFeature())
        
        process_batch_func = partial(
            self._process_image_batch,
            assets_base=assets_base,
            shard_config=shard_config
        )

        try:
            processed_dataset = dataset_obj.map(
                process_batch_func,
                batched=True,
                batch_size=self.batch_size,
                num_proc=self.num_proc,
                remove_columns=[self.image_data_key],  # ì›ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì œê±°
                desc="ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬",
                load_from_cache_file=False,  # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            )
            self.logger.debug(f"ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì»¬ëŸ¼: {processed_dataset.column_names}")
            # ì²˜ë¦¬ ì¤‘ ì‹¤íŒ¨ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
            if self.processing_failed:
                error_summary = f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {'; '.join(self.error_messages[:5])}"
                raise RuntimeError(error_summary)
                
            self.logger.info(f"âœ… ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {len(processed_dataset)}ê°œ")
            return processed_dataset
            
        except Exception as e:
            self.logger.error(f"âŒ datasets.map() ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
        
        
    def _process_files_with_map(self, dataset_obj: Dataset, metadata: Dict, assets_base: Path) -> Dataset:
        """íŒŒì¼ ì²˜ë¦¬ (staging/assets â†’ final/assets + hash)"""
        print(metadata)
        total_files = len(dataset_obj)
        self.logger.info(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {self.file_path_key} ({total_files}ê°œ)")
        
        shard_config = self._get_shard_config(total_files)
        self.logger.info(f"ğŸ”§ ìƒ¤ë”© ì„¤ì •: {shard_config['info']}")
        process_batch_func = partial(
            self._process_file_batch,
            assets_base=assets_base,
            shard_config=shard_config,
        )
        
        try:
            processed_dataset = dataset_obj.map(
                process_batch_func,
                batched=True,
                batch_size=self.batch_size,
                num_proc=self.num_proc,
                remove_columns=[self.file_path_key],  # ì›ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ ì œê±°
                desc="ğŸ“„ íŒŒì¼ ì´ë™",
                load_from_cache_file=False,
            )
            
            self.logger.info(f"âœ… íŒŒì¼ ì´ë™ ì™„ë£Œ: {len(processed_dataset)}ê°œ")
            return processed_dataset
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _process_image_batch(self, batch: Dict, assets_base: Path, shard_config: Dict) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ ì´ë¯¸ì§€ ì²˜ë¦¬ (PIL Image/bytes â†’ hash.jpg)"""
        images = batch[self.image_data_key]
        self.logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬: {len(images)}ê°œ ì´ë¯¸ì§€")        
        image_hashes = []
        image_paths = []
        
        saved_count = 0
        duplicate_count = 0
        
        for idx, image_data in enumerate(images):
            try:
                # ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                if self.processing_failed:
                    break
                
                if image_data is None:
                    image_hashes.append(None)
                    image_paths.append(None)
                    continue
                
                # PIL Image ì²˜ë¦¬
                pil_image = image_data if hasattr(image_data, 'save') else Image.open(io.BytesIO(image_data))
                
                image_hash  = self._get_image_hash(pil_image)
                image_path = self._get_level_path(assets_base, shard_config, image_hash)
                if image_hash in self.existing_hashes:
                    duplicate_count += 1    
                else:
                    image_path.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
                    
                    if pil_image.mode != 'RGB':
                        pil_image = pil_image.convert('RGB')
                    pil_image.save(str(image_path), 'JPEG', quality=95)
                    
                    # ìºì‹œì— ì¶”ê°€ (thread-safe)
                    with self.cache_lock:
                        self.existing_hashes.add(image_hash)
                    
                    saved_count += 1
                
                relative_path = str(image_path.relative_to(self.assets_path))
                image_hashes.append(image_hash)
                image_paths.append(relative_path)
                
            except Exception as e:
                # ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì‹¤íŒ¨ë¡œ ë§ˆí‚¹
                with self.failure_lock:
                    if not self.processing_failed:
                        self.processing_failed = True
                        error_msg = f"ì´ë¯¸ì§€ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                        self.error_messages.append(error_msg)
                        self.logger.error(f"âŒ {error_msg}")
                
                # ì‹¤íŒ¨ ì¦‰ì‹œ ì¤‘ë‹¨
                raise RuntimeError(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        # ë¡œê·¸ ì¶œë ¥ (ë°°ì¹˜ë³„)
        if saved_count > 0 or duplicate_count > 0:
            self.logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬: ì €ì¥={saved_count}, ì¤‘ë³µ={duplicate_count}")
        
        return {
            "path": image_paths,
            "hash": image_hashes,
        }
        
    def _process_file_batch(self, batch: Dict, assets_base: Path, shard_config: Dict) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ íŒŒì¼ ì²˜ë¦¬ (staging/assets â†’ final/assets + hash)"""
        
        file_paths = batch[self.file_path_key]
        self.logger.debug(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬: {len(file_paths)}ê°œ")
        file_hashes = []
        new_file_paths = []
        
        saved_count = 0
        duplicate_count = 0
        # print iterdir staging_pending_path
        for idx, file_path in enumerate(file_paths):
            try:
                if self.processing_failed:
                    break
                
                if file_path is None:
                    file_hashes.append(None)
                    file_paths.append(None)
                    continue
                    
                file_path = self.staging_processing_path / file_path
                if not file_path.exists():
                    raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                
                file_hash = self._get_file_hash(file_path)
                new_file_path = self._get_level_path(assets_base, shard_config, file_hash)
                if file_hash in self.existing_hashes:
                    duplicate_count += 1
                else:
                    new_file_path.parent.mkdir(mode=0o775, parents=True, exist_ok=True)
                    shutil.move(str(file_path), str(new_file_path))
                    with self.cache_lock:
                        self.existing_hashes.add(file_hash)
                    saved_count += 1
                relative_path = str(new_file_path.relative_to(self.assets_path))
                file_hashes.append(file_hash)
                new_file_paths.append(relative_path)
                    
            except Exception as e:
                with self.failure_lock:
                    if not self.processing_failed:
                        self.processing_failed = True
                        error_msg = f"íŒŒì¼ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                        self.error_messages.append(error_msg)
                        self.logger.error(f"âŒ {error_msg}")
                
                # ì‹¤íŒ¨ ì¦‰ì‹œ ì¤‘ë‹¨
                raise RuntimeError(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        if saved_count > 0 or duplicate_count > 0:
            self.logger.debug(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬: ì €ì¥={saved_count}, ì¤‘ë³µ={duplicate_count}")

        return {
            "path": new_file_paths,
            "hash": file_hashes
        }
        
    def _build_hash_cache(self, assets_base: Path):
        """ê¸°ì¡´ ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ êµ¬ì¶•"""
        if self.cache_built:
            return
            
        with self.cache_lock:
            if self.cache_built:
                return
            start_time = time.time()
            
            # ëª¨ë“  .jpg íŒŒì¼ì—ì„œ í•´ì‹œ ì¶”ì¶œ
            for image_file in assets_base.rglob("*.jpg"):
                hash_from_filename = image_file.stem
                if len(hash_from_filename) == 64:  # SHA256 ê¸¸ì´ ê²€ì¦
                    self.existing_hashes.add(hash_from_filename)
            
            build_time = time.time() - start_time
            self.logger.info(f"ğŸ” ê¸°ì¡´ ì´ë¯¸ì§€ í•´ì‹œ ìºì‹œ êµ¬ì¶• ì™„ë£Œ: {len(self.existing_hashes)}ê°œ, ì‹œê°„: {build_time:.2f}ì´ˆ")
            self.cache_built = True
    
                
    def _get_image_hash(self, pil_image: Image.Image) -> str:
        """ì´ë¯¸ì§€ í•´ì‹œ ê³„ì‚°"""
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        img_buffer = io.BytesIO()
        pil_image.save(img_buffer, format='JPEG', quality=95)
        jpeg_bytes = img_buffer.getvalue()
        return hashlib.sha256(jpeg_bytes).hexdigest()

    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (SHA256)"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    def _get_shard_config(self, total_images: int) -> Dict:
        """ìƒ¤ë”© ì„¤ì •"""
        if total_images < 1000:
            return {"levels": 0, "info": "ìƒ¤ë”© ì—†ìŒ"}
        elif total_images < 50000:
            return {"levels": 1, "info": "1ë‹¨ê³„ ìƒ¤ë”© (xx/)"}
        else:
            return {"levels": 2, "info": "2ë‹¨ê³„ ìƒ¤ë”© (xx/xx/)"}

    def _get_level_path(self, base_path: Path, shard_config: Dict, image_hash: str) -> Path:
        """ìƒ¤ë”© ì„¤ì •ì— ë”°ë¥¸ ê²½ë¡œ"""
        levels = shard_config["levels"]
        if levels == 0:
            return base_path / f"{image_hash}.jpg"
        elif levels == 1:
            return base_path / image_hash[:2] / f"{image_hash}.jpg"
        elif levels == 2:  
            return base_path / image_hash[:2] / image_hash[2:4] / f"{image_hash}.jpg"
        else:
            raise ValueError(f"ì˜ëª»ëœ ìƒ¤ë”© ë ˆë²¨: {levels}")
    
    def _save_to_catalog(self, dataset_obj: Dataset, metadata: Dict):
        """Catalogì— ì €ì¥"""
        provider = metadata['provider']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        
        # Catalog ê²½ë¡œ
        catalog_dir = (
            self.catalog_path /
            f"provider={provider}" /
            f"dataset={dataset_name}" /
            f"task={task}" /
            f"variant={variant}"
        )
        catalog_dir.mkdir(mode=0o775, parents=True, exist_ok=True)
        
        # Parquet ì €ì¥ (datasets ë‚´ì¥ ìµœì í™”)
        parquet_file = catalog_dir / "data.parquet"
        dataset_obj.to_parquet(str(parquet_file))
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_file = catalog_dir / "_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # íŒŒì¼ í¬ê¸° ë¡œê·¸
        file_size_mb = parquet_file.stat().st_size / (1024 * 1024)
        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {parquet_file.name} ({file_size_mb:.1f}MB, {len(dataset_obj)}í–‰)")
        
if __name__ == "__main__":
    # datasets.map() í™œìš© ë²„ì „
    processor = NASDataProcessor(
        batch_size=1000,    # map()ì˜ ë°°ì¹˜ í¬ê¸°
        num_proc=4         # ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜
    )
    
    # ìƒíƒœ í™•ì¸
    status = processor.get_status()
    print(f"í˜„ì¬ ìƒíƒœ: {status}")
    
    # ì²˜ë¦¬ ì‹¤í–‰
    result = processor.process_all_pending()
    print(f"ì²˜ë¦¬ ê²°ê³¼: {result}")