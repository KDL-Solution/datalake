import logging
import os
import uuid
import json
import shutil
import pandas as pd
import requests 
import time 

from pathlib import Path
from datetime import datetime
from datasets import Dataset, load_from_disk
from datasets.features import Image as ImageFeature
from typing import Dict, Optional, List
from PIL import Image

import sys
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from managers.schema_manager import SchemaManager
from managers.logger import setup_logging

class LocalDataManager:
    def __init__(
        self, 
        base_path: str = "/mnt/AI_NAS/datalake/migrate_test",
        nas_api_url: str = "http://192.168.20.62:8000",
        log_level: str = "INFO",
        num_proc: int = 8, # ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        auto_process: bool = True, # NAS ìë™ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        polling_interval: int = 10, # NAS ìƒíƒœ ì¡°íšŒ ì£¼ê¸° (ì´ˆ)
        schema_manager: Optional[SchemaManager] = None,
    ):
        self.base_path = Path(base_path)
        self.nas_api_url = nas_api_url.rstrip('/')
        self.auto_process = auto_process
        self.polling_interval = polling_interval
        
        # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        self.catalog_path = self.base_path / "catalog"
        self.assets_path  = self.base_path / "assets"
        
        
        self.num_proc = num_proc
        self.image_data_candidates = ['image', 'image_bytes']
        self.image_data_key = 'image'  # ê¸°ë³¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‚¤
        self.file_path_candidates = ['image_path', 'file', 'file_path']
        self.file_path_key = 'file_path'  # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ í‚¤
        
        self._check_path_and_setup_logging(log_level)
        
        self._check_nas_api_connection()

        self.schema_manager = schema_manager if schema_manager else SchemaManager(
            base_path=self.base_path, 
            create_default=True
        )
      
    def upload_raw_data(
        self,
        data_file: str,
        provider: str,
        dataset: str,
        dataset_description: str = "", # ë°ì´í„°ì…‹ ì„¤ëª…
        original_source: str = "", # ì›ë³¸ ì†ŒìŠ¤ URL 
    ):
        task = "raw"
        
        self.logger.info(f"ğŸ“¥ Raw data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}")
        
        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        self._cleanup_existing_pending(provider, dataset, task, is_raw=True)
        
        dataset_obj, file_info = self._load_data(data_file, process_assets=True)
        
        variant = file_info['variant']
        has_images = bool(file_info['image_columns'])
        has_files = bool(file_info['file_columns'])
        
        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=variant,
            total_rows=len(dataset_obj),
            data_type="raw",
            source_task=None,  # ì›ë³¸ ì‘ì—…ì´ë¯€ë¡œ None
            has_images=has_images,
            has_files=has_files,
            dataset_description=dataset_description,
            original_source=original_source,
        )
        
        staging_dir = self._save_to_staging(dataset_obj, metadata, file_info)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        job_id = None
        if self.auto_process:
            job_id = self.trigger_nas_processing()
            if job_id:
                self.logger.info(f"ğŸ”„ ìë™ ì²˜ë¦¬ ì‹œì‘ë¨: {job_id}")
        
        return staging_dir, job_id

    def upload_task_data(
        self,
        data_file: str,
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        dataset_description: str = "",
        source_task: str = None,
        **kwargs
    ) -> str:
        """Task ë°ì´í„° ì—…ë¡œë“œ (ê¸°ì¡´ catalogì—ì„œ íŠ¹ì • task ì¶”ì¶œ, ì´ë¯¸ì§€ ì°¸ì¡°ë§Œ)"""
        self.logger.info(f"ğŸ“¥ Task data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}/{task}/{variant}")
        
        # 1. Provider ê²€ì¦
        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        # 2. Task ë©”íƒ€ë°ì´í„° ê²€ì¦
        is_valid, error_msg = self.schema_manager.validate_task_metadata(task, kwargs)
        if not is_valid:
            raise ValueError(f"âŒ Task ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
        
        # ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬
        self._cleanup_existing_pending(provider, dataset, task, variant=variant, is_raw=False)
        
        # ë°ì´í„° ë¡œë“œ ë° ì»¬ëŸ¼ ë³€í™˜ (ì´ë¯¸ì§€ ì œì™¸)
        dataset_obj, _ = self._load_data(data_file, process_assets=False)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=variant,
            dataset_description=dataset_description,
            source_task=source_task,
            has_images=False,  # ì´ë¯¸ì§€ëŠ” ì°¸ì¡°ë§Œ
            has_files=False,  # íŒŒì¼ ê²½ë¡œëŠ” ì—†ìŒ
            total_rows=len(dataset_obj),
            data_type='task',
            **kwargs
        )
        
        # Stagingì— ì €ì¥
        staging_dir = self._save_to_staging(dataset_obj, metadata)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        job_id = None
        if self.auto_process:
            job_id = self.trigger_nas_processing()
            if job_id:
                self.logger.info(f"ğŸ”„ ìë™ ì²˜ë¦¬ ì‹œì‘ë¨: {job_id}")
        
        return staging_dir, job_id
    
    def get_nas_status(self) -> Optional[Dict]:
        """NAS ì„œë²„ ìƒíƒœ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/status", timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def list_nas_jobs(self) -> Optional[List[Dict]]:
        """ëª¨ë“  ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/jobs", timeout=10)
            if response.status_code == 200:
                return response.json().get('jobs', [])
            else:
                self.logger.error(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def show_nas_dashboard(self):
        """NAS ìƒíƒœ ëŒ€ì‹œë³´ë“œ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š NAS Data Processing Dashboard")
        print("="*60)
        
        # ìƒíƒœ ì¡°íšŒ
        status = self.get_nas_status()
        if status:
            print(f"ğŸ“¦ Pending: {status['pending']}ê°œ")
            print(f"ğŸ”„ Processing: {status['processing']}ê°œ")
            print(f"âŒ Failed: {status['failed']}ê°œ")
            print(f"ğŸ–¥ï¸ Server Status: {status['server_status']}")
            print(f"â° Last Updated: {status['last_updated']}")
        else:
            print("âŒ NAS ì„œë²„ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨")
        
        # ì‘ì—… ëª©ë¡
        jobs = self.list_nas_jobs()
        if jobs:
            print(f"\nğŸ“‹ Recent Jobs ({len(jobs)}ê°œ):")
            for job in jobs[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(job['status'], "â“")
                print(f"  {status_emoji} {job['job_id']} - {job['status']} ({job['started_at']})")
        
        print("="*60 + "\n")
        
    def trigger_nas_processing(self) -> Optional[str]:
        """NASì—ì„œ ì²˜ë¦¬ ì‹œì‘"""
        self.logger.info("ğŸ”„ NAS ì²˜ë¦¬ ìš”ì²­ ì¤‘...")
        start_time = time.time()
        try:
            response = requests.post(
                f"{self.nas_api_url}/process", 
                timeout=30,
                headers={'Content-Type': 'application/json'}
            )
            elapsed = time.time() - start_time
            self.logger.debug(f"â±ï¸ NAS ì²˜ë¦¬ ìš”ì²­ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            if response.status_code == 200:
                result = response.json()
                job_id = result.get('job_id')
                status = result.get('status')
                message = result.get('message', '')
                
                if status == 'already_running':
                    self.logger.info("ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤")
                    return job_id
                elif status == 'started':
                    self.logger.info(f"âœ… ì²˜ë¦¬ ì‘ì—… ì‹œì‘ë¨: {job_id}")
                    return job_id
                else:
                    self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ: {status}, ë©”ì‹œì§€: {message}")
                    return job_id
            else:                    
                self.logger.error(f"âŒ ì²˜ë¦¬ ì‹œì‘ ì‹¤íŒ¨: {response.status_code}")
                try:
                    error_detail = response.json().get('detail', response.text)
                    self.logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {error_detail}")
                except:
                    self.logger.error(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
                return None
            
        except requests.exceptions.Timeout:
            elapsed = time.time() - start_time
            self.logger.error(f"âŒ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ({elapsed:.2f}ì´ˆ)")
            return None
        except requests.exceptions.ConnectionError:
            self.logger.error(f"âŒ NAS ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {self.nas_api_url}")
            return None
        except requests.exceptions.RequestException as e:
            elapsed = time.time() - start_time
            self.logger.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
            return None
    
    def get_job_status(self, job_id: str) -> Optional[Dict]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/jobs/{job_id}", timeout=10)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 404:
                self.logger.warning(f"âš ï¸ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {job_id}")
                return None
            else:
                self.logger.error(f"âŒ ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def wait_for_job_completion(self, job_id: str, timeout: int = 3600) -> Dict:
        """ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (í´ë§)"""
        self.logger.info(f"â³ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì¤‘: {job_id}")
        
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            job_status = self.get_job_status(job_id)
            if not job_status:
                raise RuntimeError(f"ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {job_id}")
            
            status = job_status.get('status')
            
            if status == 'completed':
                result = job_status.get('result', {})
                self.logger.info(f"âœ… ì‘ì—… ì™„ë£Œ: {job_id}")
                self.logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì„±ê³µ={result.get('success', 0)}, ì‹¤íŒ¨={result.get('failed', 0)}")
                return job_status
                
            elif status == 'failed':
                error = job_status.get('error', 'Unknown error')
                self.logger.error(f"âŒ ì‘ì—… ì‹¤íŒ¨: {job_id}, ì˜¤ë¥˜: {error}")
                raise RuntimeError(f"ì‘ì—… ì‹¤íŒ¨: {error}")
                
            elif status == 'running':
                self.logger.debug(f"ğŸ”„ ì‘ì—… ì§„í–‰ ì¤‘: {job_id}")
                time.sleep(self.polling_interval)
            else:
                self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… ìƒíƒœ: {status}")
                time.sleep(self.pooling_interval)
        
        raise TimeoutError(f"ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {job_id}")    
        
    def _check_nas_api_connection(self):
        """NAS API ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(f"{self.nas_api_url}/health", timeout=5)
            if response.status_code == 200:
                self.logger.info(f"âœ… NAS API ì„œë²„ ì—°ê²° í™•ì¸: {self.nas_api_url}")
            else:
                self.logger.warning(f"âš ï¸ NAS API ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"âš ï¸ NAS API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            self.logger.warning("ğŸ”„ ë¡œì»¬ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤ (ìë™ ì²˜ë¦¬ ë¹„í™œì„±í™”)")
            self.auto_process = False
    
    def _create_metadata(
        self,
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        total_rows: int,
        data_type: str,
        source_task: Optional[str],
        has_images: bool = False,
        has_files: bool = False,
        dataset_description: str = "",
        original_source: str = "",
        **kwargs
    ) -> Dict:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata = {
            'provider': provider,
            'dataset': dataset,
            'task': task,
            'variant': variant,
            'data_type': data_type,
            'dataset_description': dataset_description,
            'original_source': original_source,
            'source_task': source_task,
            'has_images': has_images,
            'has_files': has_files,
            'total_rows': total_rows,
            'uploaded_by': os.getenv('USER', 'unknown'),
            'uploaded_at': datetime.now().isoformat(),
            'file_id': str(uuid.uuid4())[:8],
        }
        metadata.update(kwargs) # taskì˜ ì¶”ê°€ í•„ë“œ
        self.logger.debug(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {metadata}")
        return metadata

    def _cleanup_existing_pending(
        self, 
        provider: str, 
        dataset: str, 
        task: str, 
        variant: str = None, 
        is_raw: bool = True,
    ):
        """ê°™ì€ provider/dataset/task ì¡°í•©ì˜ ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬"""
        pending_path = self.staging_path / "pending"
        
        if not pending_path.exists():
            return
        
        existing_dirs = []
        
        for pending_dir in pending_path.iterdir():
            if not pending_dir.is_dir():
                continue
                
            # ë©”íƒ€ë°ì´í„°ë¡œ ì •í™•íˆ í™•ì¸
            try:
                metadata_file = pending_dir / "upload_metadata.json"
                if metadata_file.exists():
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    basic_match = (
                        metadata.get('provider') == provider and 
                        metadata.get('dataset') == dataset and 
                        metadata.get('task') == task
                    )
                    
                    should_cleanup = False
                    if is_raw:
                        should_cleanup = basic_match
                    else:
                        should_cleanup = basic_match and metadata.get('variant') == variant
                        
                    if should_cleanup:
                        existing_dirs.append(pending_dir)
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {pending_dir} - {e}")
                continue
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        if existing_dirs:
            self.logger.info(f"ğŸ—‘ï¸  ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬: {len(existing_dirs)}ê°œ ë°œê²¬")
            self.logger.debug("ì‚­ì œí•  ë””ë ‰í† ë¦¬ ëª©ë¡:")
            self.logger.debug("\n".join(str(d) for d in existing_dirs))
            
            try:
                response = input("\nğŸ—‘ï¸  ìœ„ pending ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if response not in ['y', 'yes']:
                    self.logger.info("âŒ ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                    raise ValueError("ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            except KeyboardInterrupt:
                self.logger.info("âŒ ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                raise ValueError("ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                
            for existing_dir in existing_dirs:
                try:
                    shutil.rmtree(existing_dir)
                    self.logger.info(f"ğŸ—‘ï¸ ì‚­ì œ ì™„ë£Œ: {existing_dir.name}")
                except Exception as e:
                    self.logger.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {existing_dir.name} - {e}")
            
            self.logger.info(f"âœ… ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(existing_dirs)}ê°œ ì‚­ì œ")
        else:
            self.logger.debug("ğŸ“­ ì •ë¦¬í•  ê¸°ì¡´ pending ë°ì´í„° ì—†ìŒ")
    
    def _load_data(self, data_file: str, process_assets: bool = False) -> Dataset:
        """ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ"""
        data_path = Path(data_file).resolve()
        if not data_path.exists():
            raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        
        self.logger.info(f"ğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {data_path}")   
        
        if data_path.is_dir():
            try:
                dataset_obj = load_from_disk(str(data_path))
                self.logger.info(f"âœ… datasets í´ë” ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ datasets í´ë” ë¡œë“œ ì‹¤íŒ¨: {e}")   
        elif data_path.suffix == '.parquet':
            try:
                df = pd.read_parquet(data_path)
                dataset_obj = Dataset.from_pandas(df)
                self.logger.info(f"âœ… Parquet íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ Parquet íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {data_path.suffix}")
        
        self.logger.info(f"âœ… ë°ì´í„° íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {data_file}")
        
        column_names = dataset_obj.column_names
        self.logger.info(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {column_names}")
                
       # í†µí•©ëœ ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ì²˜ë¦¬ (JSON dumps + ì´ë¯¸ì§€)
        dataset_obj = self._process_cast_columns(dataset_obj)
        
        if process_assets:
            # íŒŒì¼ ì»¬ëŸ¼ ë¶„ì„ ë° variant ê²°ì •
            file_info = self._detect_file_columns_and_variant(dataset_obj)
            dataset_obj = self._normalize_column_names(dataset_obj, file_info)

            self.logger.info(f"ğŸ“„ íŒŒì¼ ë¶„ì„ ê²°ê³¼: variant={file_info['variant']}, "
                           f"ì´ë¯¸ì§€ì»¬ëŸ¼={file_info['image_columns']}, "
                           f"íŒŒì¼ì»¬ëŸ¼={file_info['file_columns']}, "
                           f"í™•ì¥ì={file_info['extensions']}")
        else:
            file_info = {'image_columns': [], 'file_columns': [], 'variant': 'text', 'extensions': set()}
            self.logger.debug("ğŸ“„ Assets ì»¬ëŸ¼ ì²˜ë¦¬ ìƒëµ")
        
        return dataset_obj, file_info
    
    def _detect_file_columns_and_variant(self, dataset_obj: Dataset) -> Dict:
        """íŒŒì¼ ì»¬ëŸ¼ë“¤ì„ ì°¾ê³  í™•ì¥ì ê¸°ë°˜ìœ¼ë¡œ variant ê²°ì •"""
        result = {
            'image_columns': [],
            'file_columns': [],
            'extensions': set(),
            'variant': 'text'
        }
        for key in dataset_obj.column_names:
            sample_value = dataset_obj[0][key]
            
            # PIL Imageë‚˜ bytes ë°ì´í„°ì¸ ê²½ìš°
            if key in self.image_data_candidates:
                if hasattr(sample_value, 'save') or isinstance(sample_value, bytes):
                    result['image_columns'].append(key)
                    continue
            
            # ê²½ë¡œ ê¸°ë°˜ íŒŒì¼ì¸ ê²½ìš°
            if key in self.file_path_candidates:
                if isinstance(sample_value, str) and Path(sample_value).exists():
                    ext = Path(sample_value).suffix.lower()
                    result['extensions'].add(ext)
                    result['file_columns'].append(key)
        
        if len(result['image_columns']) > 1:
            raise ValueError(f"âŒ ì´ë¯¸ì§€ ì»¬ëŸ¼ì´ 2ê°œ ì´ìƒì…ë‹ˆë‹¤: {result['image_columns']}. "
                             f"í•˜ë‚˜ì˜ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        if len(result['file_columns']) > 1:
            raise ValueError(f"âŒ íŒŒì¼ ì»¬ëŸ¼ì´ 2ê°œ ì´ìƒì…ë‹ˆë‹¤: {result['file_columns']}. "
                             f"í•˜ë‚˜ì˜ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
            
        result['image_columns'] = result['image_columns'][:1]
        result['file_columns'] = result['file_columns'][:1]
        # variant ê²°ì •
        has_image_data = bool(result['image_columns'])
        has_file_paths = bool(result['file_columns'])
        extensions = result['extensions']
        
        if has_image_data and has_file_paths:
            result['variant'] = "mixed"
        elif has_image_data:
            result['variant'] = "image"
        elif has_file_paths:
            # í™•ì¥ì ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì  ë¶„ë¥˜
            if any(ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'] for ext in extensions):
                result['variant'] = "image"
            else:
                result['variant'] = "files"
        else:
            result['variant'] = "text"
        
        return result

    def _normalize_column_names(self, dataset_obj: Dataset, file_info: Dict) -> Dataset:
        """ì»¬ëŸ¼ëª…ì„ í‘œì¤€í™” (image_columns â†’ image, file_columns â†’ file_path)"""
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ í‘œì¤€í™”
        if len(file_info['image_columns']):
            image_col = file_info['image_columns'][0]
            self.logger.info(f"ğŸ”„ ì´ë¯¸ì§€ ì»¬ëŸ¼ í‘œì¤€í™”: {image_col} â†’ {self.image_data_key}")
            
            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ì„ í‘œì¤€ ì»¬ëŸ¼ìœ¼ë¡œ ì‚¬ìš©
            
            if image_col != self.image_data_key:
                # ì»¬ëŸ¼ëª… ë³€ê²½
                dataset_obj = dataset_obj.rename_column(image_col, self.image_data_key)
        
        # íŒŒì¼ ì»¬ëŸ¼ í‘œì¤€í™”
        if len(file_info['file_columns']):
            file_col = file_info['file_columns'][0]
            self.logger.info(f"ğŸ”„ íŒŒì¼ ì»¬ëŸ¼ í‘œì¤€í™”: {file_col} â†’ {self.file_path_key}")
            
            
            # ì»¬ëŸ¼ëª… ë³€ê²½ (í•„ìš”í•œ ê²½ìš°)
            if file_col != self.file_path_key:
                dataset_obj = dataset_obj.rename_column(file_col, self.file_path_key)
        return dataset_obj
    
    def _process_cast_columns(self, dataset_obj: Dataset):
        
        self.logger.info("ğŸ” JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ê²€ì‚¬ ì‹œì‘")
        
        json_cast_columns = []
        
        for key in dataset_obj.column_names:
            sample_value = dataset_obj[0][key]
            
            if isinstance(sample_value, (dict, list)):
                json_cast_columns.append(key)
                self.logger.info(f"ğŸ“ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ë°œê²¬: '{key}' (íƒ€ì…: {type(sample_value).__name__})")
        
        # JSON dumps ì²˜ë¦¬
        if json_cast_columns:
            dataset_obj = self._apply_json_transform(dataset_obj, json_cast_columns)
        else:
            self.logger.info("ğŸ“„ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ì—†ìŒ")
        
        return dataset_obj
    
    def _apply_json_transform(self, dataset_obj: Dataset, json_cast_columns: list) -> Dataset:
        """JSON ë³€í™˜ ì ìš©"""
        self.logger.info(f"ğŸ”„ {len(json_cast_columns)}ê°œ ì»¬ëŸ¼ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘: {json_cast_columns}")
        
        def json_transform(x):
            if isinstance(x, (dict, list)):
                return json.dumps(x, ensure_ascii=False)
            return x
        
        try:
            dataset_obj = dataset_obj.map(
                lambda x: {col: json_transform(x[col]) for col in json_cast_columns},
                num_proc=self.num_proc,
                desc="JSON ë³€í™˜ ì¤‘",
            )
            self.logger.info(f"âœ… JSON ë³€í™˜ ì™„ë£Œ: {json_cast_columns}")
            return dataset_obj
        except Exception as e:
            self.logger.error(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise ValueError(f"âŒ JSON ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _save_to_staging(self, dataset_obj: Dataset, metadata: dict, file_info: Optional[Dict] = None) -> str:
        """ë°ì´í„°ë¥¼ staging í´ë”ì— ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        file_id = metadata['file_id']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        user = metadata['uploaded_by']
        
        staging_dirname = f"{dataset_name}_{task}_{variant}_{file_id}_{timestamp}_{user}"
        staging_dir= self.staging_path / "pending" / staging_dirname
        try:
            if metadata.get('data_type') == 'raw' and file_info:
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                staging_assets_dir = staging_dir / "assets"
            
                if len(file_info['file_columns']):
                    dataset_obj  = self._copy_file_path_to_staging(
                        dataset_obj, staging_assets_dir
                    )
                
            if metadata.get('data_type') == 'task':
                dataset_obj = self._add_metadata_columns(dataset_obj, metadata)
                
            dataset_obj.save_to_disk(str(staging_dir))
            
            metadata_file = staging_dir / "upload_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=4)
                
            self.logger.info(f"ğŸ“¦ datasets ì €ì¥ ì™„ë£Œ: {staging_dir}")
            return str(staging_dir)
        except Exception as e:
            if staging_dir.exists():
                shutil.rmtree(staging_dir)
            raise 
    
    def _copy_file_path_to_staging(self, dataset_obj: Dataset, staging_assets_dir: Path):
        """íŒŒì¼ ê²½ë¡œë¥¼ stagingìœ¼ë¡œ ë³µì‚¬"""
        sample_value = dataset_obj[0][self.file_path_key]
        
        if isinstance(sample_value, str) and Path(sample_value).exists():
            def copy_file(example, idx):
                original_path = Path(example[self.file_path_key]).resolve()
                if original_path.exists():
                    ext = original_path.suffix or ""
                    prefix = "file"
                    new_filename = f"{prefix}_{idx:06d}{ext}"
                    target_path = staging_assets_dir / new_filename
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    shutil.copy2(original_path, target_path)
                    
                    example[self.file_path_key] = f"assets/{new_filename}"
                    
                return example
            
            dataset_obj = dataset_obj.map(copy_file, with_indices=True)
            return dataset_obj
        else:
            self.logger.warning(f"âš ï¸ íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ '{self.file_path_key}'ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {sample_value}")
            raise ValueError(f"íŒŒì¼ ê²½ë¡œ ì»¬ëŸ¼ '{self.file_path_key}'ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    def _check_path_and_setup_logging(self, log_level: str):
        
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
        }
        
        missing_paths = []
        for path_name, path_obj in required_paths.items():
            if not path_obj.exists():
                missing_paths.append(f"  - {path_name}: {path_obj}")
        
        if missing_paths:
            missing_list = '\n'.join(missing_paths)
            raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
        setup_logging(log_level=log_level, base_path=str(self.base_path))
        self.logger = logging.getLogger(__name__)
        self.logger.debug("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")
        
    def _add_metadata_columns(self, dataset_obj: Dataset, metadata: Dict):
        """Task ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€"""
        self.logger.info("ğŸ“ Task ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€ ì¤‘")
        
        required_fields = self.schema_manager.get_required_fields(metadata['task'])
            
        # ë°ì´í„°ì…‹ ê¸¸ì´
        num_rows = len(dataset_obj)
        
        # í•„ìˆ˜ í•„ë“œë“¤ë§Œ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
        added_columns = []
        
        values = []
        for field in required_fields:
            value = metadata.get(field)
            if value is None:
                self.logger.warning(f"âš ï¸ í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ë©”íƒ€ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            values.append(value)
        for value in values:
            column_data = [metadata[field]] * num_rows
            dataset_obj = dataset_obj.add_column(field, column_data)
            added_columns.append(f"{field}={metadata[field]}")
            self.logger.debug(f"ğŸ“ ì»¬ëŸ¼ ì¶”ê°€: {field} = {metadata[field]}")
        
        if added_columns:
            self.logger.info(f"âœ… í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ: {', '.join(added_columns)}")
        else:
            self.logger.info("ğŸ“ ì¶”ê°€í•  í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì—†ìŒ")
            
        return dataset_obj
    
            
if __name__ == "__main__":
    manager = LocalDataManager(
        log_level="DEBUG",
        )
    
    manager.show_nas_dashboard()
    manager.show_schema_info()
    
    manager.add_provider("example_provider")
    manager.remove_provider("example_provider")
    manager.add_provider("example_provider")
    manager.add_task("example_task", required_fields=["field1", "field2"], allowed_values={"field1": ["value1", "value2"]})
    staging_dir, job_id = manager.upload_raw_data(
        data_file="/home/kai/workspace/DeepDocs_Project/datalake/managers/sample_data_1",
        provider="example_provider",
        dataset="example_dataset",
        dataset_description="This is a sample dataset for testing.",
        original_source="https://example.com/original_source"
    )
    
    if job_id:
        try:
            job_status = manager.wait_for_job_completion(job_id, timeout=600)
            print(f"ì‘ì—… ì™„ë£Œ: {job_status}")
        except Exception as e:
            print(f"ì‘ì—… ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    
    
    
