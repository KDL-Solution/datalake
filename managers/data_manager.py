import logging
import os
import uuid
import json
import shutil
import pandas as pd
import requests 
import time 

from pathlib import Path
from datetime import datetime
from datasets import Dataset, load_from_disk
from datasets.features import Image as ImageFeature
from typing import Dict, Optional, List
from PIL import Image

import sys
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from managers.schema_manager import SchemaManager

class LocalDataManager:
    def __init__(
        self, 
        base_path: str = "/mnt/AI_NAS/datalake/migrate_test",
        nas_api_url: str = "http://192.168.20.62:8000",
        log_level: str = "INFO",
        num_proc: int = 8, # ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        auto_process: bool = True, # NAS ìë™ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        polling_interval: int = 10, # NAS ìƒíƒœ ì¡°íšŒ ì£¼ê¸° (ì´ˆ)
    ):
        self.base_path = Path(base_path)
        self.nas_api_url = nas_api_url.rstrip('/')
        self.auto_process = auto_process
        self.polling_interval = polling_interval
        
        # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        self.catalog_path = self.base_path / "catalog"
        self.assets_path  = self.base_path / "assets"
        self.schema_path = self.base_path / "config" / "schema.yaml"
        
        self.num_proc = num_proc
        self.image_column_candidates = ['image', 'image_bytes']
        
        
        self._setup_console_logging(log_level)
        self._check_path_and_setup_logging()
        
        self._check_nas_api_connection()

        self.schema_manager = SchemaManager(
            config_path=self.schema_path,
            create_default=True
        )
      
    def upload_raw_data(
        self,
        data_file: str,
        provider: str,
        dataset: str,
        dataset_description: str = "", # ë°ì´í„°ì…‹ ì„¤ëª…
        original_source: str = "", # ì›ë³¸ ì†ŒìŠ¤ URL 
    ):
        task = "raw"
        variant = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        self.logger.info(f"ğŸ“¥ Raw data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}")
        
        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        self._cleanup_existing_pending(provider, dataset, task, is_raw=True)
        
        dataset_obj, has_images = self._load_data(data_file, process_images=True)
        
        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=variant,
            total_rows=len(dataset_obj),
            data_type="raw",
            source_task=None,  # ì›ë³¸ ì‘ì—…ì´ë¯€ë¡œ None
            has_images=has_images,
            dataset_description=dataset_description,
            original_source=original_source,
        )
        
        staging_dir = self._save_to_staging(dataset_obj, metadata)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        job_id = None
        if self.auto_process:
            job_id = self.trigger_nas_processing()
            if job_id:
                self.logger.info(f"ğŸ”„ ìë™ ì²˜ë¦¬ ì‹œì‘ë¨: {job_id}")
        
        return staging_dir, job_id

    def upload_task_data(
        self,
        data_file: str,
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        dataset_description: str = "",
        source_task: str = None,
        **kwargs
    ) -> str:
        """Task ë°ì´í„° ì—…ë¡œë“œ (ê¸°ì¡´ catalogì—ì„œ íŠ¹ì • task ì¶”ì¶œ, ì´ë¯¸ì§€ ì°¸ì¡°ë§Œ)"""
        self.logger.info(f"ğŸ“¥ Task data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}/{task}/{variant}")
        
        # 1. Provider ê²€ì¦
        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        # 2. Task ë©”íƒ€ë°ì´í„° ê²€ì¦
        is_valid, error_msg = self.schema_manager.validate_task_metadata(task, kwargs)
        if not is_valid:
            raise ValueError(f"âŒ Task ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
        
        # ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬
        self._cleanup_existing_pending(provider, dataset, task, variant=variant, is_raw=False)
        
        # ë°ì´í„° ë¡œë“œ ë° ì»¬ëŸ¼ ë³€í™˜ (ì´ë¯¸ì§€ ì œì™¸)
        dataset_obj, _ = self._load_data(data_file, process_images=False)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=variant,
            dataset_description=dataset_description,
            source_task=source_task,
            has_images=False,  # ì´ë¯¸ì§€ëŠ” ì°¸ì¡°ë§Œ
            total_rows=len(dataset_obj),
            data_type='task',
            **kwargs
        )
        
        # Stagingì— ì €ì¥
        staging_dir = self._save_to_staging(dataset_obj, metadata)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        job_id = None
        if self.auto_process:
            job_id = self.trigger_nas_processing()
            if job_id:
                self.logger.info(f"ğŸ”„ ìë™ ì²˜ë¦¬ ì‹œì‘ë¨: {job_id}")
        
        return staging_dir, job_id
    
    def get_nas_status(self) -> Optional[Dict]:
        """NAS ì„œë²„ ìƒíƒœ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/status", timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def list_nas_jobs(self) -> Optional[List[Dict]]:
        """ëª¨ë“  ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/jobs", timeout=10)
            if response.status_code == 200:
                return response.json().get('jobs', [])
            else:
                self.logger.error(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def show_nas_dashboard(self):
        """NAS ìƒíƒœ ëŒ€ì‹œë³´ë“œ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š NAS Data Processing Dashboard")
        print("="*60)
        
        # ìƒíƒœ ì¡°íšŒ
        status = self.get_nas_status()
        if status:
            print(f"ğŸ“¦ Pending: {status['pending']}ê°œ")
            print(f"ğŸ”„ Processing: {status['processing']}ê°œ")
            print(f"âŒ Failed: {status['failed']}ê°œ")
            print(f"ğŸ–¥ï¸ Server Status: {status['server_status']}")
            print(f"â° Last Updated: {status['last_updated']}")
        else:
            print("âŒ NAS ì„œë²„ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨")
        
        # ì‘ì—… ëª©ë¡
        jobs = self.list_nas_jobs()
        if jobs:
            print(f"\nğŸ“‹ Recent Jobs ({len(jobs)}ê°œ):")
            for job in jobs[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                status_emoji = {"running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(job['status'], "â“")
                print(f"  {status_emoji} {job['job_id']} - {job['status']} ({job['started_at']})")
        
        print("="*60 + "\n")
        
    def trigger_nas_processing(self) -> Optional[str]:
        """NASì—ì„œ ì²˜ë¦¬ ì‹œì‘"""
        self.logger.info("ğŸ”„ NAS ì²˜ë¦¬ ìš”ì²­ ì¤‘...")
        start_time = time.time()
        try:
            response = requests.post(
                f"{self.nas_api_url}/process", 
                timeout=30,
                headers={'Content-Type': 'application/json'}
            )
            elapsed = time.time() - start_time
            self.logger.debug(f"â±ï¸ NAS ì²˜ë¦¬ ìš”ì²­ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            if response.status_code == 200:
                result = response.json()
                job_id = result.get('job_id')
                status = result.get('status')
                message = result.get('message', '')
                
                if status == 'already_running':
                    self.logger.info("ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤")
                    return job_id
                elif status == 'started':
                    self.logger.info(f"âœ… ì²˜ë¦¬ ì‘ì—… ì‹œì‘ë¨: {job_id}")
                    return job_id
                else:
                    self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ: {status}, ë©”ì‹œì§€: {message}")
                    return job_id
            else:                    
                self.logger.error(f"âŒ ì²˜ë¦¬ ì‹œì‘ ì‹¤íŒ¨: {response.status_code}")
                try:
                    error_detail = response.json().get('detail', response.text)
                    self.logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {error_detail}")
                except:
                    self.logger.error(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
                return None
            
        except requests.exceptions.Timeout:
            elapsed = time.time() - start_time
            self.logger.error(f"âŒ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ({elapsed:.2f}ì´ˆ)")
            return None
        except requests.exceptions.ConnectionError:
            self.logger.error(f"âŒ NAS ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {self.nas_api_url}")
            return None
        except requests.exceptions.RequestException as e:
            elapsed = time.time() - start_time
            self.logger.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
            return None
    
    def get_job_status(self, job_id: str) -> Optional[Dict]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.nas_api_url}/jobs/{job_id}", timeout=10)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 404:
                self.logger.warning(f"âš ï¸ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {job_id}")
                return None
            else:
                self.logger.error(f"âŒ ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ NAS API ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
        
    def wait_for_job_completion(self, job_id: str, timeout: int = 3600) -> Dict:
        """ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (í´ë§)"""
        self.logger.info(f"â³ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì¤‘: {job_id}")
        
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            job_status = self.get_job_status(job_id)
            if not job_status:
                raise RuntimeError(f"ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {job_id}")
            
            status = job_status.get('status')
            
            if status == 'completed':
                result = job_status.get('result', {})
                self.logger.info(f"âœ… ì‘ì—… ì™„ë£Œ: {job_id}")
                self.logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: ì„±ê³µ={result.get('success', 0)}, ì‹¤íŒ¨={result.get('failed', 0)}")
                return job_status
                
            elif status == 'failed':
                error = job_status.get('error', 'Unknown error')
                self.logger.error(f"âŒ ì‘ì—… ì‹¤íŒ¨: {job_id}, ì˜¤ë¥˜: {error}")
                raise RuntimeError(f"ì‘ì—… ì‹¤íŒ¨: {error}")
                
            elif status == 'running':
                self.logger.debug(f"ğŸ”„ ì‘ì—… ì§„í–‰ ì¤‘: {job_id}")
                time.sleep(self.polling_interval)
            else:
                self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… ìƒíƒœ: {status}")
                time.sleep(self.pooling_interval)
        
        raise TimeoutError(f"ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {job_id}")
 
    def add_provider(self, provider: str) -> bool:
        """ìƒˆë¡œìš´ Provider ì¶”ê°€"""
        if self.schema_manager.add_provider(provider):
            self.logger.info(f"âœ… Provider '{provider}' ì¶”ê°€ ì™„ë£Œ")
            return True
        else:
            self.logger.warning(f"âš ï¸ Provider '{provider}'ëŠ” ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
            return False
    
    def add_task(
        self, 
        task: str, 
        required_fields: Optional[List[str]] = None, 
        allowed_values: Optional[Dict[str, List[str]]] = None
    ) -> bool:
        """ìƒˆë¡œìš´ Task ì¶”ê°€"""
        if self.schema_manager.add_task(task, required_fields, allowed_values):
            self.logger.info(f"âœ… Task '{task}' ì¶”ê°€ ì™„ë£Œ")
            if required_fields:
                    self.logger.info(f"  ğŸ“ í•„ìˆ˜ í•„ë“œ: {required_fields}")
            if allowed_values:
                self.logger.info(f"  ğŸ”§ í—ˆìš© ê°’: {allowed_values}")
            return True
        else:
            self.logger.warning(f"âš ï¸ Task '{task}'ëŠ” ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
            return False
        
    def update_task(
        self, 
        task: str, 
        required_fields: Optional[List[str]] = None, 
        allowed_values: Optional[Dict[str, List[str]]] = None
    ) -> bool:
        """ê¸°ì¡´ Task ì—…ë°ì´íŠ¸"""
        if self.schema_manager.update_task(task, required_fields, allowed_values):
            self.logger.info(f"âœ… Task '{task}' ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            if required_fields:
                self.logger.info(f"  ğŸ“ í•„ìˆ˜ í•„ë“œ: {required_fields}")
            if allowed_values:
                self.logger.info(f"  ğŸ”§ í—ˆìš© ê°’: {allowed_values}")
            return True
        else:
            self.logger.warning(f"âš ï¸ Task '{task}'ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return False
    
    def remove_provider(self, provider: str) -> bool:
        """Provider ì œê±°"""
        if self.schema_manager.remove_provider(provider):
            self.logger.info(f"âœ… Provider '{provider}' ì œê±° ì™„ë£Œ")
            return True
        else:
            self.logger.warning(f"âš ï¸ Provider '{provider}'ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return False
    
    def remove_task(self, task: str) -> bool:
        """Task ì œê±°"""
        if self.schema_manager.remove_task(task):
            self.logger.info(f"âœ… Task '{task}' ì œê±° ì™„ë£Œ")
            return True
        else:
            self.logger.warning(f"âš ï¸ Task '{task}'ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return False
        
    def list_providers(self) -> List[str]:
        """ëª¨ë“  Provider ëª©ë¡ ì¡°íšŒ"""
        return self.schema_manager.get_all_providers()
    
    def list_tasks(self) -> Dict[str, Dict]:
        """ëª¨ë“  Task ëª©ë¡ ì¡°íšŒ"""
        return self.schema_manager.get_all_tasks()
    
    def show_schema_info(self):
        """ìŠ¤í‚¤ë§ˆ ì •ë³´ ëŒ€ì‹œë³´ë“œ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“‹ Schema Configuration Dashboard")
        print("="*60)
        
        # Providers
        providers = self.list_providers()
        print(f"\nğŸ¢ Providers ({len(providers)}ê°œ):")
        for provider in providers:
            print(f"  â€¢ {provider}")
        
        # Tasks
        tasks = self.list_tasks()
        print(f"\nğŸ“ Tasks ({len(tasks)}ê°œ):")
        for task_name, task_config in tasks.items():
            print(f"  â€¢ {task_name}")
            
            required_fields = task_config.get('required_fields', [])
            if required_fields:
                print(f"    ğŸ“ í•„ìˆ˜ í•„ë“œ: {', '.join(required_fields)}")
            
            allowed_values = task_config.get('allowed_values', {})
            if allowed_values:
                print(f"    ğŸ”§ í—ˆìš© ê°’:")
                for field, values in allowed_values.items():
                    print(f"      - {field}: {', '.join(values)}")
        
        print("="*60 + "\n")
        
    def _check_nas_api_connection(self):
        """NAS API ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(f"{self.nas_api_url}/health", timeout=5)
            if response.status_code == 200:
                self.logger.info(f"âœ… NAS API ì„œë²„ ì—°ê²° í™•ì¸: {self.nas_api_url}")
            else:
                self.logger.warning(f"âš ï¸ NAS API ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"âš ï¸ NAS API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            self.logger.warning("ğŸ”„ ë¡œì»¬ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤ (ìë™ ì²˜ë¦¬ ë¹„í™œì„±í™”)")
            self.auto_process = False
    
    def _create_metadata(
        self,
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        total_rows: int,
        data_type: str,
        source_task: Optional[str],
        has_images: bool = False,
        dataset_description: str = "",
        original_source: str = "",
        # **kwargs
    ) -> Dict:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata = {
            'provider': provider,
            'dataset': dataset,
            'task': task,
            'variant': variant,
            'data_type': data_type,
            'dataset_description': dataset_description,
            'original_source': original_source,
            'source_task': source_task,
            'has_images': has_images,
            'total_rows': total_rows,
            'uploaded_by': os.getenv('USER', 'unknown'),
            'uploaded_at': datetime.now().isoformat(),
            'file_id': str(uuid.uuid4())[:8],
        }
        # metadata.update(kwargs)
        self.logger.debug(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {metadata}")
        return metadata

    def _cleanup_existing_pending(
        self, 
        provider: str, 
        dataset: str, 
        task: str, 
        variant: str = None, 
        is_raw: bool = True,
    ):
        """ê°™ì€ provider/dataset/task ì¡°í•©ì˜ ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬"""
        pending_path = self.staging_path / "pending"
        
        if not pending_path.exists():
            return
        
        existing_dirs = []
        
        for pending_dir in pending_path.iterdir():
            if not pending_dir.is_dir():
                continue
                
            # ë©”íƒ€ë°ì´í„°ë¡œ ì •í™•íˆ í™•ì¸
            try:
                metadata_file = pending_dir / "upload_metadata.json"
                if metadata_file.exists():
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    basic_match = (
                        metadata.get('provider') == provider and 
                        metadata.get('dataset') == dataset and 
                        metadata.get('task') == task
                    )
                    
                    should_cleanup = False
                    if is_raw:
                        should_cleanup = basic_match
                    else:
                        should_cleanup = basic_match and metadata.get('variant') == variant
                        
                    if should_cleanup:
                        existing_dirs.append(pending_dir)
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {pending_dir} - {e}")
                continue
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        if existing_dirs:
            self.logger.info(f"ğŸ—‘ï¸  ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬: {len(existing_dirs)}ê°œ ë°œê²¬")
            self.logger.debug("ì‚­ì œí•  ë””ë ‰í† ë¦¬ ëª©ë¡:")
            self.logger.debug("\n".join(str(d) for d in existing_dirs))
            
            try:
                response = input("\nğŸ—‘ï¸  ìœ„ pending ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if response not in ['y', 'yes']:
                    self.logger.info("âŒ ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                    raise ValueError("ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            except KeyboardInterrupt:
                self.logger.info("âŒ ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                raise ValueError("ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                
            for existing_dir in existing_dirs:
                try:
                    shutil.rmtree(existing_dir)
                    self.logger.info(f"ğŸ—‘ï¸ ì‚­ì œ ì™„ë£Œ: {existing_dir.name}")
                except Exception as e:
                    self.logger.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {existing_dir.name} - {e}")
            
            self.logger.info(f"âœ… ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(existing_dirs)}ê°œ ì‚­ì œ")
        else:
            self.logger.debug("ğŸ“­ ì •ë¦¬í•  ê¸°ì¡´ pending ë°ì´í„° ì—†ìŒ")
    
    def _load_data(self, data_file: str,process_images: bool = False) -> Dataset:
        """ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ"""
        data_path = Path(data_file).resolve()
        if not data_path.exists():
            raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        
        self.logger.info(f"ğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {data_path}")   
        
        if data_path.is_dir():
            try:
                dataset_obj = load_from_disk(str(data_path))
                self.logger.info(f"âœ… datasets í´ë” ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ datasets í´ë” ë¡œë“œ ì‹¤íŒ¨: {e}")   
        elif data_path.suffix == '.parquet':
            try:
                df = pd.read_parquet(data_path)
                dataset_obj = Dataset.from_pandas(df)
                self.logger.info(f"âœ… Parquet íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ Parquet íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {data_path.suffix}")
        
        self.logger.info(f"âœ… ë°ì´í„° íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {data_file}")
        
        column_names = dataset_obj.column_names
        self.logger.info(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {column_names}")
                
       # í†µí•©ëœ ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ì²˜ë¦¬ (JSON dumps + ì´ë¯¸ì§€)
        dataset_obj = self._process_cast_columns(dataset_obj)
        
        if process_images:
            dataset_obj, has_images = self._process_images(dataset_obj)
        else:
            has_images = False
            self.logger.debug("ğŸ“„ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì²˜ë¦¬ ìƒëµ")
        
        return dataset_obj, has_images

    def _process_cast_columns(self, dataset_obj: Dataset):
        
        self.logger.info("ğŸ” JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ê²€ì‚¬ ì‹œì‘")
        
        json_cast_columns = []
        
        for key in dataset_obj.column_names:
            sample_value = dataset_obj[0][key]
            
            if isinstance(sample_value, (dict, list)):
                json_cast_columns.append(key)
                self.logger.info(f"ğŸ“ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ë°œê²¬: '{key}' (íƒ€ì…: {type(sample_value).__name__})")
        
        # JSON dumps ì²˜ë¦¬
        if json_cast_columns:
            dataset_obj = self._apply_json_transform(dataset_obj, json_cast_columns)
        else:
            self.logger.info("ğŸ“„ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ì—†ìŒ")
        
        return dataset_obj
    
    def _process_images(self, dataset_obj: Dataset):
        """ì´ë¯¸ì§€ ì»¬ëŸ¼ ì²˜ë¦¬"""
        self.logger.info("ğŸ” ì´ë¯¸ì§€ ì»¬ëŸ¼ ê²€ì‚¬ ì‹œì‘")
        
        image_column = None
        has_images = False
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ ì°¾ê¸°
        for key in dataset_obj.column_names:
            if key in self.image_column_candidates:
                image_column = key
                has_images = True
                self.logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ ë°œê²¬: '{key}'")
                break
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ ë³€í™˜
        if has_images and image_column:
            try:
                dataset_obj = dataset_obj.cast_column(image_column, ImageFeature())
                self.logger.info(f"âœ… ì´ë¯¸ì§€ ì»¬ëŸ¼ '{image_column}'ì„ PIL Imageë¡œ ë³€í™˜ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ ì´ë¯¸ì§€ ì»¬ëŸ¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
                raise ValueError(f"âŒ ì´ë¯¸ì§€ ì»¬ëŸ¼ '{image_column}'ì„ PIL Imageë¡œ ë³€í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.logger.info("ğŸ“„ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì—†ìŒ")
        
        return dataset_obj, has_images
    
    def _apply_json_transform(self, dataset_obj: Dataset, json_cast_columns: list) -> Dataset:
        """JSON ë³€í™˜ ì ìš©"""
        self.logger.info(f"ğŸ”„ {len(json_cast_columns)}ê°œ ì»¬ëŸ¼ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘: {json_cast_columns}")
        
        def json_transform(x):
            if isinstance(x, (dict, list)):
                return json.dumps(x, ensure_ascii=False)
            return x
        
        try:
            dataset_obj = dataset_obj.map(
                lambda x: {col: json_transform(x[col]) for col in json_cast_columns},
                num_proc=self.num_proc,
                desc="JSON ë³€í™˜ ì¤‘",
            )
            self.logger.info(f"âœ… JSON ë³€í™˜ ì™„ë£Œ: {json_cast_columns}")
            return dataset_obj
        except Exception as e:
            self.logger.error(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise ValueError(f"âŒ JSON ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _save_to_staging(self, dataset: Dataset, metadata: dict):
        """ë°ì´í„°ë¥¼ staging í´ë”ì— ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        file_id = metadata['file_id']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        user = metadata['uploaded_by']
        
        staging_dirname = f"{dataset_name}_{task}_{variant}_{file_id}_{timestamp}_{user}"
        staging_dir= self.staging_path / "pending" / staging_dirname
        
        try:
            dataset.save_to_disk(str(staging_dir))
            metadata_file = staging_dir / "upload_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=4)
                
            self.logger.info(f"ğŸ“¦ datasets ì €ì¥ ì™„ë£Œ: {staging_dir}")
            return str(staging_dir)
        except Exception as e:
            if staging_dir.exists():
                shutil.rmtree(staging_dir)
            raise ValueError(f"âŒ datasets ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _setup_console_logging(self, log_level: str):
        """ì½˜ì†” ë¡œê¹… ì„¤ì •"""
    
        self.formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(self.formatter)
        self.logger = logging.getLogger(__name__)
        self.logger.addHandler(console_handler)
        
        if log_level.upper() == "DEBUG":
            self.logger.setLevel(logging.DEBUG)
        else:
            self.logger.setLevel(logging.INFO)
            
    def _check_path_and_setup_logging(self):
        
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
        }
        
        missing_paths = []
        for path_name, path_obj in required_paths.items():
            if not path_obj.exists():
                missing_paths.append(f"  - {path_name}: {path_obj}")
        
        if missing_paths:
            missing_list = '\n'.join(missing_paths)
            raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
            
        self.logger.info("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")
        
        log_dir = self.base_path / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        date_str = datetime.now().strftime("%Y%m%d")
        user = os.getenv('USER', 'unknown')
        log_file = log_dir / f"DataManager_{date_str}_{user}.log"
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(self.formatter)
        self.logger.addHandler(file_handler)
        self.logger.info(f"ğŸ“ íŒŒì¼ ë¡œê¹… í™œì„±í™”: {log_file}")
        self.logger.info(f"ğŸš€ DataManager ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _convert_and_save_data(self, staging_dir: Path, target_path: Path, metadata: Dict):
        """Arrow ë°ì´í„°ë¥¼ Parquetìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
        self.logger.info("ğŸ”„ Arrow â†’ Parquet ë³€í™˜ ì¤‘")
        
        try:
            # Arrow ë°ì´í„° ë¡œë“œ
            dataset_obj = load_from_disk(str(staging_dir))
            
            if metadata.get("data_type") == "task":
                dataset_obj = self._add_metadata_columns(dataset_obj, metadata)
            
            # Parquetìœ¼ë¡œ ì €ì¥
            parquet_file = target_path / "data.parquet"
            dataset_obj.to_parquet(str(parquet_file))
            
            # ë©”íƒ€ë°ì´í„° ë³µì‚¬
            metadata_source = staging_dir / "upload_metadata.json"
            metadata_target = target_path / "_metadata.json"
            shutil.copy(str(metadata_source), str(metadata_target))
            
            self.logger.info(f"ğŸ’¾ Parquet ì €ì¥ ì™„ë£Œ: {parquet_file}")
            
        except Exception as e:
            raise ValueError(f"ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
        
    def _add_metadata_columns(self, dataset_obj: Dataset, metadata: Dict):
        """Task ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€"""
        self.logger.info("ğŸ“ Task ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€ ì¤‘")
        
        required_fields = self.schema_manager.get_required_fields(metadata['task'])
            
        # ë°ì´í„°ì…‹ ê¸¸ì´
        num_rows = len(dataset_obj)
        
        # í•„ìˆ˜ í•„ë“œë“¤ë§Œ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
        added_columns = []
        
        values = []
        for field in required_fields:
            value = metadata.get(field)
            if value is None:
                self.logger.warning(f"âš ï¸ í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ë©”íƒ€ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            values.append(value)
        for value in values:
            column_data = [metadata[field]] * num_rows
            dataset_obj = dataset_obj.add_column(field, column_data)
            added_columns.append(f"{field}={metadata[field]}")
            self.logger.debug(f"ğŸ“ ì»¬ëŸ¼ ì¶”ê°€: {field} = {metadata[field]}")
        
        if added_columns:
            self.logger.info(f"âœ… í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ: {', '.join(added_columns)}")
        else:
            self.logger.info("ğŸ“ ì¶”ê°€í•  í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì—†ìŒ")
            
        return dataset_obj
    
            
if __name__ == "__main__":
    manager = LocalDataManager(
        log_level="DEBUG",
        )
    
    manager.show_nas_dashboard()
    manager.show_schema_info()
    
    manager.add_provider("example_provider")
    manager.remove_provider("example_provider")
    manager.add_provider("example_provider")
    manager.add_task("example_task", required_fields=["field1", "field2"], allowed_values={"field1": ["value1", "value2"]})
    staging_dir, job_id = manager.upload_raw_data(
        data_file="/home/kai/workspace/DeepDocs_Project/datalake/managers/sample_data_1",
        provider="example_provider",
        dataset="example_dataset",
        dataset_description="This is a sample dataset for testing.",
        original_source="https://example.com/original_source"
    )
    
    if job_id:
        try:
            job_status = manager.wait_for_job_completion(job_id, timeout=600)
            print(f"ì‘ì—… ì™„ë£Œ: {job_status}")
        except Exception as e:
            print(f"ì‘ì—… ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    
    
    
