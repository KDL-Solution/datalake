import logging
import os
import uuid
import json
import shutil
import pandas as pd

from pathlib import Path
from datetime import datetime
from datasets import Dataset, load_from_disk
from datasets.features import Image as ImageFeature
from typing import Dict, Optional
from PIL import Image

import io
import hashlib
import sys
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from managers.schema_manager import SchemaManager

class DataManager:
    def __init__(
        self, 
        base_path: str = "/mnt/AI_NAS/datalake/migrate_test",
        log_level: str = "INFO",
        num_proc: int = 8, # ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
    ):
        self.base_path = Path(base_path)
        self.staging_path = self.base_path / "staging"
        self.staging_pending_path = self.staging_path / "pending"
        self.staging_processing_path = self.staging_path / "processing"
        self.staging_failed_path = self.staging_path / "failed"
        self.catalog_path = self.base_path / "catalog"
        self.assets_path  = self.base_path / "assets"
        self.archive_path = self.base_path / "archive"
        self.schema_path = self.base_path / "config" / "schema.yaml"
        self.num_proc = num_proc
        self.image_column_candidates = ['image', 'image_bytes']
        
        
        self._setup_console_logging(log_level)
        self._check_path_and_setup_logging()
        
        self.schema_manager = SchemaManager(
            config_path=self.schema_path,
            create_default=True
        )
      
    def upload_raw_data(
        self,
        data_file: str,
        provider: str,
        dataset: str,
        dataset_description: str = "", # ë°ì´í„°ì…‹ ì„¤ëª…
        original_source: str = "", # ì›ë³¸ ì†ŒìŠ¤ URL 
    ):
        task = "raw"
        variant = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        self.logger.info(f"ğŸ“¥ Raw data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}")
        
        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        self._cleanup_existing_pending(provider, dataset, task, is_raw=True)
        
        dataset_obj, has_images = self._load_data(data_file, process_images=True)
        
        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=variant,
            total_rows=len(dataset_obj),
            data_type="raw",
            source_task=None,  # ì›ë³¸ ì‘ì—…ì´ë¯€ë¡œ None
            has_images=has_images,
            dataset_description=dataset_description,
            original_source=original_source,
        )
        
        staging_dir = self._save_to_staging(dataset_obj, metadata)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        return staging_dir

    def upload_task_data(
        self,
        data_file: str,
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        dataset_description: str = "",
        source_task: str = None,
        **kwargs
    ) -> str:
        """Task ë°ì´í„° ì—…ë¡œë“œ (ê¸°ì¡´ catalogì—ì„œ íŠ¹ì • task ì¶”ì¶œ, ì´ë¯¸ì§€ ì°¸ì¡°ë§Œ)"""
        self.logger.info(f"ğŸ“¥ Task data ì—…ë¡œë“œ ì‹œì‘: {provider}/{dataset}/{task}/{variant}")
        
        # 1. Provider ê²€ì¦
        if not self.schema_manager.validate_provider(provider):
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì…ë‹ˆë‹¤: {provider}")
        
        # 2. Task ë©”íƒ€ë°ì´í„° ê²€ì¦
        is_valid, error_msg = self.schema_manager.validate_task_metadata(task, kwargs)
        if not is_valid:
            raise ValueError(f"âŒ Task ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
        
        # ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬
        self._cleanup_existing_pending(provider, dataset, task, variant=variant, is_raw=False)
        
        # ë°ì´í„° ë¡œë“œ ë° ì»¬ëŸ¼ ë³€í™˜ (ì´ë¯¸ì§€ ì œì™¸)
        dataset_obj, _ = self._load_data(data_file, process_images=False)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = self._create_metadata(
            provider=provider,
            dataset=dataset,
            task=task,
            variant=variant,
            dataset_description=dataset_description,
            source_task=source_task,
            has_images=False,  # ì´ë¯¸ì§€ëŠ” ì°¸ì¡°ë§Œ
            total_rows=len(dataset_obj),
            data_type='task',
            **kwargs
        )
        
        # Stagingì— ì €ì¥
        staging_dir = self._save_to_staging(dataset_obj, metadata)
        self.logger.info(f"âœ… Task ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ: {staging_dir}")
        
        return staging_dir
    
    def _create_metadata(
        self,
        provider: str,
        dataset: str,
        task: str,
        variant: str,
        total_rows: int,
        data_type: str,
        source_task: Optional[str],
        has_images: bool = False,
        dataset_description: str = "",
        original_source: str = "",
        # **kwargs
    ) -> Dict:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata = {
            'provider': provider,
            'dataset': dataset,
            'task': task,
            'variant': variant,
            'data_type': data_type,
            'dataset_description': dataset_description,
            'original_source': original_source,
            'source_task': source_task,
            'has_images': has_images,
            'total_rows': total_rows,
            'uploaded_by': os.getenv('USER', 'unknown'),
            'uploaded_at': datetime.now().isoformat(),
            'file_id': str(uuid.uuid4())[:8],
        }
        # metadata.update(kwargs)
        self.logger.debug(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {metadata}")
        return metadata

    def _cleanup_existing_pending(
        self, 
        provider: str, 
        dataset: str, 
        task: str, 
        variant: str = None, 
        is_raw: bool = True,
    ):
        """ê°™ì€ provider/dataset/task ì¡°í•©ì˜ ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬"""
        pending_path = self.staging_path / "pending"
        
        if not pending_path.exists():
            return
        
        existing_dirs = []
        
        for pending_dir in pending_path.iterdir():
            if not pending_dir.is_dir():
                continue
                
            # ë©”íƒ€ë°ì´í„°ë¡œ ì •í™•íˆ í™•ì¸
            try:
                metadata_file = pending_dir / "upload_metadata.json"
                if metadata_file.exists():
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    basic_match = (
                        metadata.get('provider') == provider and 
                        metadata.get('dataset') == dataset and 
                        metadata.get('task') == task
                    )
                    
                    should_cleanup = False
                    if is_raw:
                        should_cleanup = basic_match
                    else:
                        should_cleanup = basic_match and metadata.get('variant') == variant
                        
                    if should_cleanup:
                        existing_dirs.append(pending_dir)
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {pending_dir} - {e}")
                continue
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        if existing_dirs:
            self.logger.info(f"ğŸ—‘ï¸  ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬: {len(existing_dirs)}ê°œ ë°œê²¬")
            self.logger.debug("ì‚­ì œí•  ë””ë ‰í† ë¦¬ ëª©ë¡:")
            self.logger.debug("\n".join(str(d) for d in existing_dirs))
            
            try:
                response = input("\nğŸ—‘ï¸  ìœ„ pending ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if response not in ['y', 'yes']:
                    self.logger.info("âŒ ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                    raise ValueError("ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            except KeyboardInterrupt:
                self.logger.info("âŒ ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                raise ValueError("ì‚¬ìš©ìê°€ ì‚­ì œë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                
            for existing_dir in existing_dirs:
                try:
                    shutil.rmtree(existing_dir)
                    self.logger.info(f"ğŸ—‘ï¸ ì‚­ì œ ì™„ë£Œ: {existing_dir.name}")
                except Exception as e:
                    self.logger.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {existing_dir.name} - {e}")
            
            self.logger.info(f"âœ… ê¸°ì¡´ pending ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(existing_dirs)}ê°œ ì‚­ì œ")
        else:
            self.logger.debug("ğŸ“­ ì •ë¦¬í•  ê¸°ì¡´ pending ë°ì´í„° ì—†ìŒ")
    
    def _load_data(self, data_file: str,process_images: bool = False) -> Dataset:
        """ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ"""
        data_path = Path(data_file).resolve()
        if not data_path.exists():
            raise FileNotFoundError(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        
        self.logger.info(f"ğŸ“‚ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {data_path}")   
        
        if data_path.is_dir():
            try:
                dataset_obj = load_from_disk(str(data_path))
                self.logger.info(f"âœ… datasets í´ë” ë¡œë“œ ì™„ë£Œ: {len(dataset_obj)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ datasets í´ë” ë¡œë“œ ì‹¤íŒ¨: {e}")   
        elif data_path.suffix == '.parquet':
            try:
                df = pd.read_parquet(data_path)
                dataset_obj = Dataset.from_pandas(df)
                self.logger.info(f"âœ… Parquet íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
            except Exception as e:
                raise ValueError(f"âŒ Parquet íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {data_path.suffix}")
        
        self.logger.info(f"âœ… ë°ì´í„° íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {data_file}")
        
        column_names = dataset_obj.column_names
        self.logger.info(f"ë°ì´í„°ì…‹ ì»¬ëŸ¼: {column_names}")
                
       # í†µí•©ëœ ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ì²˜ë¦¬ (JSON dumps + ì´ë¯¸ì§€)
        dataset_obj = self._process_cast_columns(dataset_obj)
        
        if process_images:
            dataset_obj, has_images = self._process_images(dataset_obj)
        else:
            has_images = False
            self.logger.debug("ğŸ“„ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì²˜ë¦¬ ìƒëµ")
        
        return dataset_obj, has_images

    def _process_cast_columns(self, dataset_obj: Dataset):
        
        self.logger.info("ğŸ” JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ê²€ì‚¬ ì‹œì‘")
        
        json_cast_columns = []
        
        for key in dataset_obj.column_names:
            sample_value = dataset_obj[0][key]
            
            if isinstance(sample_value, (dict, list)):
                json_cast_columns.append(key)
                self.logger.info(f"ğŸ“ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ë°œê²¬: '{key}' (íƒ€ì…: {type(sample_value).__name__})")
        
        # JSON dumps ì²˜ë¦¬
        if json_cast_columns:
            dataset_obj = self._apply_json_transform(dataset_obj, json_cast_columns)
        else:
            self.logger.info("ğŸ“„ JSON ë³€í™˜ ëŒ€ìƒ ì»¬ëŸ¼ ì—†ìŒ")
        
        return dataset_obj
    
    def _process_images(self, dataset_obj: Dataset):
        """ì´ë¯¸ì§€ ì»¬ëŸ¼ ì²˜ë¦¬"""
        self.logger.info("ğŸ” ì´ë¯¸ì§€ ì»¬ëŸ¼ ê²€ì‚¬ ì‹œì‘")
        
        image_column = None
        has_images = False
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ ì°¾ê¸°
        for key in dataset_obj.column_names:
            if key in self.image_column_candidates:
                image_column = key
                has_images = True
                self.logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ ë°œê²¬: '{key}'")
                break
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ ë³€í™˜
        if has_images and image_column:
            try:
                dataset_obj = dataset_obj.cast_column(image_column, ImageFeature())
                self.logger.info(f"âœ… ì´ë¯¸ì§€ ì»¬ëŸ¼ '{image_column}'ì„ PIL Imageë¡œ ë³€í™˜ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"âŒ ì´ë¯¸ì§€ ì»¬ëŸ¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
                raise ValueError(f"âŒ ì´ë¯¸ì§€ ì»¬ëŸ¼ '{image_column}'ì„ PIL Imageë¡œ ë³€í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.logger.info("ğŸ“„ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì—†ìŒ")
        
        return dataset_obj, has_images
    
    def _apply_json_transform(self, dataset_obj: Dataset, json_cast_columns: list) -> Dataset:
        """JSON ë³€í™˜ ì ìš©"""
        self.logger.info(f"ğŸ”„ {len(json_cast_columns)}ê°œ ì»¬ëŸ¼ì„ JSONìœ¼ë¡œ ë³€í™˜ ì¤‘: {json_cast_columns}")
        
        def json_transform(x):
            if isinstance(x, (dict, list)):
                return json.dumps(x, ensure_ascii=False)
            return x
        
        try:
            dataset_obj = dataset_obj.map(
                lambda x: {col: json_transform(x[col]) for col in json_cast_columns},
                num_proc=self.num_proc,
                desc="JSON ë³€í™˜ ì¤‘",
            )
            self.logger.info(f"âœ… JSON ë³€í™˜ ì™„ë£Œ: {json_cast_columns}")
            return dataset_obj
        except Exception as e:
            self.logger.error(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise ValueError(f"âŒ JSON ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _save_to_staging(self, dataset: Dataset, metadata: dict):
        """ë°ì´í„°ë¥¼ staging í´ë”ì— ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        file_id = metadata['file_id']
        dataset_name = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        user = metadata['uploaded_by']
        
        staging_dirname = f"{dataset_name}_{task}_{variant}_{file_id}_{timestamp}_{user}"
        staging_dir= self.staging_path / "pending" / staging_dirname
        
        try:
            dataset.save_to_disk(str(staging_dir))
            metadata_file = staging_dir / "upload_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=4)
                
            self.logger.info(f"ğŸ“¦ datasets ì €ì¥ ì™„ë£Œ: {staging_dir}")
            return str(staging_dir)
        except Exception as e:
            if staging_dir.exists():
                shutil.rmtree(staging_dir)
            raise ValueError(f"âŒ datasets ì €ì¥ ì‹¤íŒ¨: {e}")
        
    def process_pending_data(self) -> Dict[str, int]:
        """pending í´ë”ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ catalogë¡œ ì´ë™"""
        self.logger.info("ğŸ”„ Pending ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        pending_path = self.staging_path / "pending"
        processing_path = self.staging_path / "processing"
        failed_path = self.staging_path / "failed"
        
        # pending í´ë”ì—ì„œ ì²˜ë¦¬ ëŒ€ìƒ ì°¾ê¸°
        pending_dirs = [d for d in pending_path.iterdir() if d.is_dir()]
        
        if not pending_dirs:
            self.logger.info("ğŸ“­ ì²˜ë¦¬í•  pending ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {"success": 0, "failed": 0}
        
        results = {"success": 0, "failed": 0}
        
        for pending_dir in pending_dirs:
            try:
                self.logger.info(f"ğŸ“¦ ì²˜ë¦¬ ì¤‘: {pending_dir.name}")
                
                # processingìœ¼ë¡œ ì´ë™
                processing_dir = processing_path / pending_dir.name
                shutil.move(str(pending_dir), str(processing_dir))
                
                # catalogë¡œ ì´ë™ ì²˜ë¦¬
                self._move_to_catalog(processing_dir)
                
                # ì„±ê³µ ì‹œ processing í´ë” ì •ë¦¬
                shutil.rmtree(processing_dir)
                
                results["success"] += 1
                self.logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {pending_dir.name}")
                
            except Exception as e:
                self.logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {pending_dir.name} - {e}")
                
                # ì‹¤íŒ¨ ì‹œ failedë¡œ ì´ë™
                if processing_dir.exists():
                    failed_dir = failed_path / pending_dir.name
                    failed_dir.parent.mkdir(exist_ok=True)
                    shutil.move(str(processing_dir), str(failed_dir))
                    
                    # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
                    error_log = failed_dir / "error.log"
                    with open(error_log, 'w', encoding='utf-8') as f:
                        f.write(f"Error: {str(e)}\n")
                        f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                
                results["failed"] += 1
        
        self.logger.info(f"ğŸ¯ ì²˜ë¦¬ ê²°ê³¼: ì„±ê³µ={results['success']}, ì‹¤íŒ¨={results['failed']}")
        return results
    
    def _setup_console_logging(self, log_level: str):
        """ì½˜ì†” ë¡œê¹… ì„¤ì •"""
    
        self.formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(self.formatter)
        self.logger = logging.getLogger(__name__)
        self.logger.addHandler(console_handler)
        
        if log_level.upper() == "DEBUG":
            self.logger.setLevel(logging.DEBUG)
        else:
            self.logger.setLevel(logging.INFO)
            
    def _check_path_and_setup_logging(self):
        
        required_paths = {
            'base': self.base_path,
            'staging': self.staging_path,
            'staging/pending': self.staging_pending_path,
            'staging/processing': self.staging_processing_path, 
            'staging/failed': self.staging_failed_path,
            'catalog': self.catalog_path,
            'assets': self.assets_path,
            'archive': self.archive_path
        }
        
        missing_paths = []
        for path_name, path_obj in required_paths.items():
            if not path_obj.exists():
                missing_paths.append(f"  - {path_name}: {path_obj}")
        
        if missing_paths:
            missing_list = '\n'.join(missing_paths)
            raise FileNotFoundError(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤:\n{missing_list}")
            
        self.logger.info("âœ… ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸ ì™„ë£Œ")
        
        log_dir = self.base_path / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        date_str = datetime.now().strftime("%Y%m%d")
        user = os.getenv('USER', 'unknown')
        log_file = log_dir / f"staging_manager_{date_str}_{user}.log"
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(self.formatter)
        self.logger.addHandler(file_handler)
        self.logger.info(f"ğŸ“ íŒŒì¼ ë¡œê¹… í™œì„±í™”: {log_file}")
        self.logger.info(f"ğŸš€ StagingManager ì´ˆê¸°í™” ì™„ë£Œ")
        
    def process_pending_data(self) -> Dict[str, int]:
        """pending í´ë”ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ catalogë¡œ ì´ë™"""
        self.logger.info("ğŸ”„ Pending ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        # pending í´ë”ì—ì„œ ì²˜ë¦¬ ëŒ€ìƒ ì°¾ê¸°
        pending_dirs = [d for d in self.staging_pending_path.iterdir() if d.is_dir()]
        
        if not pending_dirs:
            self.logger.info("ğŸ“­ ì²˜ë¦¬í•  pending ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {"success": 0, "failed": 0}
        
        results = {"success": 0, "failed": 0}
        
        for pending_dir in pending_dirs:
            try:
                self.logger.info(f"ğŸ“¦ ì²˜ë¦¬ ì¤‘: {pending_dir.name}")
                
                # processingìœ¼ë¡œ ì´ë™
                processing_dir = self.staging_processing_path / pending_dir.name
                shutil.move(str(pending_dir), str(processing_dir))
                
                # catalogë¡œ ì´ë™ ì²˜ë¦¬
                self._move_to_catalog(processing_dir)
                
                # ì„±ê³µ ì‹œ processing í´ë” ì •ë¦¬
                shutil.rmtree(processing_dir)
                
                results["success"] += 1
                self.logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {pending_dir.name}")
                
            except Exception as e:
                self.logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {pending_dir.name} - {e}")
                
                # ì‹¤íŒ¨ ì‹œ failedë¡œ ì´ë™
                if processing_dir.exists():
                    failed_dir = self.staging_failed_path / pending_dir.name
                    failed_dir.parent.mkdir(exist_ok=True)
                    shutil.move(str(processing_dir), str(failed_dir))
                    
                    # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
                    error_log = failed_dir / "error.log"
                    with open(error_log, 'w', encoding='utf-8') as f:
                        f.write(f"Error: {str(e)}\n")
                        f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                
                results["failed"] += 1
        
        self.logger.info(f"ğŸ¯ ì²˜ë¦¬ ê²°ê³¼: ì„±ê³µ={results['success']}, ì‹¤íŒ¨={results['failed']}")
        return results

    def _move_to_catalog(self, staging_dir: Path):
        self.logger.info(f"ğŸ“ Catalogë¡œ ì´ë™: {staging_dir.name}")
        
        # 1. metadata ì½ê¸°
        metadata_file = staging_dir / "upload_metadata.json"
        if not metadata_file.exists():
            raise FileNotFoundError(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {metadata_file}")
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        catalog_target_path = self._get_catalog_path(metadata)
        catalog_target_path.mkdir(parents=True, exist_ok=True)
                
        assets_target_path = None
        if metadata.get('data_type') == 'raw' and metadata.get('has_images', False):
            provider = metadata['provider']
            dataset = metadata['dataset']
            assets_target_path = (
                self.assets_path / 
                f"provider={provider}" / 
                f"dataset={dataset}"
            )
        try:
            if assets_target_path:
                self._extract_and_save_images(staging_dir, assets_target_path, metadata)
            self._convert_and_save_data(staging_dir, catalog_target_path, metadata)
        except Exception as e:
            self.logger.error(f"âŒ Catalogë¡œ ì´ë™ ì‹¤íŒ¨, ë¡¤ë°± ì¤‘: {e}")
            if catalog_target_path.exists():
                try:
                    shutil.rmtree(catalog_target_path)
                    self.logger.info(f"ğŸ”„ Catalog ë¡¤ë°± ì™„ë£Œ: {catalog_target_path}")
                except Exception as rollback_e:
                    self.logger.error(f"âŒ Catalog ë¡¤ë°± ì‹¤íŒ¨: {rollback_e}")
            if assets_target_path and assets_target_path.exists(): 
                try:
                    shutil.rmtree(assets_target_path)
                    self.logger.info(f"ğŸ”„ Assets ë¡¤ë°± ì™„ë£Œ: {assets_target_path}")
                except Exception as rollback_e:
                    self.logger.error(f"âŒ Assets ë¡¤ë°± ì‹¤íŒ¨: {rollback_e}")
            raise            
        
        self.logger.info(f"âœ… Catalog ì €ì¥ ì™„ë£Œ: {catalog_target_path}")
        
    def _get_catalog_path(self, metadata: Dict) -> Path:
        """metadata ê¸°ë°˜ìœ¼ë¡œ catalog ê²½ë¡œ ìƒì„±"""
        provider = metadata['provider']
        dataset = metadata['dataset']
        task = metadata['task']
        variant = metadata['variant']
        
        catalog_path = (
            self.catalog_path / 
            f"provider={provider}" / 
            f"dataset={dataset}" / 
            f"task={task}" / 
            f"variant={variant}"
        )    
        return catalog_path
    
    def _convert_and_save_data(self, staging_dir: Path, target_path: Path, metadata: Dict):
        """Arrow ë°ì´í„°ë¥¼ Parquetìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
        self.logger.info("ğŸ”„ Arrow â†’ Parquet ë³€í™˜ ì¤‘")
        
        try:
            # Arrow ë°ì´í„° ë¡œë“œ
            dataset_obj = load_from_disk(str(staging_dir))
            
            if metadata.get("data_type") == "task":
                dataset_obj = self._add_metadata_columns(dataset_obj, metadata)
            
            # Parquetìœ¼ë¡œ ì €ì¥
            parquet_file = target_path / "data.parquet"
            dataset_obj.to_parquet(str(parquet_file))
            
            # ë©”íƒ€ë°ì´í„° ë³µì‚¬
            metadata_source = staging_dir / "upload_metadata.json"
            metadata_target = target_path / "_metadata.json"
            shutil.copy(str(metadata_source), str(metadata_target))
            
            self.logger.info(f"ğŸ’¾ Parquet ì €ì¥ ì™„ë£Œ: {parquet_file}")
            
        except Exception as e:
            raise ValueError(f"ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
        
    def _add_metadata_columns(self, dataset_obj: Dataset, metadata: Dict):
        """Task ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€"""
        self.logger.info("ğŸ“ Task ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€ ì¤‘")
        
        required_fields = self.schema_manager.get_required_fields(metadata['task'])
            
        # ë°ì´í„°ì…‹ ê¸¸ì´
        num_rows = len(dataset_obj)
        
        # í•„ìˆ˜ í•„ë“œë“¤ë§Œ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
        added_columns = []
        
        values = []
        for field in required_fields:
            value = metadata.get(field)
            if value is None:
                self.logger.warning(f"âš ï¸ í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ë©”íƒ€ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            values.append(value)
        for value in values:
            column_data = [metadata[field]] * num_rows
            dataset_obj = dataset_obj.add_column(field, column_data)
            added_columns.append(f"{field}={metadata[field]}")
            self.logger.debug(f"ğŸ“ ì»¬ëŸ¼ ì¶”ê°€: {field} = {metadata[field]}")
        
        if added_columns:
            self.logger.info(f"âœ… í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ: {', '.join(added_columns)}")
        else:
            self.logger.info("ğŸ“ ì¶”ê°€í•  í•„ìˆ˜ í•„ë“œ ì»¬ëŸ¼ ì—†ìŒ")
            
        return dataset_obj
        
    def _extract_and_save_images(self, staging_dir: Path, assets_target_path: Path, metadata: Dict):
        """Arrow ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œí•˜ì—¬ assets í´ë”ì— ì €ì¥"""
        self.logger.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥ ì‹œì‘")
        assets_target_path.mkdir(parents=True, exist_ok=True)
        try:
            # Arrow ë°ì´í„° ë¡œë“œ
            dataset_obj = load_from_disk(str(staging_dir))
            
            # ì´ë¯¸ì§€ ì»¬ëŸ¼ ì°¾ê¸°
            image_column = None
            for col in dataset_obj.column_names:
                if col in self.image_column_candidates:
                    image_column = col
                    break
            
            if not image_column:
                self.logger.warning("âš ï¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                raise ValueError("ì´ë¯¸ì§€ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. 'image' ë˜ëŠ” 'image_bytes' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            self.logger.info(f"ğŸ“· ì´ë¯¸ì§€ ì»¬ëŸ¼ '{image_column}'ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘")
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥
            saved_count = 0
            for idx, row in enumerate(dataset_obj):
                try:
                    image_data = row[image_column]
                    
                    # PIL Image ê°ì²´ì¸ì§€ í™•ì¸
                    if hasattr(image_data, 'save'):  # PIL Image
                        pil_image = image_data
                    else:
                        # bytes ë°ì´í„°ë¼ë©´ PIL Imageë¡œ ë³€í™˜
                        if isinstance(image_data, bytes):
                            pil_image = Image.open(io.BytesIO(image_data))
                        else:
                            self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image_data)}")
                            continue
                    
                    # ì´ë¯¸ì§€ë¥¼ bytesë¡œ ë³€í™˜í•˜ì—¬ í•´ì‹œ ê³„ì‚°
                    img_bytes = self._pil_to_bytes(pil_image)
                    image_hash = hashlib.md5(img_bytes).hexdigest()
                    
                    # ì €ì¥ ê²½ë¡œ
                    image_filename = f"{image_hash}.jpg"
                    image_path = assets_target_path / image_filename
                    
                    # ì´ë¯¸ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì €ì¥
                    if not image_path.exists():
                        # RGB ëª¨ë“œë¡œ ë³€í™˜ (JPEG ì €ì¥ì„ ìœ„í•´)
                        if pil_image.mode != 'RGB':
                            pil_image = pil_image.convert('RGB')
                        
                        pil_image.save(str(image_path), 'JPEG', quality=95)
                        saved_count += 1
                        
                        if saved_count % 100 == 0:  # ì§„í–‰ ìƒí™© ë¡œê·¸
                            self.logger.info(f"ğŸ“· ì´ë¯¸ì§€ ì €ì¥ ì¤‘... {saved_count}ê°œ ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì´ë¯¸ì§€ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"âœ… ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥, ê²½ë¡œ: {assets_target_path}")
                
        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    manager = DataManager(log_level="DEBUG")
    
    manager.upload_raw_data(
        data_file="/home/kai/workspace/DeepDocs_Project/datalake/managers/table_test",
        provider="example_provider",
        dataset="example_dataset",
        dataset_description="This is a sample dataset for testing.",
        original_source="https://example.com/original_source"
    )
    
    manager.process_pending_data()